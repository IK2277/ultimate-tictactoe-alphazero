Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
Model saved to './model/best.pth'
>> Starting from cycle 0

Train 0 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 32, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 6: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 8: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 2: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 4: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 1: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 5: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 7: 10/62 games completed
Worker 3: 20/63 games completed
Worker 2: 20/63 games completed
Worker 6: 20/62 games completed
Worker 8: 20/62 games completed
Worker 4: 20/63 games completed
Worker 1: 20/63 games completed
Worker 5: 20/62 games completed
Worker 7: 20/62 games completed
Worker 3: 30/63 games completed
Worker 6: 30/62 games completed
Worker 2: 30/63 games completed
Worker 4: 30/63 games completed
Worker 1: 30/63 games completed
Worker 8: 30/62 games completed
Worker 5: 30/62 games completed
Worker 7: 30/62 games completed
Worker 3: 40/63 games completed
Worker 6: 40/62 games completed
Worker 2: 40/63 games completed
Worker 4: 40/63 games completed
Worker 1: 40/63 games completed
Worker 8: 40/62 games completed
Worker 5: 40/62 games completed
Worker 3: 50/63 games completed
Worker 7: 40/62 games completed
Worker 6: 50/62 games completed
Worker 2: 50/63 games completed
Worker 1: 50/63 games completed
Worker 4: 50/63 games completed
Worker 8: 50/62 games completed
Worker 5: 50/62 games completed
Worker 3: 60/63 games completed
Worker 7: 50/62 games completed
Worker 2: 60/63 games completed
Worker 6: 60/62 games completed
Worker 1: 60/63 games completed
Worker 4: 60/63 games completed
Worker 8: 60/62 games completed
Worker 5: 60/62 games completed
Worker 7: 60/62 games completed
>> Collected 26198 training samples from 500 games
>> Saved to ./data/20251030153546.history
>> Train 0
>> Learning Rate: 0.001
Epoch 1/100, Loss: 2.3468, LR: 0.001000
Epoch 2/100, Loss: 1.1093, LR: 0.001000
Epoch 3/100, Loss: 1.0882, LR: 0.001000
Epoch 4/100, Loss: 1.0088, LR: 0.001000
Epoch 5/100, Loss: 0.9745, LR: 0.001000
Epoch 6/100, Loss: 0.9620, LR: 0.001000
Epoch 7/100, Loss: 0.9546, LR: 0.001000
Epoch 8/100, Loss: 0.9471, LR: 0.001000
Epoch 9/100, Loss: 0.9415, LR: 0.001000
Epoch 10/100, Loss: 1.0002, LR: 0.001000
Epoch 11/100, Loss: 1.0042, LR: 0.001000
Epoch 12/100, Loss: 0.9682, LR: 0.001000
Epoch 13/100, Loss: 0.9415, LR: 0.001000
Epoch 14/100, Loss: 0.9326, LR: 0.001000
Epoch 15/100, Loss: 0.9267, LR: 0.001000
Epoch 16/100, Loss: 0.9241, LR: 0.001000
Epoch 17/100, Loss: 0.9215, LR: 0.001000
Epoch 18/100, Loss: 0.9177, LR: 0.001000
Epoch 19/100, Loss: 0.9187, LR: 0.001000
Epoch 20/100, Loss: 0.9404, LR: 0.001000
Epoch 21/100, Loss: 0.9233, LR: 0.001000
Epoch 22/100, Loss: 0.9152, LR: 0.001000
Epoch 23/100, Loss: 0.9125, LR: 0.001000
Epoch 24/100, Loss: 0.9087, LR: 0.001000
Epoch 25/100, Loss: 0.9088, LR: 0.001000
Epoch 26/100, Loss: 0.9082, LR: 0.001000
Epoch 27/100, Loss: 0.9078, LR: 0.001000
Epoch 28/100, Loss: 0.9078, LR: 0.001000
Epoch 29/100, Loss: 0.9059, LR: 0.001000
Epoch 30/100, Loss: 0.9043, LR: 0.001000
Epoch 31/100, Loss: 0.9346, LR: 0.001000
Epoch 32/100, Loss: 1.0100, LR: 0.001000
Epoch 33/100, Loss: 0.9397, LR: 0.001000
Epoch 34/100, Loss: 0.9225, LR: 0.001000
Epoch 35/100, Loss: 0.9130, LR: 0.001000
Epoch 36/100, Loss: 0.9074, LR: 0.001000
Epoch 37/100, Loss: 0.9050, LR: 0.001000
Epoch 38/100, Loss: 0.9024, LR: 0.001000
Epoch 39/100, Loss: 0.8994, LR: 0.001000
Epoch 40/100, Loss: 0.8989, LR: 0.001000
Epoch 41/100, Loss: 0.8978, LR: 0.001000
Epoch 42/100, Loss: 0.8982, LR: 0.001000
Epoch 43/100, Loss: 0.8970, LR: 0.001000
Epoch 44/100, Loss: 0.8961, LR: 0.001000
Epoch 45/100, Loss: 0.8946, LR: 0.001000
Epoch 46/100, Loss: 0.8929, LR: 0.001000
Epoch 47/100, Loss: 0.8946, LR: 0.001000
Epoch 48/100, Loss: 0.8937, LR: 0.001000
Epoch 49/100, Loss: 0.8937, LR: 0.001000
Epoch 50/100, Loss: 0.8928, LR: 0.000500
Epoch 51/100, Loss: 0.8859, LR: 0.000500
Epoch 52/100, Loss: 0.8811, LR: 0.000500
Epoch 53/100, Loss: 0.8798, LR: 0.000500
Epoch 54/100, Loss: 0.8797, LR: 0.000500
Epoch 55/100, Loss: 0.8797, LR: 0.000500
Epoch 56/100, Loss: 0.8802, LR: 0.000500
Epoch 57/100, Loss: 0.8805, LR: 0.000500
Epoch 58/100, Loss: 0.8810, LR: 0.000500
Epoch 59/100, Loss: 0.8815, LR: 0.000500
Epoch 60/100, Loss: 0.8819, LR: 0.000500
Epoch 61/100, Loss: 0.8818, LR: 0.000500
Epoch 62/100, Loss: 0.8818, LR: 0.000500
Epoch 63/100, Loss: 0.8812, LR: 0.000500
Epoch 64/100, Loss: 0.8809, LR: 0.000500
Epoch 65/100, Loss: 0.8811, LR: 0.000500
Epoch 66/100, Loss: 0.8829, LR: 0.000500
Epoch 67/100, Loss: 0.8819, LR: 0.000500
Epoch 68/100, Loss: 0.8802, LR: 0.000500
Epoch 69/100, Loss: 0.8798, LR: 0.000500
Epoch 70/100, Loss: 0.8797, LR: 0.000500
Epoch 71/100, Loss: 0.8800, LR: 0.000500
Epoch 72/100, Loss: 0.8807, LR: 0.000500
Epoch 73/100, Loss: 0.8798, LR: 0.000500
Epoch 74/100, Loss: 0.8795, LR: 0.000500
Epoch 75/100, Loss: 0.8801, LR: 0.000500
Epoch 76/100, Loss: 0.8796, LR: 0.000500
Epoch 77/100, Loss: 0.8794, LR: 0.000500
Epoch 78/100, Loss: 0.8799, LR: 0.000500
Epoch 79/100, Loss: 0.8796, LR: 0.000500
Epoch 80/100, Loss: 0.8791, LR: 0.000250
Epoch 81/100, Loss: 0.8775, LR: 0.000250
Epoch 82/100, Loss: 0.8759, LR: 0.000250
Epoch 83/100, Loss: 0.8752, LR: 0.000250
Epoch 84/100, Loss: 0.8750, LR: 0.000250
Epoch 85/100, Loss: 0.8749, LR: 0.000250
Epoch 86/100, Loss: 0.8750, LR: 0.000250
Epoch 87/100, Loss: 0.8751, LR: 0.000250
Epoch 88/100, Loss: 0.8756, LR: 0.000250
Epoch 89/100, Loss: 0.8763, LR: 0.000250
Epoch 90/100, Loss: 0.8762, LR: 0.000250
Epoch 91/100, Loss: 0.8762, LR: 0.000250
Epoch 92/100, Loss: 0.8758, LR: 0.000250
Epoch 93/100, Loss: 0.8757, LR: 0.000250
Epoch 94/100, Loss: 0.8757, LR: 0.000250
Epoch 95/100, Loss: 0.8756, LR: 0.000250
Epoch 96/100, Loss: 0.8759, LR: 0.000250
Epoch 97/100, Loss: 0.8759, LR: 0.000250
Epoch 98/100, Loss: 0.8756, LR: 0.000250
Epoch 99/100, Loss: 0.8757, LR: 0.000250
Epoch 100/100, Loss: 0.8756, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.445
Change BestPlayer
>> Model updated (win rate: 44.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.4
>> Cycle 0 completed. Checkpoint saved.

Train 1 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 32, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 8: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 2: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 1: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 5: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 6: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 4: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 7: 10/62 games completed
Worker 2: 20/63 games completed
Worker 1: 20/63 games completed
Worker 8: 20/62 games completed
Worker 6: 20/62 games completed
Worker 5: 20/62 games completed
Worker 3: 20/63 games completed
Worker 4: 20/63 games completed
Worker 7: 20/62 games completed
Worker 2: 30/63 games completed
Worker 1: 30/63 games completed
Worker 5: 30/62 games completed
Worker 6: 30/62 games completed
Worker 8: 30/62 games completed
Worker 3: 30/63 games completed
Worker 4: 30/63 games completed
Worker 7: 30/62 games completed
Worker 2: 40/63 games completed
Worker 1: 40/63 games completed
Worker 5: 40/62 games completed
Worker 8: 40/62 games completed
Worker 6: 40/62 games completed
Worker 3: 40/63 games completed
Worker 4: 40/63 games completed
Worker 7: 40/62 games completed
Worker 2: 50/63 games completed
Worker 5: 50/62 games completed
Worker 1: 50/63 games completed
Worker 6: 50/62 games completed
Worker 8: 50/62 games completed
Worker 4: 50/63 games completed
Worker 3: 50/63 games completed
Worker 7: 50/62 games completed
Worker 2: 60/63 games completed
Worker 1: 60/63 games completed
Worker 5: 60/62 games completed
Worker 8: 60/62 games completed
Worker 3: 60/63 games completed
Worker 4: 60/63 games completed
Worker 6: 60/62 games completed
Worker 7: 60/62 games completed
>> Collected 24371 training samples from 500 games
>> Saved to ./data/20251030154858.history
>> Train 1
>> Learning Rate: 0.001
Epoch 1/100, Loss: 1.5376, LR: 0.001000
Epoch 2/100, Loss: 1.4212, LR: 0.001000
Epoch 3/100, Loss: 2.7110, LR: 0.001000
Epoch 4/100, Loss: 2.6724, LR: 0.001000
Epoch 5/100, Loss: 2.6591, LR: 0.001000
Epoch 6/100, Loss: 2.6502, LR: 0.001000
Epoch 7/100, Loss: 2.6438, LR: 0.001000
Epoch 8/100, Loss: 2.6384, LR: 0.001000
Epoch 9/100, Loss: 2.6368, LR: 0.001000
Epoch 10/100, Loss: 2.6347, LR: 0.001000
Epoch 11/100, Loss: 2.6325, LR: 0.001000
Epoch 12/100, Loss: 2.6327, LR: 0.001000
Epoch 13/100, Loss: 2.6330, LR: 0.001000
Epoch 14/100, Loss: 2.6464, LR: 0.001000
Epoch 15/100, Loss: 2.6477, LR: 0.001000
Epoch 16/100, Loss: 2.6434, LR: 0.001000
Epoch 17/100, Loss: 2.6354, LR: 0.001000
Epoch 18/100, Loss: 2.6291, LR: 0.001000
Epoch 19/100, Loss: 2.6269, LR: 0.001000
Epoch 20/100, Loss: 2.6266, LR: 0.001000
Epoch 21/100, Loss: 2.6281, LR: 0.001000
Epoch 22/100, Loss: 2.6294, LR: 0.001000
Epoch 23/100, Loss: 2.6307, LR: 0.001000
Epoch 24/100, Loss: 2.6282, LR: 0.001000
Epoch 25/100, Loss: 2.6281, LR: 0.001000
Epoch 26/100, Loss: 2.6432, LR: 0.001000
Epoch 27/100, Loss: 2.4202, LR: 0.001000
Epoch 28/100, Loss: 1.2638, LR: 0.001000
Epoch 29/100, Loss: 1.2762, LR: 0.001000
Epoch 30/100, Loss: 1.4715, LR: 0.001000
Epoch 31/100, Loss: 1.2583, LR: 0.001000
Epoch 32/100, Loss: 1.2978, LR: 0.001000
Epoch 33/100, Loss: 1.3978, LR: 0.001000
Epoch 34/100, Loss: 1.2129, LR: 0.001000
Epoch 35/100, Loss: 1.1413, LR: 0.001000
Epoch 36/100, Loss: 1.1366, LR: 0.001000
Epoch 37/100, Loss: 1.1435, LR: 0.001000
Epoch 38/100, Loss: 1.1479, LR: 0.001000
Epoch 39/100, Loss: 1.1563, LR: 0.001000
Epoch 40/100, Loss: 1.1231, LR: 0.001000
Epoch 41/100, Loss: 1.1208, LR: 0.001000
Epoch 42/100, Loss: 1.1229, LR: 0.001000
Epoch 43/100, Loss: 1.1273, LR: 0.001000
Epoch 44/100, Loss: 1.1164, LR: 0.001000
Epoch 45/100, Loss: 1.1102, LR: 0.001000
Epoch 46/100, Loss: 1.1688, LR: 0.001000
Epoch 47/100, Loss: 1.2023, LR: 0.001000
Epoch 48/100, Loss: 1.2392, LR: 0.001000
Epoch 49/100, Loss: 1.2986, LR: 0.001000
Epoch 50/100, Loss: 1.2653, LR: 0.000500
Epoch 51/100, Loss: 1.1715, LR: 0.000500
Epoch 52/100, Loss: 1.1239, LR: 0.000500
Epoch 53/100, Loss: 1.1219, LR: 0.000500
Epoch 54/100, Loss: 1.1132, LR: 0.000500
Epoch 55/100, Loss: 1.1149, LR: 0.000500
Epoch 56/100, Loss: 1.1098, LR: 0.000500
Epoch 57/100, Loss: 1.1091, LR: 0.000500
Epoch 58/100, Loss: 1.1088, LR: 0.000500
Epoch 59/100, Loss: 1.1089, LR: 0.000500
Epoch 60/100, Loss: 1.1075, LR: 0.000500
Epoch 61/100, Loss: 1.1080, LR: 0.000500
Epoch 62/100, Loss: 1.1079, LR: 0.000500
Epoch 63/100, Loss: 1.1088, LR: 0.000500
Epoch 64/100, Loss: 1.1088, LR: 0.000500
Epoch 65/100, Loss: 1.1082, LR: 0.000500
Epoch 66/100, Loss: 1.1078, LR: 0.000500
Epoch 67/100, Loss: 1.1076, LR: 0.000500
Epoch 68/100, Loss: 1.1096, LR: 0.000500
Epoch 69/100, Loss: 1.1073, LR: 0.000500
Epoch 70/100, Loss: 1.1080, LR: 0.000500
Epoch 71/100, Loss: 1.1080, LR: 0.000500
Epoch 72/100, Loss: 1.1082, LR: 0.000500
Epoch 73/100, Loss: 1.1079, LR: 0.000500
Epoch 74/100, Loss: 1.1078, LR: 0.000500
Epoch 75/100, Loss: 1.1070, LR: 0.000500
Epoch 76/100, Loss: 1.1073, LR: 0.000500
Epoch 77/100, Loss: 1.1084, LR: 0.000500
Epoch 78/100, Loss: 1.1073, LR: 0.000500
Epoch 79/100, Loss: 1.1085, LR: 0.000500
Epoch 80/100, Loss: 1.1143, LR: 0.000250
Epoch 81/100, Loss: 1.1115, LR: 0.000250
Epoch 82/100, Loss: 1.1076, LR: 0.000250
Epoch 83/100, Loss: 1.1066, LR: 0.000250
Epoch 84/100, Loss: 1.1056, LR: 0.000250
Epoch 85/100, Loss: 1.1053, LR: 0.000250
Epoch 86/100, Loss: 1.1056, LR: 0.000250
Epoch 87/100, Loss: 1.1061, LR: 0.000250
Epoch 88/100, Loss: 1.1053, LR: 0.000250
Epoch 89/100, Loss: 1.1055, LR: 0.000250
Epoch 90/100, Loss: 1.1050, LR: 0.000250
Epoch 91/100, Loss: 1.1057, LR: 0.000250
Epoch 92/100, Loss: 1.1094, LR: 0.000250
Epoch 93/100, Loss: 1.1073, LR: 0.000250
Epoch 94/100, Loss: 1.1071, LR: 0.000250
Epoch 95/100, Loss: 1.1061, LR: 0.000250
Epoch 96/100, Loss: 1.1053, LR: 0.000250
Epoch 97/100, Loss: 1.1057, LR: 0.000250
Epoch 98/100, Loss: 1.1058, LR: 0.000250
Epoch 99/100, Loss: 1.1058, LR: 0.000250
Epoch 100/100, Loss: 1.1053, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.53
Change BestPlayer
>> Model updated (win rate: 53.0%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.15
>> Cycle 1 completed. Checkpoint saved.

Train 2 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 32, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 8: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 6: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 1: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 7: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 4: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 2: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 5: 10/62 games completed
Worker 6: 20/62 games completed
Worker 3: 20/63 games completed
Worker 5: 20/62 games completed
Worker 1: 20/63 games completed
Worker 7: 20/62 games completed
Worker 8: 20/62 games completed
Worker 2: 20/63 games completed
Worker 4: 20/63 games completed
Worker 3: 30/63 games completed
Worker 6: 30/62 games completed
Worker 1: 30/63 games completed
Worker 8: 30/62 games completed
Worker 5: 30/62 games completed
Worker 7: 30/62 games completed
Worker 2: 30/63 games completed
Worker 4: 30/63 games completed
Worker 3: 40/63 games completed
Worker 1: 40/63 games completed
Worker 6: 40/62 games completed
Worker 5: 40/62 games completed
Worker 8: 40/62 games completed
Worker 7: 40/62 games completed
Worker 2: 40/63 games completed
Worker 4: 40/63 games completed
Worker 1: 50/63 games completed
Worker 3: 50/63 games completed
Worker 8: 50/62 games completed
Worker 6: 50/62 games completed
Worker 5: 50/62 games completed
Worker 7: 50/62 games completed
Worker 4: 50/63 games completed
Worker 2: 50/63 games completed
Worker 1: 60/63 games completed
Worker 3: 60/63 games completed
Worker 6: 60/62 games completed
Worker 8: 60/62 games completed
Worker 5: 60/62 games completed
Worker 7: 60/62 games completed
Worker 4: 60/63 games completed
Worker 2: 60/63 games completed
>> Collected 24142 training samples from 500 games
>> Saved to ./data/20251030160133.history
>> Train 2
>> Learning Rate: 0.001
Epoch 1/100, Loss: 1.0847, LR: 0.001000
Epoch 2/100, Loss: 1.0333, LR: 0.001000
Epoch 3/100, Loss: 1.0336, LR: 0.001000
Epoch 4/100, Loss: 1.0902, LR: 0.001000
Epoch 5/100, Loss: 1.0039, LR: 0.001000
Epoch 6/100, Loss: 1.0346, LR: 0.001000
Epoch 7/100, Loss: 1.0338, LR: 0.001000
Epoch 8/100, Loss: 1.0190, LR: 0.001000
Epoch 9/100, Loss: 1.0041, LR: 0.001000
Epoch 10/100, Loss: 0.9980, LR: 0.001000
Epoch 11/100, Loss: 0.9962, LR: 0.001000
Epoch 12/100, Loss: 0.9945, LR: 0.001000
Epoch 13/100, Loss: 1.0497, LR: 0.001000
Epoch 14/100, Loss: 1.1297, LR: 0.001000
Epoch 15/100, Loss: 1.0428, LR: 0.001000
Epoch 16/100, Loss: 1.0336, LR: 0.001000
Epoch 17/100, Loss: 1.0316, LR: 0.001000
Epoch 18/100, Loss: 1.0172, LR: 0.001000
Epoch 19/100, Loss: 0.9938, LR: 0.001000
Epoch 20/100, Loss: 0.9860, LR: 0.001000
Epoch 21/100, Loss: 0.9836, LR: 0.001000
Epoch 22/100, Loss: 0.9835, LR: 0.001000
Epoch 23/100, Loss: 0.9830, LR: 0.001000
Epoch 24/100, Loss: 0.9823, LR: 0.001000
Epoch 25/100, Loss: 0.9829, LR: 0.001000
Epoch 26/100, Loss: 0.9829, LR: 0.001000
Epoch 27/100, Loss: 0.9829, LR: 0.001000
Epoch 28/100, Loss: 0.9823, LR: 0.001000
Epoch 29/100, Loss: 0.9826, LR: 0.001000
Epoch 30/100, Loss: 0.9833, LR: 0.001000
Epoch 31/100, Loss: 0.9890, LR: 0.001000
Epoch 32/100, Loss: 1.0023, LR: 0.001000
Epoch 33/100, Loss: 0.9963, LR: 0.001000
Epoch 34/100, Loss: 0.9866, LR: 0.001000
Epoch 35/100, Loss: 0.9859, LR: 0.001000
Epoch 36/100, Loss: 0.9828, LR: 0.001000
Epoch 37/100, Loss: 0.9810, LR: 0.001000
Epoch 38/100, Loss: 0.9806, LR: 0.001000
Epoch 39/100, Loss: 0.9802, LR: 0.001000
Epoch 40/100, Loss: 0.9802, LR: 0.001000
Epoch 41/100, Loss: 0.9804, LR: 0.001000
Epoch 42/100, Loss: 0.9808, LR: 0.001000
Epoch 43/100, Loss: 0.9807, LR: 0.001000
Epoch 44/100, Loss: 0.9801, LR: 0.001000
Epoch 45/100, Loss: 0.9804, LR: 0.001000
Epoch 46/100, Loss: 0.9817, LR: 0.001000
Epoch 47/100, Loss: 0.9827, LR: 0.001000
Epoch 48/100, Loss: 0.9823, LR: 0.001000
Epoch 49/100, Loss: 0.9833, LR: 0.001000
Epoch 50/100, Loss: 1.0061, LR: 0.000500
Epoch 51/100, Loss: 0.9875, LR: 0.000500
Epoch 52/100, Loss: 0.9821, LR: 0.000500
Epoch 53/100, Loss: 0.9809, LR: 0.000500
Epoch 54/100, Loss: 0.9804, LR: 0.000500
Epoch 55/100, Loss: 0.9799, LR: 0.000500
Epoch 56/100, Loss: 0.9808, LR: 0.000500
Epoch 57/100, Loss: 0.9797, LR: 0.000500
Epoch 58/100, Loss: 0.9799, LR: 0.000500
Epoch 59/100, Loss: 0.9791, LR: 0.000500
Epoch 60/100, Loss: 0.9793, LR: 0.000500
Epoch 61/100, Loss: 0.9793, LR: 0.000500
Epoch 62/100, Loss: 0.9793, LR: 0.000500
Epoch 63/100, Loss: 0.9793, LR: 0.000500
Epoch 64/100, Loss: 0.9791, LR: 0.000500
Epoch 65/100, Loss: 0.9794, LR: 0.000500
Epoch 66/100, Loss: 0.9797, LR: 0.000500
Epoch 67/100, Loss: 0.9797, LR: 0.000500
Epoch 68/100, Loss: 0.9799, LR: 0.000500
Epoch 69/100, Loss: 0.9797, LR: 0.000500
Epoch 70/100, Loss: 0.9793, LR: 0.000500
Epoch 71/100, Loss: 0.9797, LR: 0.000500
Epoch 72/100, Loss: 0.9798, LR: 0.000500
Epoch 73/100, Loss: 0.9839, LR: 0.000500
Epoch 74/100, Loss: 0.9832, LR: 0.000500
Epoch 75/100, Loss: 0.9815, LR: 0.000500
Epoch 76/100, Loss: 0.9811, LR: 0.000500
Epoch 77/100, Loss: 0.9808, LR: 0.000500
Epoch 78/100, Loss: 0.9804, LR: 0.000500
Epoch 79/100, Loss: 0.9807, LR: 0.000500
Epoch 80/100, Loss: 0.9809, LR: 0.000250
Epoch 81/100, Loss: 0.9799, LR: 0.000250
Epoch 82/100, Loss: 0.9802, LR: 0.000250
Epoch 83/100, Loss: 0.9800, LR: 0.000250
Epoch 84/100, Loss: 0.9805, LR: 0.000250
Epoch 85/100, Loss: 0.9802, LR: 0.000250
Epoch 86/100, Loss: 0.9798, LR: 0.000250
Epoch 87/100, Loss: 0.9802, LR: 0.000250
Epoch 88/100, Loss: 0.9803, LR: 0.000250
Epoch 89/100, Loss: 0.9801, LR: 0.000250
Epoch 90/100, Loss: 0.9801, LR: 0.000250
Epoch 91/100, Loss: 0.9803, LR: 0.000250
Epoch 92/100, Loss: 0.9804, LR: 0.000250
Epoch 93/100, Loss: 0.9803, LR: 0.000250
Epoch 94/100, Loss: 0.9804, LR: 0.000250
Epoch 95/100, Loss: 0.9804, LR: 0.000250
Epoch 96/100, Loss: 0.9802, LR: 0.000250
Epoch 97/100, Loss: 0.9805, LR: 0.000250
Epoch 98/100, Loss: 0.9804, LR: 0.000250
Epoch 99/100, Loss: 0.9805, LR: 0.000250
Epoch 100/100, Loss: 0.9802, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.5
Change BestPlayer
>> Model updated (win rate: 50.0%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.7
>> Cycle 2 completed. Checkpoint saved.

Train 3 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 32, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 1: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 2: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 6: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 5: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 7: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 4: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 8: 10/62 games completed
Worker 2: 20/63 games completed
Worker 1: 20/63 games completed
Worker 6: 20/62 games completed
Worker 3: 20/63 games completed
Worker 5: 20/62 games completed
Worker 7: 20/62 games completed
Worker 4: 20/63 games completed
Worker 8: 20/62 games completed
Worker 6: 30/62 games completed
Worker 2: 30/63 games completed
Worker 1: 30/63 games completed
Worker 7: 30/62 games completed
Worker 5: 30/62 games completed
Worker 3: 30/63 games completed
Worker 4: 30/63 games completed
Worker 8: 30/62 games completed
Worker 2: 40/63 games completed
Worker 1: 40/63 games completed
Worker 6: 40/62 games completed
Worker 7: 40/62 games completed
Worker 3: 40/63 games completed
Worker 5: 40/62 games completed
Worker 4: 40/63 games completed
Worker 8: 40/62 games completed
Worker 2: 50/63 games completed
Worker 1: 50/63 games completed
Worker 6: 50/62 games completed
Worker 7: 50/62 games completed
Worker 3: 50/63 games completed
Worker 4: 50/63 games completed
Worker 5: 50/62 games completed
Worker 2: 60/63 games completed
Worker 8: 50/62 games completed
Worker 6: 60/62 games completed
Worker 1: 60/63 games completed
Worker 7: 60/62 games completed
Worker 3: 60/63 games completed
Worker 4: 60/63 games completed
Worker 5: 60/62 games completed
Worker 8: 60/62 games completed
>> Collected 24066 training samples from 500 games
>> Saved to ./data/20251030161357.history
>> Train 3
>> Learning Rate: 0.001
Epoch 1/100, Loss: 1.1260, LR: 0.001000
Epoch 2/100, Loss: 1.1032, LR: 0.001000
Epoch 3/100, Loss: 1.0907, LR: 0.001000
Epoch 4/100, Loss: 1.0803, LR: 0.001000
Epoch 5/100, Loss: 1.0831, LR: 0.001000
Epoch 6/100, Loss: 1.0761, LR: 0.001000
Epoch 7/100, Loss: 1.0738, LR: 0.001000
Epoch 8/100, Loss: 1.1139, LR: 0.001000
Epoch 9/100, Loss: 1.0935, LR: 0.001000
Epoch 10/100, Loss: 1.0970, LR: 0.001000
Epoch 11/100, Loss: 1.0863, LR: 0.001000
Epoch 12/100, Loss: 1.0757, LR: 0.001000
Epoch 13/100, Loss: 1.0844, LR: 0.001000
Epoch 14/100, Loss: 1.0738, LR: 0.001000
Epoch 15/100, Loss: 1.0648, LR: 0.001000
Epoch 16/100, Loss: 1.0650, LR: 0.001000
Epoch 17/100, Loss: 1.0811, LR: 0.001000
Epoch 18/100, Loss: 1.1373, LR: 0.001000
Epoch 19/100, Loss: 1.0814, LR: 0.001000
Epoch 20/100, Loss: 1.0977, LR: 0.001000
Epoch 21/100, Loss: 1.0801, LR: 0.001000
Epoch 22/100, Loss: 1.2509, LR: 0.001000
Epoch 23/100, Loss: 1.1813, LR: 0.001000
Epoch 24/100, Loss: 1.1496, LR: 0.001000
Epoch 25/100, Loss: 1.1296, LR: 0.001000
Epoch 26/100, Loss: 1.1295, LR: 0.001000
Epoch 27/100, Loss: 1.0599, LR: 0.001000
Epoch 28/100, Loss: 1.0488, LR: 0.001000
Epoch 29/100, Loss: 1.0494, LR: 0.001000
Epoch 30/100, Loss: 1.0441, LR: 0.001000
Epoch 31/100, Loss: 1.0609, LR: 0.001000
Epoch 32/100, Loss: 1.1585, LR: 0.001000
Epoch 33/100, Loss: 1.1682, LR: 0.001000
Epoch 34/100, Loss: 1.2064, LR: 0.001000
Epoch 35/100, Loss: 1.1413, LR: 0.001000
Epoch 36/100, Loss: 1.1371, LR: 0.001000
Epoch 37/100, Loss: 1.1983, LR: 0.001000
Epoch 38/100, Loss: 1.2411, LR: 0.001000
Epoch 39/100, Loss: 1.2288, LR: 0.001000
Epoch 40/100, Loss: 1.2183, LR: 0.001000
Epoch 41/100, Loss: 1.1783, LR: 0.001000
Epoch 42/100, Loss: 1.1691, LR: 0.001000
Epoch 43/100, Loss: 1.1637, LR: 0.001000
Epoch 44/100, Loss: 1.1397, LR: 0.001000
Epoch 45/100, Loss: 1.1419, LR: 0.001000
Epoch 46/100, Loss: 1.1410, LR: 0.001000
Epoch 47/100, Loss: 1.1394, LR: 0.001000
Epoch 48/100, Loss: 1.1391, LR: 0.001000
Epoch 49/100, Loss: 1.1622, LR: 0.001000
Epoch 50/100, Loss: 1.1550, LR: 0.000500
Epoch 51/100, Loss: 1.1208, LR: 0.000500
Epoch 52/100, Loss: 1.0787, LR: 0.000500
Epoch 53/100, Loss: 1.0571, LR: 0.000500
Epoch 54/100, Loss: 1.0621, LR: 0.000500
Epoch 55/100, Loss: 1.0770, LR: 0.000500
Epoch 56/100, Loss: 1.0462, LR: 0.000500
Epoch 57/100, Loss: 1.0354, LR: 0.000500
Epoch 58/100, Loss: 1.0691, LR: 0.000500
Epoch 59/100, Loss: 1.0213, LR: 0.000500
Epoch 60/100, Loss: 1.0117, LR: 0.000500
Epoch 61/100, Loss: 1.0182, LR: 0.000500
Epoch 62/100, Loss: 1.0168, LR: 0.000500
Epoch 63/100, Loss: 1.0043, LR: 0.000500
Epoch 64/100, Loss: 0.9988, LR: 0.000500
Epoch 65/100, Loss: 1.0376, LR: 0.000500
Epoch 66/100, Loss: 1.0203, LR: 0.000500
Epoch 67/100, Loss: 1.0096, LR: 0.000500
Epoch 68/100, Loss: 1.0110, LR: 0.000500
Epoch 69/100, Loss: 1.0131, LR: 0.000500
Epoch 70/100, Loss: 1.0147, LR: 0.000500
Epoch 71/100, Loss: 1.0026, LR: 0.000500
Epoch 72/100, Loss: 0.9935, LR: 0.000500
Epoch 73/100, Loss: 0.9827, LR: 0.000500
Epoch 74/100, Loss: 0.9946, LR: 0.000500
Epoch 75/100, Loss: 0.9805, LR: 0.000500
Epoch 76/100, Loss: 0.9826, LR: 0.000500
Epoch 77/100, Loss: 0.9715, LR: 0.000500
Epoch 78/100, Loss: 0.9672, LR: 0.000500
Epoch 79/100, Loss: 0.9738, LR: 0.000500
Epoch 80/100, Loss: 0.9763, LR: 0.000250
Epoch 81/100, Loss: 0.9777, LR: 0.000250
Epoch 82/100, Loss: 0.9697, LR: 0.000250
Epoch 83/100, Loss: 0.9639, LR: 0.000250
Epoch 84/100, Loss: 0.9689, LR: 0.000250
Epoch 85/100, Loss: 0.9658, LR: 0.000250
Epoch 86/100, Loss: 0.9629, LR: 0.000250
Epoch 87/100, Loss: 0.9565, LR: 0.000250
Epoch 88/100, Loss: 0.9594, LR: 0.000250
Epoch 89/100, Loss: 0.9552, LR: 0.000250
Epoch 90/100, Loss: 0.9579, LR: 0.000250
Epoch 91/100, Loss: 0.9640, LR: 0.000250
Epoch 92/100, Loss: 0.9524, LR: 0.000250
Epoch 93/100, Loss: 0.9523, LR: 0.000250
Epoch 94/100, Loss: 0.9523, LR: 0.000250
Epoch 95/100, Loss: 0.9474, LR: 0.000250
Epoch 96/100, Loss: 0.9517, LR: 0.000250
Epoch 97/100, Loss: 0.9492, LR: 0.000250
Epoch 98/100, Loss: 0.9542, LR: 0.000250
Epoch 99/100, Loss: 0.9545, LR: 0.000250
Epoch 100/100, Loss: 0.9495, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.535
Change BestPlayer
>> Model updated (win rate: 53.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.35
>> Cycle 3 completed. Checkpoint saved.

Train 4 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 32, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 1: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 7: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 5: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 6: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 8: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 2: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 4: 10/63 games completed
Worker 8: 20/62 games completed
Worker 1: 20/63 games completed
Worker 7: 20/62 games completed
Worker 5: 20/62 games completed
Worker 2: 20/63 games completed
Worker 6: 20/62 games completed
Worker 3: 20/63 games completed
Worker 4: 20/63 games completed
Worker 8: 30/62 games completed
Worker 5: 30/62 games completed
Worker 7: 30/62 games completed
Worker 1: 30/63 games completed
Worker 6: 30/62 games completed
Worker 2: 30/63 games completed
Worker 3: 30/63 games completed
Worker 4: 30/63 games completed
Worker 8: 40/62 games completed
Worker 5: 40/62 games completed
Worker 7: 40/62 games completed
Worker 1: 40/63 games completed
Worker 2: 40/63 games completed
Worker 6: 40/62 games completed
Worker 3: 40/63 games completed
Worker 4: 40/63 games completed
Worker 8: 50/62 games completed
Worker 5: 50/62 games completed
Worker 1: 50/63 games completed
Worker 2: 50/63 games completed
Worker 7: 50/62 games completed
Worker 6: 50/62 games completed
Worker 3: 50/63 games completed
Worker 4: 50/63 games completed
Worker 2: 60/63 games completed
Worker 1: 60/63 games completed
Worker 8: 60/62 games completed
Worker 5: 60/62 games completed
Worker 7: 60/62 games completed
Worker 6: 60/62 games completed
Worker 3: 60/63 games completed
Worker 4: 60/63 games completed
>> Collected 24131 training samples from 500 games
>> Saved to ./data/20251030162622.history
>> Train 4
>> Learning Rate: 0.001
Epoch 1/100, Loss: 0.9927, LR: 0.001000
Epoch 2/100, Loss: 0.9530, LR: 0.001000
Epoch 3/100, Loss: 0.9466, LR: 0.001000
Epoch 4/100, Loss: 0.9370, LR: 0.001000
Epoch 5/100, Loss: 0.9723, LR: 0.001000
Epoch 6/100, Loss: 0.9322, LR: 0.001000
Epoch 7/100, Loss: 0.9311, LR: 0.001000
Epoch 8/100, Loss: 0.9267, LR: 0.001000
Epoch 9/100, Loss: 0.9264, LR: 0.001000
Epoch 10/100, Loss: 0.9500, LR: 0.001000
Epoch 11/100, Loss: 1.0759, LR: 0.001000
Epoch 12/100, Loss: 1.0018, LR: 0.001000
Epoch 13/100, Loss: 0.9940, LR: 0.001000
Epoch 14/100, Loss: 0.9278, LR: 0.001000
Epoch 15/100, Loss: 0.9245, LR: 0.001000
Epoch 16/100, Loss: 0.9400, LR: 0.001000
Epoch 17/100, Loss: 0.9239, LR: 0.001000
Epoch 18/100, Loss: 0.9228, LR: 0.001000
Epoch 19/100, Loss: 0.9216, LR: 0.001000
Epoch 20/100, Loss: 0.9211, LR: 0.001000
Epoch 21/100, Loss: 0.9204, LR: 0.001000
Epoch 22/100, Loss: 0.9208, LR: 0.001000
Epoch 23/100, Loss: 0.9203, LR: 0.001000
Epoch 24/100, Loss: 0.9202, LR: 0.001000
Epoch 25/100, Loss: 0.9202, LR: 0.001000
Epoch 26/100, Loss: 0.9197, LR: 0.001000
Epoch 27/100, Loss: 0.9196, LR: 0.001000
Epoch 28/100, Loss: 0.9213, LR: 0.001000
Epoch 29/100, Loss: 0.9287, LR: 0.001000
Epoch 30/100, Loss: 0.9260, LR: 0.001000
Epoch 31/100, Loss: 0.9295, LR: 0.001000
Epoch 32/100, Loss: 0.9219, LR: 0.001000
Epoch 33/100, Loss: 0.9205, LR: 0.001000
Epoch 34/100, Loss: 0.9196, LR: 0.001000
Epoch 35/100, Loss: 0.9190, LR: 0.001000
Epoch 36/100, Loss: 0.9191, LR: 0.001000
Epoch 37/100, Loss: 0.9183, LR: 0.001000
Epoch 38/100, Loss: 0.9184, LR: 0.001000
Epoch 39/100, Loss: 0.9204, LR: 0.001000
Epoch 40/100, Loss: 0.9193, LR: 0.001000
Epoch 41/100, Loss: 0.9204, LR: 0.001000
Epoch 42/100, Loss: 0.9203, LR: 0.001000
Epoch 43/100, Loss: 0.9209, LR: 0.001000
Epoch 44/100, Loss: 0.9193, LR: 0.001000
Epoch 45/100, Loss: 0.9192, LR: 0.001000
Epoch 46/100, Loss: 0.9186, LR: 0.001000
Epoch 47/100, Loss: 0.9190, LR: 0.001000
Epoch 48/100, Loss: 0.9189, LR: 0.001000
Epoch 49/100, Loss: 0.9195, LR: 0.001000
Epoch 50/100, Loss: 0.9321, LR: 0.000500
Epoch 51/100, Loss: 0.9259, LR: 0.000500
Epoch 52/100, Loss: 0.9180, LR: 0.000500
Epoch 53/100, Loss: 0.9172, LR: 0.000500
Epoch 54/100, Loss: 0.9163, LR: 0.000500
Epoch 55/100, Loss: 0.9159, LR: 0.000500
Epoch 56/100, Loss: 0.9162, LR: 0.000500
Epoch 57/100, Loss: 0.9156, LR: 0.000500
Epoch 58/100, Loss: 0.9155, LR: 0.000500
Epoch 59/100, Loss: 0.9155, LR: 0.000500
Epoch 60/100, Loss: 0.9152, LR: 0.000500
Epoch 61/100, Loss: 0.9152, LR: 0.000500
Epoch 62/100, Loss: 0.9154, LR: 0.000500
Epoch 63/100, Loss: 0.9151, LR: 0.000500
Epoch 64/100, Loss: 0.9155, LR: 0.000500
Epoch 65/100, Loss: 0.9155, LR: 0.000500
Epoch 66/100, Loss: 0.9155, LR: 0.000500
Epoch 67/100, Loss: 0.9150, LR: 0.000500
Epoch 68/100, Loss: 0.9156, LR: 0.000500
Epoch 69/100, Loss: 0.9157, LR: 0.000500
Epoch 70/100, Loss: 0.9155, LR: 0.000500
Epoch 71/100, Loss: 0.9155, LR: 0.000500
Epoch 72/100, Loss: 0.9169, LR: 0.000500
Epoch 73/100, Loss: 0.9168, LR: 0.000500
Epoch 74/100, Loss: 0.9157, LR: 0.000500
Epoch 75/100, Loss: 0.9154, LR: 0.000500
Epoch 76/100, Loss: 0.9164, LR: 0.000500
Epoch 77/100, Loss: 0.9158, LR: 0.000500
Epoch 78/100, Loss: 0.9150, LR: 0.000500
Epoch 79/100, Loss: 0.9149, LR: 0.000500
Epoch 80/100, Loss: 0.9152, LR: 0.000250
Epoch 81/100, Loss: 0.9153, LR: 0.000250
Epoch 82/100, Loss: 0.9147, LR: 0.000250
Epoch 83/100, Loss: 0.9149, LR: 0.000250
Epoch 84/100, Loss: 0.9171, LR: 0.000250
Epoch 85/100, Loss: 0.9236, LR: 0.000250
Epoch 86/100, Loss: 0.9206, LR: 0.000250
Epoch 87/100, Loss: 0.9190, LR: 0.000250
Epoch 88/100, Loss: 0.9154, LR: 0.000250
Epoch 89/100, Loss: 0.9153, LR: 0.000250
Epoch 90/100, Loss: 0.9152, LR: 0.000250
Epoch 91/100, Loss: 0.9148, LR: 0.000250
Epoch 92/100, Loss: 0.9151, LR: 0.000250
Epoch 93/100, Loss: 0.9146, LR: 0.000250
Epoch 94/100, Loss: 0.9149, LR: 0.000250
Epoch 95/100, Loss: 0.9148, LR: 0.000250
Epoch 96/100, Loss: 0.9146, LR: 0.000250
Epoch 97/100, Loss: 0.9149, LR: 0.000250
Epoch 98/100, Loss: 0.9151, LR: 0.000250
Epoch 99/100, Loss: 0.9150, LR: 0.000250
Epoch 100/100, Loss: 0.9148, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.585
Change BestPlayer
>> Model updated (win rate: 58.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.4
>> Cycle 4 completed. Checkpoint saved.

Train 5 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 32, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 1: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 2: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 5: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 6: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 7: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 8: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 4: 10/63 games completed
Worker 2: 20/63 games completed
Worker 5: 20/62 games completed
Worker 1: 20/63 games completed
Worker 7: 20/62 games completed
Worker 4: 20/63 games completed
Worker 6: 20/62 games completed
Worker 8: 20/62 games completed
Worker 3: 20/63 games completed
Worker 2: 30/63 games completed
Worker 1: 30/63 games completed
Worker 5: 30/62 games completed
Worker 7: 30/62 games completed
Worker 6: 30/62 games completed
Worker 3: 30/63 games completed
Worker 4: 30/63 games completed
Worker 8: 30/62 games completed
Worker 2: 40/63 games completed
Worker 5: 40/62 games completed
Worker 7: 40/62 games completed
Worker 1: 40/63 games completed
Worker 3: 40/63 games completed
Worker 6: 40/62 games completed
Worker 4: 40/63 games completed
Worker 8: 40/62 games completed
Worker 2: 50/63 games completed
Worker 7: 50/62 games completed
Worker 1: 50/63 games completed
Worker 5: 50/62 games completed
Worker 3: 50/63 games completed
Worker 6: 50/62 games completed
Worker 4: 50/63 games completed
Worker 8: 50/62 games completed
Worker 2: 60/63 games completed
Worker 3: 60/63 games completed
Worker 5: 60/62 games completed
Worker 7: 60/62 games completed
Worker 1: 60/63 games completed
Worker 6: 60/62 games completed
Worker 4: 60/63 games completed
Worker 8: 60/62 games completed
>> Collected 23775 training samples from 500 games
>> Saved to ./data/20251030163826.history
>> Train 5
>> Learning Rate: 0.001
Epoch 1/100, Loss: 0.9526, LR: 0.001000
Epoch 2/100, Loss: 0.9288, LR: 0.001000
Epoch 3/100, Loss: 0.9222, LR: 0.001000
Epoch 4/100, Loss: 0.9183, LR: 0.001000
Epoch 5/100, Loss: 0.9157, LR: 0.001000
Epoch 6/100, Loss: 0.9139, LR: 0.001000
Epoch 7/100, Loss: 0.9133, LR: 0.001000
Epoch 8/100, Loss: 0.9126, LR: 0.001000
Epoch 9/100, Loss: 0.9117, LR: 0.001000
Epoch 10/100, Loss: 0.9115, LR: 0.001000
Epoch 11/100, Loss: 0.9114, LR: 0.001000
Epoch 12/100, Loss: 0.9117, LR: 0.001000
Epoch 13/100, Loss: 0.9114, LR: 0.001000
Epoch 14/100, Loss: 0.9120, LR: 0.001000
Epoch 15/100, Loss: 0.9127, LR: 0.001000
Epoch 16/100, Loss: 0.9304, LR: 0.001000
Epoch 17/100, Loss: 0.9261, LR: 0.001000
Epoch 18/100, Loss: 0.9180, LR: 0.001000
Epoch 19/100, Loss: 0.9135, LR: 0.001000
Epoch 20/100, Loss: 0.9115, LR: 0.001000
Epoch 21/100, Loss: 0.9109, LR: 0.001000
Epoch 22/100, Loss: 0.9104, LR: 0.001000
Epoch 23/100, Loss: 0.9104, LR: 0.001000
Epoch 24/100, Loss: 0.9101, LR: 0.001000
Epoch 25/100, Loss: 0.9102, LR: 0.001000
Epoch 26/100, Loss: 0.9102, LR: 0.001000
Epoch 27/100, Loss: 0.9103, LR: 0.001000
Epoch 28/100, Loss: 0.9102, LR: 0.001000
Epoch 29/100, Loss: 0.9103, LR: 0.001000
Epoch 30/100, Loss: 0.9108, LR: 0.001000
Epoch 31/100, Loss: 0.9105, LR: 0.001000
Epoch 32/100, Loss: 0.9111, LR: 0.001000
Epoch 33/100, Loss: 0.9113, LR: 0.001000
Epoch 34/100, Loss: 0.9220, LR: 0.001000
Epoch 35/100, Loss: 0.9254, LR: 0.001000
Epoch 36/100, Loss: 0.9166, LR: 0.001000
Epoch 37/100, Loss: 0.9127, LR: 0.001000
Epoch 38/100, Loss: 0.9116, LR: 0.001000
Epoch 39/100, Loss: 0.9109, LR: 0.001000
Epoch 40/100, Loss: 0.9109, LR: 0.001000
Epoch 41/100, Loss: 0.9107, LR: 0.001000
Epoch 42/100, Loss: 0.9105, LR: 0.001000
Epoch 43/100, Loss: 0.9105, LR: 0.001000
Epoch 44/100, Loss: 0.9104, LR: 0.001000
Epoch 45/100, Loss: 0.9105, LR: 0.001000
Epoch 46/100, Loss: 0.9107, LR: 0.001000
Epoch 47/100, Loss: 0.9105, LR: 0.001000
Epoch 48/100, Loss: 0.9106, LR: 0.001000
Epoch 49/100, Loss: 0.9111, LR: 0.001000
Epoch 50/100, Loss: 0.9115, LR: 0.000500
Epoch 51/100, Loss: 0.9110, LR: 0.000500
Epoch 52/100, Loss: 0.9103, LR: 0.000500
Epoch 53/100, Loss: 0.9103, LR: 0.000500
Epoch 54/100, Loss: 0.9100, LR: 0.000500
Epoch 55/100, Loss: 0.9101, LR: 0.000500
Epoch 56/100, Loss: 0.9101, LR: 0.000500
Epoch 57/100, Loss: 0.9101, LR: 0.000500
Epoch 58/100, Loss: 0.9102, LR: 0.000500
Epoch 59/100, Loss: 0.9103, LR: 0.000500
Epoch 60/100, Loss: 0.9101, LR: 0.000500
Epoch 61/100, Loss: 0.9103, LR: 0.000500
Epoch 62/100, Loss: 0.9105, LR: 0.000500
Epoch 63/100, Loss: 0.9128, LR: 0.000500
Epoch 64/100, Loss: 0.9119, LR: 0.000500
Epoch 65/100, Loss: 0.9112, LR: 0.000500
Epoch 66/100, Loss: 0.9104, LR: 0.000500
Epoch 67/100, Loss: 0.9102, LR: 0.000500
Epoch 68/100, Loss: 0.9100, LR: 0.000500
Epoch 69/100, Loss: 0.9100, LR: 0.000500
Epoch 70/100, Loss: 0.9099, LR: 0.000500
Epoch 71/100, Loss: 0.9098, LR: 0.000500
Epoch 72/100, Loss: 0.9100, LR: 0.000500
Epoch 73/100, Loss: 0.9100, LR: 0.000500
Epoch 74/100, Loss: 0.9101, LR: 0.000500
Epoch 75/100, Loss: 0.9101, LR: 0.000500
Epoch 76/100, Loss: 0.9101, LR: 0.000500
Epoch 77/100, Loss: 0.9101, LR: 0.000500
Epoch 78/100, Loss: 0.9101, LR: 0.000500
Epoch 79/100, Loss: 0.9113, LR: 0.000500
Epoch 80/100, Loss: 0.9113, LR: 0.000250
Epoch 81/100, Loss: 0.9102, LR: 0.000250
Epoch 82/100, Loss: 0.9103, LR: 0.000250
Epoch 83/100, Loss: 0.9098, LR: 0.000250
Epoch 84/100, Loss: 0.9098, LR: 0.000250
Epoch 85/100, Loss: 0.9100, LR: 0.000250
Epoch 86/100, Loss: 0.9099, LR: 0.000250
Epoch 87/100, Loss: 0.9098, LR: 0.000250
Epoch 88/100, Loss: 0.9097, LR: 0.000250
Epoch 89/100, Loss: 0.9098, LR: 0.000250
Epoch 90/100, Loss: 0.9098, LR: 0.000250
Epoch 91/100, Loss: 0.9097, LR: 0.000250
Epoch 92/100, Loss: 0.9099, LR: 0.000250
Epoch 93/100, Loss: 0.9098, LR: 0.000250
Epoch 94/100, Loss: 0.9096, LR: 0.000250
Epoch 95/100, Loss: 0.9099, LR: 0.000250
Epoch 96/100, Loss: 0.9098, LR: 0.000250
Epoch 97/100, Loss: 0.9098, LR: 0.000250
Epoch 98/100, Loss: 0.9098, LR: 0.000250
Epoch 99/100, Loss: 0.9097, LR: 0.000250
Epoch 100/100, Loss: 0.9098, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.475
Change BestPlayer
>> Model updated (win rate: 47.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.4
>> Cycle 5 completed. Checkpoint saved.

Train 6 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 32, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 8: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 4: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 1: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 5: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 7: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 6: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 6
Worker 2: 10/63 games completed
Worker 3: 20/63 games completed
Worker 8: 20/62 games completed
Worker 1: 20/63 games completed
Worker 4: 20/63 games completed
Worker 6: 20/62 games completed
Worker 7: 20/62 games completed
Worker 5: 20/62 games completed
Worker 2: 20/63 games completed
Worker 6: 30/62 games completed
Worker 8: 30/62 games completed
Worker 4: 30/63 games completed
Worker 3: 30/63 games completed
Worker 5: 30/62 games completed
Worker 7: 30/62 games completed
Worker 1: 30/63 games completed
Worker 2: 30/63 games completed
Worker 3: 40/63 games completed
Worker 8: 40/62 games completed
Worker 7: 40/62 games completed
Worker 5: 40/62 games completed
Worker 6: 40/62 games completed
Worker 4: 40/63 games completed
Worker 2: 40/63 games completed
Worker 1: 40/63 games completed
Worker 3: 50/63 games completed
Worker 5: 50/62 games completed
Worker 8: 50/62 games completed
Worker 6: 50/62 games completed
Worker 7: 50/62 games completed
Worker 4: 50/63 games completed
Worker 2: 50/63 games completed
Worker 1: 50/63 games completed
Worker 3: 60/63 games completed
Worker 8: 60/62 games completed
Worker 2: 60/63 games completed
Worker 6: 60/62 games completed
Worker 5: 60/62 games completed
Worker 7: 60/62 games completed
Worker 4: 60/63 games completed
Worker 1: 60/63 games completed
>> Collected 23978 training samples from 500 games
>> Saved to ./data/20251030165039.history
>> Train 6
>> Learning Rate: 0.001
Epoch 1/100, Loss: 0.9329, LR: 0.001000
Epoch 2/100, Loss: 0.9125, LR: 0.001000
Epoch 3/100, Loss: 0.9066, LR: 0.001000
Epoch 4/100, Loss: 0.9018, LR: 0.001000
Epoch 5/100, Loss: 0.9000, LR: 0.001000
Epoch 6/100, Loss: 0.9005, LR: 0.001000
Epoch 7/100, Loss: 0.9003, LR: 0.001000
Epoch 8/100, Loss: 0.9021, LR: 0.001000
Epoch 9/100, Loss: 0.9097, LR: 0.001000
Epoch 10/100, Loss: 0.9010, LR: 0.001000
Epoch 11/100, Loss: 0.8996, LR: 0.001000
Epoch 12/100, Loss: 0.8981, LR: 0.001000
Epoch 13/100, Loss: 0.8982, LR: 0.001000
Epoch 14/100, Loss: 0.8974, LR: 0.001000
Epoch 15/100, Loss: 0.8974, LR: 0.001000
Epoch 16/100, Loss: 0.8976, LR: 0.001000
Epoch 17/100, Loss: 0.8971, LR: 0.001000
Epoch 18/100, Loss: 0.8978, LR: 0.001000
Epoch 19/100, Loss: 0.8984, LR: 0.001000
Epoch 20/100, Loss: 0.8986, LR: 0.001000
Epoch 21/100, Loss: 0.8989, LR: 0.001000
Epoch 22/100, Loss: 0.9139, LR: 0.001000
Epoch 23/100, Loss: 0.9038, LR: 0.001000
Epoch 24/100, Loss: 0.9001, LR: 0.001000
Epoch 25/100, Loss: 0.8986, LR: 0.001000
Epoch 26/100, Loss: 0.8985, LR: 0.001000
Epoch 27/100, Loss: 0.8978, LR: 0.001000
Epoch 28/100, Loss: 0.8971, LR: 0.001000
Epoch 29/100, Loss: 0.8977, LR: 0.001000
Epoch 30/100, Loss: 0.8975, LR: 0.001000
Epoch 31/100, Loss: 0.8975, LR: 0.001000
Epoch 32/100, Loss: 0.8967, LR: 0.001000
Epoch 33/100, Loss: 0.8970, LR: 0.001000
Epoch 34/100, Loss: 0.8968, LR: 0.001000
Epoch 35/100, Loss: 0.8974, LR: 0.001000
Epoch 36/100, Loss: 0.8976, LR: 0.001000
Epoch 37/100, Loss: 0.8971, LR: 0.001000
Epoch 38/100, Loss: 0.8973, LR: 0.001000
Epoch 39/100, Loss: 0.8980, LR: 0.001000
Epoch 40/100, Loss: 0.8987, LR: 0.001000
Epoch 41/100, Loss: 0.9026, LR: 0.001000
Epoch 42/100, Loss: 0.9171, LR: 0.001000
Epoch 43/100, Loss: 0.9024, LR: 0.001000
Epoch 44/100, Loss: 0.9008, LR: 0.001000
Epoch 45/100, Loss: 0.9004, LR: 0.001000
Epoch 46/100, Loss: 0.9003, LR: 0.001000
Epoch 47/100, Loss: 0.9002, LR: 0.001000
Epoch 48/100, Loss: 0.8996, LR: 0.001000
Epoch 49/100, Loss: 0.9004, LR: 0.001000
Epoch 50/100, Loss: 0.8999, LR: 0.000500
Epoch 51/100, Loss: 0.8998, LR: 0.000500
Epoch 52/100, Loss: 0.8993, LR: 0.000500
Epoch 53/100, Loss: 0.8999, LR: 0.000500
Epoch 54/100, Loss: 0.9045, LR: 0.000500
Epoch 55/100, Loss: 0.9009, LR: 0.000500
Epoch 56/100, Loss: 0.9002, LR: 0.000500
Epoch 57/100, Loss: 0.8999, LR: 0.000500
Epoch 58/100, Loss: 0.8992, LR: 0.000500
Epoch 59/100, Loss: 0.8976, LR: 0.000500
Epoch 60/100, Loss: 0.8978, LR: 0.000500
Epoch 61/100, Loss: 0.8972, LR: 0.000500
Epoch 62/100, Loss: 0.8982, LR: 0.000500
Epoch 63/100, Loss: 0.8977, LR: 0.000500
Epoch 64/100, Loss: 0.8972, LR: 0.000500
Epoch 65/100, Loss: 0.8978, LR: 0.000500
Epoch 66/100, Loss: 0.8974, LR: 0.000500
Epoch 67/100, Loss: 0.8981, LR: 0.000500
Epoch 68/100, Loss: 0.8973, LR: 0.000500
Epoch 69/100, Loss: 0.8977, LR: 0.000500
Epoch 70/100, Loss: 0.8974, LR: 0.000500
Epoch 71/100, Loss: 0.8977, LR: 0.000500
Epoch 72/100, Loss: 0.8975, LR: 0.000500
Epoch 73/100, Loss: 0.8977, LR: 0.000500
Epoch 74/100, Loss: 0.8976, LR: 0.000500
Epoch 75/100, Loss: 0.8973, LR: 0.000500
Epoch 76/100, Loss: 0.8975, LR: 0.000500
Epoch 77/100, Loss: 0.8976, LR: 0.000500
Epoch 78/100, Loss: 0.8977, LR: 0.000500
Epoch 79/100, Loss: 0.8975, LR: 0.000500
Epoch 80/100, Loss: 0.8985, LR: 0.000250
Epoch 81/100, Loss: 0.8975, LR: 0.000250
Epoch 82/100, Loss: 0.8975, LR: 0.000250
Epoch 83/100, Loss: 0.8974, LR: 0.000250
Epoch 84/100, Loss: 0.8972, LR: 0.000250
Epoch 85/100, Loss: 0.8975, LR: 0.000250
Epoch 86/100, Loss: 0.8971, LR: 0.000250
Epoch 87/100, Loss: 0.8973, LR: 0.000250
Epoch 88/100, Loss: 0.8973, LR: 0.000250
Epoch 89/100, Loss: 0.8972, LR: 0.000250
Epoch 90/100, Loss: 0.8971, LR: 0.000250
Epoch 91/100, Loss: 0.8971, LR: 0.000250
Epoch 92/100, Loss: 0.8974, LR: 0.000250
Epoch 93/100, Loss: 0.8977, LR: 0.000250
Epoch 94/100, Loss: 0.8979, LR: 0.000250
Epoch 95/100, Loss: 0.8974, LR: 0.000250
Epoch 96/100, Loss: 0.8974, LR: 0.000250
Epoch 97/100, Loss: 0.8977, LR: 0.000250
Epoch 98/100, Loss: 0.8974, LR: 0.000250
Epoch 99/100, Loss: 0.8967, LR: 0.000250
Epoch 100/100, Loss: 0.8977, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.51
Change BestPlayer
>> Model updated (win rate: 51.0%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.5
>> Cycle 6 completed. Checkpoint saved.

Train 7 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 32, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 4070 Ti
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> Resuming from cycle 7

Train 7 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 8, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/62 games completed
Worker 8: 20/62 games completed
Worker 7: 20/62 games completed
Worker 5: 20/62 games completed
Worker 2: 20/63 games completed
Worker 1: 20/63 games completed
Worker 4: 20/63 games completed
Worker 3: 20/63 games completed
Worker 6: 20/62 games completed
Worker 7: 30/62 games completed
Worker 8: 30/62 games completed
Worker 2: 30/63 games completed
Worker 5: 30/62 games completed
Worker 4: 30/63 games completed
Worker 1: 30/63 games completed
Worker 3: 30/63 games completed
Worker 6: 30/62 games completed
Worker 7: 40/62 games completed
Worker 2: 40/63 games completed
Worker 8: 40/62 games completed
Worker 3: 40/63 games completed
Worker 5: 40/62 games completed
Worker 4: 40/63 games completed
Worker 1: 40/63 games completed
Worker 6: 40/62 games completed
Worker 3: 50/63 games completed
Worker 7: 50/62 games completed
Worker 6: 50/62 games completed
Worker 8: 50/62 games completed
Worker 5: 50/62 games completed
Worker 2: 50/63 games completed
Worker 4: 50/63 games completed
Worker 1: 50/63 games completed
Worker 3: 60/63 games completed
Worker 6: 60/62 games completed
Worker 7: 60/62 games completed
Worker 8: 60/62 games completed
Worker 2: 60/63 games completed
Worker 5: 60/62 games completed
Worker 1: 60/63 games completed
Worker 4: 60/63 games completed
>> Collected 28542 training samples from 500 games
>> Saved to ./data/20251030175611.history
>> Train 7
>> Learning Rate: 0.001
Epoch 1/100, Loss: 3.6255, LR: 0.001000
Epoch 2/100, Loss: 3.5227, LR: 0.001000
Epoch 3/100, Loss: 3.5036, LR: 0.001000
Epoch 4/100, Loss: 3.4925, LR: 0.001000
Epoch 5/100, Loss: 3.4860, LR: 0.001000
Epoch 6/100, Loss: 3.4816, LR: 0.001000
Epoch 7/100, Loss: 3.4793, LR: 0.001000
Epoch 8/100, Loss: 3.4786, LR: 0.001000
Epoch 9/100, Loss: 3.4769, LR: 0.001000
Epoch 10/100, Loss: 3.4752, LR: 0.001000
Epoch 11/100, Loss: 3.4747, LR: 0.001000
Epoch 12/100, Loss: 3.4739, LR: 0.001000
Epoch 13/100, Loss: 3.4730, LR: 0.001000
Epoch 14/100, Loss: 3.4724, LR: 0.001000
Epoch 15/100, Loss: 3.4715, LR: 0.001000
Epoch 16/100, Loss: 3.4710, LR: 0.001000
Epoch 17/100, Loss: 3.4706, LR: 0.001000
Epoch 18/100, Loss: 3.4702, LR: 0.001000
Epoch 19/100, Loss: 3.4701, LR: 0.001000
Epoch 20/100, Loss: 3.4703, LR: 0.001000
Epoch 21/100, Loss: 3.4696, LR: 0.001000
Epoch 22/100, Loss: 3.4694, LR: 0.001000
Epoch 23/100, Loss: 3.4687, LR: 0.001000
Epoch 24/100, Loss: 3.4680, LR: 0.001000
Epoch 25/100, Loss: 3.4683, LR: 0.001000
Epoch 26/100, Loss: 3.4679, LR: 0.001000
Epoch 27/100, Loss: 3.4674, LR: 0.001000
Epoch 28/100, Loss: 3.4676, LR: 0.001000
Epoch 29/100, Loss: 3.4676, LR: 0.001000
Epoch 30/100, Loss: 3.4668, LR: 0.001000
Epoch 31/100, Loss: 3.4666, LR: 0.001000
Epoch 32/100, Loss: 3.4666, LR: 0.001000
Epoch 33/100, Loss: 3.4663, LR: 0.001000
Epoch 34/100, Loss: 3.4660, LR: 0.001000
Epoch 35/100, Loss: 3.4663, LR: 0.001000
Epoch 36/100, Loss: 3.4663, LR: 0.001000
Epoch 37/100, Loss: 3.4662, LR: 0.001000
Epoch 38/100, Loss: 3.4655, LR: 0.001000
Epoch 39/100, Loss: 3.4653, LR: 0.001000
Epoch 40/100, Loss: 3.4656, LR: 0.001000
Epoch 41/100, Loss: 3.4655, LR: 0.001000
Epoch 42/100, Loss: 3.4655, LR: 0.001000
Epoch 43/100, Loss: 3.4652, LR: 0.001000
Epoch 44/100, Loss: 3.4645, LR: 0.001000
Epoch 45/100, Loss: 3.4648, LR: 0.001000
Epoch 46/100, Loss: 3.4647, LR: 0.001000
Epoch 47/100, Loss: 3.4645, LR: 0.001000
Epoch 48/100, Loss: 3.4646, LR: 0.001000
Epoch 49/100, Loss: 3.4642, LR: 0.001000
Epoch 50/100, Loss: 3.4649, LR: 0.000500
Epoch 51/100, Loss: 3.4630, LR: 0.000500
Epoch 52/100, Loss: 3.4615, LR: 0.000500
Epoch 53/100, Loss: 3.4611, LR: 0.000500
Epoch 54/100, Loss: 3.4609, LR: 0.000500
Epoch 55/100, Loss: 3.4609, LR: 0.000500
Epoch 56/100, Loss: 3.4610, LR: 0.000500
Epoch 57/100, Loss: 3.4611, LR: 0.000500
Epoch 58/100, Loss: 3.4613, LR: 0.000500
Epoch 59/100, Loss: 3.4614, LR: 0.000500
Epoch 60/100, Loss: 3.4617, LR: 0.000500
Epoch 61/100, Loss: 3.4616, LR: 0.000500
Epoch 62/100, Loss: 3.4616, LR: 0.000500
Epoch 63/100, Loss: 3.4617, LR: 0.000500
Epoch 64/100, Loss: 3.4615, LR: 0.000500
Epoch 65/100, Loss: 3.4613, LR: 0.000500
Epoch 66/100, Loss: 3.4614, LR: 0.000500
Epoch 67/100, Loss: 3.4614, LR: 0.000500
Epoch 68/100, Loss: 3.4615, LR: 0.000500
Epoch 69/100, Loss: 3.4615, LR: 0.000500
Epoch 70/100, Loss: 3.4614, LR: 0.000500
Epoch 71/100, Loss: 3.4613, LR: 0.000500
Epoch 72/100, Loss: 3.4614, LR: 0.000500
Epoch 73/100, Loss: 3.4616, LR: 0.000500
Epoch 74/100, Loss: 3.4614, LR: 0.000500
Epoch 75/100, Loss: 3.4612, LR: 0.000500
Epoch 76/100, Loss: 3.4611, LR: 0.000500
Epoch 77/100, Loss: 3.4613, LR: 0.000500
Epoch 78/100, Loss: 3.4613, LR: 0.000500
Epoch 79/100, Loss: 3.4612, LR: 0.000500
Epoch 80/100, Loss: 3.4612, LR: 0.000250
Epoch 81/100, Loss: 3.4608, LR: 0.000250
Epoch 82/100, Loss: 3.4604, LR: 0.000250
Epoch 83/100, Loss: 3.4602, LR: 0.000250
Epoch 84/100, Loss: 3.4602, LR: 0.000250
Epoch 85/100, Loss: 3.4602, LR: 0.000250
Epoch 86/100, Loss: 3.4602, LR: 0.000250
Epoch 87/100, Loss: 3.4603, LR: 0.000250
Epoch 88/100, Loss: 3.4603, LR: 0.000250
Epoch 89/100, Loss: 3.4604, LR: 0.000250
Epoch 90/100, Loss: 3.4604, LR: 0.000250
Epoch 91/100, Loss: 3.4604, LR: 0.000250
Epoch 92/100, Loss: 3.4604, LR: 0.000250
Epoch 93/100, Loss: 3.4604, LR: 0.000250
Epoch 94/100, Loss: 3.4604, LR: 0.000250
Epoch 95/100, Loss: 3.4604, LR: 0.000250
Epoch 96/100, Loss: 3.4604, LR: 0.000250
Epoch 97/100, Loss: 3.4604, LR: 0.000250
Epoch 98/100, Loss: 3.4604, LR: 0.000250
Epoch 99/100, Loss: 3.4604, LR: 0.000250
Epoch 100/100, Loss: 3.4604, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.52
Change BestPlayer
>> Model updated (win rate: 52.0%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.25
>> Cycle 7 completed. Checkpoint saved.

Train 8 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 8, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/63 games completed
Worker 5: 20/62 games completed
Worker 7: 20/62 games completed
Worker 6: 20/62 games completed
Worker 2: 20/63 games completed
Worker 8: 20/62 games completed
Worker 4: 20/63 games completed
Worker 1: 20/63 games completed
Worker 3: 20/63 games completed
Worker 5: 30/62 games completed
Worker 7: 30/62 games completed
Worker 2: 30/63 games completed
Worker 8: 30/62 games completed
Worker 6: 30/62 games completed
Worker 1: 30/63 games completed
Worker 4: 30/63 games completed
Worker 3: 30/63 games completed
Worker 5: 40/62 games completed
Worker 7: 40/62 games completed
Worker 8: 40/62 games completed
Worker 2: 40/63 games completed
Worker 6: 40/62 games completed
Worker 1: 40/63 games completed
Worker 4: 40/63 games completed
Worker 3: 40/63 games completed
Worker 5: 50/62 games completed
Worker 8: 50/62 games completed
Worker 7: 50/62 games completed
Worker 5: 60/62 games completed
Worker 1: 50/63 games completed
Worker 6: 50/62 games completed
Worker 3: 50/63 games completed
Worker 4: 50/63 games completed
Worker 2: 50/63 games completed
Worker 8: 60/62 games completed
Worker 7: 60/62 games completed
Worker 1: 60/63 games completed
Worker 3: 60/63 games completed
Worker 2: 60/63 games completed
Worker 6: 60/62 games completed
Worker 4: 60/63 games completed
>> Collected 28731 training samples from 500 games
>> Saved to ./data/20251030182856.history
>> Train 8
>> Learning Rate: 0.001
Epoch 1/100, Loss: 3.4971, LR: 0.001000
Epoch 2/100, Loss: 3.4832, LR: 0.001000
Epoch 3/100, Loss: 3.4761, LR: 0.001000
Epoch 4/100, Loss: 3.4724, LR: 0.001000
Epoch 5/100, Loss: 3.4720, LR: 0.001000
Epoch 6/100, Loss: 3.4692, LR: 0.001000
Epoch 7/100, Loss: 3.4696, LR: 0.001000
Epoch 8/100, Loss: 3.4682, LR: 0.001000
Epoch 9/100, Loss: 3.4679, LR: 0.001000
Epoch 10/100, Loss: 3.4687, LR: 0.001000
Epoch 11/100, Loss: 3.4683, LR: 0.001000
Epoch 12/100, Loss: 3.4684, LR: 0.001000
Epoch 13/100, Loss: 3.4678, LR: 0.001000
Epoch 14/100, Loss: 3.4680, LR: 0.001000
Epoch 15/100, Loss: 3.4675, LR: 0.001000
Epoch 16/100, Loss: 3.4680, LR: 0.001000
Epoch 17/100, Loss: 3.4682, LR: 0.001000
Epoch 18/100, Loss: 3.4679, LR: 0.001000
Epoch 19/100, Loss: 3.4684, LR: 0.001000
Epoch 20/100, Loss: 3.4677, LR: 0.001000
Epoch 21/100, Loss: 3.4713, LR: 0.001000
Epoch 22/100, Loss: 3.4688, LR: 0.001000
Epoch 23/100, Loss: 3.4670, LR: 0.001000
Epoch 24/100, Loss: 3.4669, LR: 0.001000
Epoch 25/100, Loss: 3.4660, LR: 0.001000
Epoch 26/100, Loss: 3.4647, LR: 0.001000
Epoch 27/100, Loss: 3.4665, LR: 0.001000
Epoch 28/100, Loss: 3.4656, LR: 0.001000
Epoch 29/100, Loss: 3.4669, LR: 0.001000
Epoch 30/100, Loss: 3.4664, LR: 0.001000
Epoch 31/100, Loss: 3.4671, LR: 0.001000
Epoch 32/100, Loss: 3.4668, LR: 0.001000
Epoch 33/100, Loss: 3.4668, LR: 0.001000
Epoch 34/100, Loss: 3.4651, LR: 0.001000
Epoch 35/100, Loss: 3.4647, LR: 0.001000
Epoch 36/100, Loss: 3.4653, LR: 0.001000
Epoch 37/100, Loss: 3.4667, LR: 0.001000
Epoch 38/100, Loss: 3.4666, LR: 0.001000
Epoch 39/100, Loss: 3.4654, LR: 0.001000
Epoch 40/100, Loss: 3.4661, LR: 0.001000
Epoch 41/100, Loss: 3.4666, LR: 0.001000
Epoch 42/100, Loss: 3.4668, LR: 0.001000
Epoch 43/100, Loss: 3.4667, LR: 0.001000
Epoch 44/100, Loss: 3.4671, LR: 0.001000
Epoch 45/100, Loss: 3.4666, LR: 0.001000
Epoch 46/100, Loss: 3.4653, LR: 0.001000
Epoch 47/100, Loss: 3.4653, LR: 0.001000
Epoch 48/100, Loss: 3.4639, LR: 0.001000
Epoch 49/100, Loss: 3.4635, LR: 0.001000
Epoch 50/100, Loss: 3.4649, LR: 0.000500
Epoch 51/100, Loss: 3.4650, LR: 0.000500
Epoch 52/100, Loss: 3.4640, LR: 0.000500
Epoch 53/100, Loss: 3.4641, LR: 0.000500
Epoch 54/100, Loss: 3.4634, LR: 0.000500
Epoch 55/100, Loss: 3.4634, LR: 0.000500
Epoch 56/100, Loss: 3.4626, LR: 0.000500
Epoch 57/100, Loss: 3.4639, LR: 0.000500
Epoch 58/100, Loss: 3.4641, LR: 0.000500
Epoch 59/100, Loss: 3.4636, LR: 0.000500
Epoch 60/100, Loss: 3.4646, LR: 0.000500
Epoch 61/100, Loss: 3.4638, LR: 0.000500
Epoch 62/100, Loss: 3.4639, LR: 0.000500
Epoch 63/100, Loss: 3.4634, LR: 0.000500
Epoch 64/100, Loss: 3.4638, LR: 0.000500
Epoch 65/100, Loss: 3.4650, LR: 0.000500
Epoch 66/100, Loss: 3.4641, LR: 0.000500
Epoch 67/100, Loss: 3.4657, LR: 0.000500
Epoch 68/100, Loss: 3.4647, LR: 0.000500
Epoch 69/100, Loss: 3.4641, LR: 0.000500
Epoch 70/100, Loss: 3.4637, LR: 0.000500
Epoch 71/100, Loss: 3.4638, LR: 0.000500
Epoch 72/100, Loss: 3.4627, LR: 0.000500
Epoch 73/100, Loss: 3.4640, LR: 0.000500
Epoch 74/100, Loss: 3.4645, LR: 0.000500
Epoch 75/100, Loss: 3.4649, LR: 0.000500
Epoch 76/100, Loss: 3.4646, LR: 0.000500
Epoch 77/100, Loss: 3.4649, LR: 0.000500
Epoch 78/100, Loss: 3.4656, LR: 0.000500
Epoch 79/100, Loss: 3.4637, LR: 0.000500
Epoch 80/100, Loss: 3.4630, LR: 0.000250
Epoch 81/100, Loss: 3.4643, LR: 0.000250
Epoch 82/100, Loss: 3.4645, LR: 0.000250
Epoch 83/100, Loss: 3.4640, LR: 0.000250
Epoch 84/100, Loss: 3.4632, LR: 0.000250
Epoch 85/100, Loss: 3.4630, LR: 0.000250
Epoch 86/100, Loss: 3.4623, LR: 0.000250
Epoch 87/100, Loss: 3.4637, LR: 0.000250
Epoch 88/100, Loss: 3.4637, LR: 0.000250
Epoch 89/100, Loss: 3.4632, LR: 0.000250
Epoch 90/100, Loss: 3.4631, LR: 0.000250
Epoch 91/100, Loss: 3.4633, LR: 0.000250
Epoch 92/100, Loss: 3.4643, LR: 0.000250
Epoch 93/100, Loss: 3.4625, LR: 0.000250
Epoch 94/100, Loss: 3.4625, LR: 0.000250
Epoch 95/100, Loss: 3.4640, LR: 0.000250
Epoch 96/100, Loss: 3.4631, LR: 0.000250
Epoch 97/100, Loss: 3.4636, LR: 0.000250
Epoch 98/100, Loss: 3.4643, LR: 0.000250
Epoch 99/100, Loss: 3.4642, LR: 0.000250
Epoch 100/100, Loss: 3.4628, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.445
Change BestPlayer
>> Model updated (win rate: 44.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.45
>> Cycle 8 completed. Checkpoint saved.

Train 9 ====================================
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 500, Games per worker: 62
>> MCTS batch size: 8, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/63 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/62 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/63 games completed
Worker 8: 20/62 games completed
Worker 6: 20/62 games completed
Worker 3: 20/63 games completed
Worker 1: 20/63 games completed
Worker 5: 20/62 games completed
Worker 7: 20/62 games completed
Worker 4: 20/63 games completed
Worker 2: 20/63 games completed
Worker 8: 30/62 games completed
Worker 3: 30/63 games completed
Worker 6: 30/62 games completed
Worker 1: 30/63 games completed
Worker 7: 30/62 games completed
Worker 5: 30/62 games completed
Worker 4: 30/63 games completed
Worker 2: 30/63 games completed
Worker 8: 40/62 games completed
Worker 6: 40/62 games completed
Worker 3: 40/63 games completed
Worker 7: 40/62 games completed
Worker 1: 40/63 games completed
Worker 5: 40/62 games completed
Worker 8: 50/62 games completed
Worker 4: 40/63 games completed
Worker 2: 40/63 games completed
Worker 8: 60/62 games completed
Worker 6: 50/62 games completed
Worker 3: 50/63 games completed
Worker 7: 50/62 games completed
Worker 1: 50/63 games completed
Worker 5: 50/62 games completed
Worker 4: 50/63 games completed
Worker 2: 50/63 games completed
Worker 6: 60/62 games completed
Worker 7: 60/62 games completed
Worker 3: 60/63 games completed
Worker 1: 60/63 games completed
Worker 2: 60/63 games completed
Worker 5: 60/62 games completed
Worker 4: 60/63 games completed
>> Collected 28760 training samples from 500 games
>> Saved to ./data/20251030190214.history
>> Train 9
>> Learning Rate: 0.001
Epoch 1/100, Loss: 3.4707, LR: 0.001000
Epoch 2/100, Loss: 3.4604, LR: 0.001000
Epoch 3/100, Loss: 3.4550, LR: 0.001000
Epoch 4/100, Loss: 3.4508, LR: 0.001000
Epoch 5/100, Loss: 3.4501, LR: 0.001000
Epoch 6/100, Loss: 3.4484, LR: 0.001000
Epoch 7/100, Loss: 3.4483, LR: 0.001000
Epoch 8/100, Loss: 3.4478, LR: 0.001000
Epoch 9/100, Loss: 3.4476, LR: 0.001000
Epoch 10/100, Loss: 3.4473, LR: 0.001000
Epoch 11/100, Loss: 3.4471, LR: 0.001000
Epoch 12/100, Loss: 3.4473, LR: 0.001000
Epoch 13/100, Loss: 3.4475, LR: 0.001000
Epoch 14/100, Loss: 3.4481, LR: 0.001000
Epoch 15/100, Loss: 3.4500, LR: 0.001000
Epoch 16/100, Loss: 3.4488, LR: 0.001000
Epoch 17/100, Loss: 3.4471, LR: 0.001000
Epoch 18/100, Loss: 3.4468, LR: 0.001000
Epoch 19/100, Loss: 3.4464, LR: 0.001000
Epoch 20/100, Loss: 3.4463, LR: 0.001000
Epoch 21/100, Loss: 3.4465, LR: 0.001000
Epoch 22/100, Loss: 3.4470, LR: 0.001000
Epoch 23/100, Loss: 3.4462, LR: 0.001000
Epoch 24/100, Loss: 3.4463, LR: 0.001000
Epoch 25/100, Loss: 3.4460, LR: 0.001000
Epoch 26/100, Loss: 3.4468, LR: 0.001000
Epoch 27/100, Loss: 3.4467, LR: 0.001000
Epoch 28/100, Loss: 3.4463, LR: 0.001000
Epoch 29/100, Loss: 3.4460, LR: 0.001000
Epoch 30/100, Loss: 3.4469, LR: 0.001000
Epoch 31/100, Loss: 3.4455, LR: 0.001000
Epoch 32/100, Loss: 3.4463, LR: 0.001000
Epoch 33/100, Loss: 3.4455, LR: 0.001000
Epoch 34/100, Loss: 3.4463, LR: 0.001000
Epoch 35/100, Loss: 3.4459, LR: 0.001000
Epoch 36/100, Loss: 3.4475, LR: 0.001000
Epoch 37/100, Loss: 3.4489, LR: 0.001000
Epoch 38/100, Loss: 3.4494, LR: 0.001000
Epoch 39/100, Loss: 3.4461, LR: 0.001000
Epoch 40/100, Loss: 3.4455, LR: 0.001000
Epoch 41/100, Loss: 3.4458, LR: 0.001000
Epoch 42/100, Loss: 3.4455, LR: 0.001000
Epoch 43/100, Loss: 3.4463, LR: 0.001000
Epoch 44/100, Loss: 3.4452, LR: 0.001000
Epoch 45/100, Loss: 3.4453, LR: 0.001000
Epoch 46/100, Loss: 3.4462, LR: 0.001000
Epoch 47/100, Loss: 3.4466, LR: 0.001000
Epoch 48/100, Loss: 3.4467, LR: 0.001000
Epoch 49/100, Loss: 3.4456, LR: 0.001000
Epoch 50/100, Loss: 3.4454, LR: 0.000500
Epoch 51/100, Loss: 3.4455, LR: 0.000500
Epoch 52/100, Loss: 3.4449, LR: 0.000500
Epoch 53/100, Loss: 3.4452, LR: 0.000500
Epoch 54/100, Loss: 3.4445, LR: 0.000500
Epoch 55/100, Loss: 3.4445, LR: 0.000500
Epoch 56/100, Loss: 3.4444, LR: 0.000500
Epoch 57/100, Loss: 3.4445, LR: 0.000500
Epoch 58/100, Loss: 3.4446, LR: 0.000500
Epoch 59/100, Loss: 3.4447, LR: 0.000500
Epoch 60/100, Loss: 3.4448, LR: 0.000500
Epoch 61/100, Loss: 3.4449, LR: 0.000500
Epoch 62/100, Loss: 3.4443, LR: 0.000500
Epoch 63/100, Loss: 3.4445, LR: 0.000500
Epoch 64/100, Loss: 3.4445, LR: 0.000500
Epoch 65/100, Loss: 3.4452, LR: 0.000500
Epoch 66/100, Loss: 3.4449, LR: 0.000500
Epoch 67/100, Loss: 3.4443, LR: 0.000500
Epoch 68/100, Loss: 3.4446, LR: 0.000500
Epoch 69/100, Loss: 3.4444, LR: 0.000500
Epoch 70/100, Loss: 3.4448, LR: 0.000500
Epoch 71/100, Loss: 3.4445, LR: 0.000500
Epoch 72/100, Loss: 3.4445, LR: 0.000500
Epoch 73/100, Loss: 3.4461, LR: 0.000500
Epoch 74/100, Loss: 3.4454, LR: 0.000500
Epoch 75/100, Loss: 3.4453, LR: 0.000500
Epoch 76/100, Loss: 3.4451, LR: 0.000500
Epoch 77/100, Loss: 3.4440, LR: 0.000500
Epoch 78/100, Loss: 3.4444, LR: 0.000500
Epoch 79/100, Loss: 3.4447, LR: 0.000500
Epoch 80/100, Loss: 3.4445, LR: 0.000250
Epoch 81/100, Loss: 3.4446, LR: 0.000250
Epoch 82/100, Loss: 3.4446, LR: 0.000250
Epoch 83/100, Loss: 3.4444, LR: 0.000250
Epoch 84/100, Loss: 3.4446, LR: 0.000250
Epoch 85/100, Loss: 3.4443, LR: 0.000250
Epoch 86/100, Loss: 3.4442, LR: 0.000250
Epoch 87/100, Loss: 3.4441, LR: 0.000250
Epoch 88/100, Loss: 3.4446, LR: 0.000250
Epoch 89/100, Loss: 3.4445, LR: 0.000250
Epoch 90/100, Loss: 3.4442, LR: 0.000250
Epoch 91/100, Loss: 3.4446, LR: 0.000250
Epoch 92/100, Loss: 3.4445, LR: 0.000250
Epoch 93/100, Loss: 3.4448, LR: 0.000250
Epoch 94/100, Loss: 3.4444, LR: 0.000250
Epoch 95/100, Loss: 3.4445, LR: 0.000250
Epoch 96/100, Loss: 3.4439, LR: 0.000250
Epoch 97/100, Loss: 3.4448, LR: 0.000250
Epoch 98/100, Loss: 3.4451, LR: 0.000250
Epoch 99/100, Loss: 3.4451, LR: 0.000250
Epoch 100/100, Loss: 3.4444, LR: 0.000250
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.45
Change BestPlayer
>> Model updated (win rate: 45.0%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.55
>> Cycle 9 completed. Checkpoint saved.

Train 10 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 1000, Games per worker: 125
>> MCTS batch size: 8, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/125 games completed
Worker 2: 20/125 games completed
Worker 3: 20/125 games completed
Worker 5: 20/125 games completed
Worker 7: 20/125 games completed
Worker 1: 20/125 games completed
Worker 8: 20/125 games completed
Worker 6: 20/125 games completed
Worker 4: 20/125 games completed
Worker 2: 30/125 games completed
Worker 3: 30/125 games completed
Worker 7: 30/125 games completed
Worker 5: 30/125 games completed
Worker 1: 30/125 games completed
Worker 8: 30/125 games completed
Worker 4: 30/125 games completed
Worker 6: 30/125 games completed
Worker 2: 40/125 games completed
Worker 3: 40/125 games completed
Worker 5: 40/125 games completed
Worker 7: 40/125 games completed
Worker 1: 40/125 games completed
Worker 8: 40/125 games completed
Worker 4: 40/125 games completed
Worker 6: 40/125 games completed
Worker 2: 50/125 games completed
Worker 3: 50/125 games completed
Worker 1: 50/125 games completed
Worker 7: 50/125 games completed
Worker 5: 50/125 games completed
Worker 4: 50/125 games completed
Worker 6: 50/125 games completed
Worker 8: 50/125 games completed
Worker 2: 60/125 games completed
Worker 1: 60/125 games completed
Worker 3: 60/125 games completed
Worker 5: 60/125 games completed
Worker 7: 60/125 games completed
Worker 4: 60/125 games completed
Worker 6: 60/125 games completed
Worker 8: 60/125 games completed
Worker 2: 70/125 games completed
Worker 1: 70/125 games completed
Worker 3: 70/125 games completed
Worker 7: 70/125 games completed
Worker 5: 70/125 games completed
Worker 4: 70/125 games completed
Worker 6: 70/125 games completed
Worker 8: 70/125 games completed
Worker 2: 80/125 games completed
Worker 1: 80/125 games completed
Worker 3: 80/125 games completed
Worker 7: 80/125 games completed
Worker 5: 80/125 games completed
Worker 6: 80/125 games completed
Worker 4: 80/125 games completed
Worker 8: 80/125 games completed
Worker 1: 90/125 games completed
Worker 2: 90/125 games completed
Worker 3: 90/125 games completed
Worker 7: 90/125 games completed
Worker 5: 90/125 games completed
Worker 6: 90/125 games completed
Worker 4: 90/125 games completed
Worker 8: 90/125 games completed
Worker 1: 100/125 games completed
Worker 2: 100/125 games completed
Worker 3: 100/125 games completed
Worker 7: 100/125 games completed
Worker 5: 100/125 games completed
Worker 6: 100/125 games completed
Worker 8: 100/125 games completed
Worker 4: 100/125 games completed
Worker 1: 110/125 games completed
Worker 2: 110/125 games completed
Worker 3: 110/125 games completed
Worker 7: 110/125 games completed
Worker 5: 110/125 games completed
Worker 6: 110/125 games completed
Worker 4: 110/125 games completed
Worker 8: 110/125 games completed
Worker 1: 120/125 games completed
Worker 2: 120/125 games completed
Worker 3: 120/125 games completed
Worker 5: 120/125 games completed
Worker 7: 120/125 games completed
Worker 6: 120/125 games completed
Worker 8: 120/125 games completed
Worker 4: 120/125 games completed
>> Collected 55356 training samples from 1000 games
>> Saved to ./data/20251030201622.history
>> Train 10
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.1866, LR: 0.000500
Epoch 2/100, Loss: 3.1488, LR: 0.000500
Epoch 3/100, Loss: 3.1381, LR: 0.000500
Epoch 4/100, Loss: 3.1304, LR: 0.000500
Epoch 5/100, Loss: 3.1268, LR: 0.000500
Epoch 6/100, Loss: 3.1241, LR: 0.000500
Epoch 7/100, Loss: 3.1228, LR: 0.000500
Epoch 8/100, Loss: 3.1207, LR: 0.000500
Epoch 9/100, Loss: 3.1201, LR: 0.000500
Epoch 10/100, Loss: 3.1198, LR: 0.000500
Epoch 11/100, Loss: 3.1197, LR: 0.000500
Epoch 12/100, Loss: 3.1194, LR: 0.000500
Epoch 13/100, Loss: 3.1195, LR: 0.000500
Epoch 14/100, Loss: 3.1221, LR: 0.000500
Epoch 15/100, Loss: 3.1213, LR: 0.000500
Epoch 16/100, Loss: 3.1196, LR: 0.000500
Epoch 17/100, Loss: 3.1190, LR: 0.000500
Epoch 18/100, Loss: 3.1187, LR: 0.000500
Epoch 19/100, Loss: 3.1179, LR: 0.000500
Epoch 20/100, Loss: 3.1188, LR: 0.000500
Epoch 21/100, Loss: 3.1183, LR: 0.000500
Epoch 22/100, Loss: 3.1189, LR: 0.000500
Epoch 23/100, Loss: 3.1183, LR: 0.000500
Epoch 24/100, Loss: 3.1185, LR: 0.000500
Epoch 25/100, Loss: 3.1183, LR: 0.000500
Epoch 26/100, Loss: 3.1216, LR: 0.000500
Epoch 27/100, Loss: 3.1208, LR: 0.000500
Epoch 28/100, Loss: 3.1194, LR: 0.000500
Epoch 29/100, Loss: 3.1183, LR: 0.000500
Epoch 30/100, Loss: 3.1175, LR: 0.000500
Epoch 31/100, Loss: 3.1180, LR: 0.000500
Epoch 32/100, Loss: 3.1175, LR: 0.000500
Epoch 33/100, Loss: 3.1177, LR: 0.000500
Epoch 34/100, Loss: 3.1180, LR: 0.000500
Epoch 35/100, Loss: 3.1179, LR: 0.000500
Epoch 36/100, Loss: 3.1180, LR: 0.000500
Epoch 37/100, Loss: 3.1179, LR: 0.000500
Epoch 38/100, Loss: 3.1175, LR: 0.000500
Epoch 39/100, Loss: 3.1181, LR: 0.000500
Epoch 40/100, Loss: 3.1193, LR: 0.000500
Epoch 41/100, Loss: 3.1206, LR: 0.000500
Epoch 42/100, Loss: 3.1181, LR: 0.000500
Epoch 43/100, Loss: 3.1183, LR: 0.000500
Epoch 44/100, Loss: 3.1165, LR: 0.000500
Epoch 45/100, Loss: 3.1179, LR: 0.000500
Epoch 46/100, Loss: 3.1181, LR: 0.000500
Epoch 47/100, Loss: 3.1178, LR: 0.000500
Epoch 48/100, Loss: 3.1180, LR: 0.000500
Epoch 49/100, Loss: 3.1174, LR: 0.000500
Epoch 50/100, Loss: 3.1187, LR: 0.000250
Epoch 51/100, Loss: 3.1197, LR: 0.000250
Epoch 52/100, Loss: 3.1181, LR: 0.000250
Epoch 53/100, Loss: 3.1169, LR: 0.000250
Epoch 54/100, Loss: 3.1171, LR: 0.000250
Epoch 55/100, Loss: 3.1177, LR: 0.000250
Epoch 56/100, Loss: 3.1169, LR: 0.000250
Epoch 57/100, Loss: 3.1173, LR: 0.000250
Epoch 58/100, Loss: 3.1170, LR: 0.000250
Epoch 59/100, Loss: 3.1173, LR: 0.000250
Epoch 60/100, Loss: 3.1178, LR: 0.000250
Epoch 61/100, Loss: 3.1169, LR: 0.000250
Epoch 62/100, Loss: 3.1175, LR: 0.000250
Epoch 63/100, Loss: 3.1175, LR: 0.000250
Epoch 64/100, Loss: 3.1167, LR: 0.000250
Epoch 65/100, Loss: 3.1171, LR: 0.000250
Epoch 66/100, Loss: 3.1169, LR: 0.000250
Epoch 67/100, Loss: 3.1170, LR: 0.000250
Epoch 68/100, Loss: 3.1174, LR: 0.000250
Epoch 69/100, Loss: 3.1177, LR: 0.000250
Epoch 70/100, Loss: 3.1174, LR: 0.000250
Epoch 71/100, Loss: 3.1170, LR: 0.000250
Epoch 72/100, Loss: 3.1173, LR: 0.000250
Epoch 73/100, Loss: 3.1165, LR: 0.000250
Epoch 74/100, Loss: 3.1179, LR: 0.000250
Epoch 75/100, Loss: 3.1171, LR: 0.000250
Epoch 76/100, Loss: 3.1167, LR: 0.000250
Epoch 77/100, Loss: 3.1174, LR: 0.000250
Epoch 78/100, Loss: 3.1169, LR: 0.000250
Epoch 79/100, Loss: 3.1171, LR: 0.000250
Epoch 80/100, Loss: 3.1172, LR: 0.000125
Epoch 81/100, Loss: 3.1169, LR: 0.000125
Epoch 82/100, Loss: 3.1169, LR: 0.000125
Epoch 83/100, Loss: 3.1171, LR: 0.000125
Epoch 84/100, Loss: 3.1172, LR: 0.000125
Epoch 85/100, Loss: 3.1175, LR: 0.000125
Epoch 86/100, Loss: 3.1172, LR: 0.000125
Epoch 87/100, Loss: 3.1174, LR: 0.000125
Epoch 88/100, Loss: 3.1165, LR: 0.000125
Epoch 89/100, Loss: 3.1172, LR: 0.000125
Epoch 90/100, Loss: 3.1174, LR: 0.000125
Epoch 91/100, Loss: 3.1170, LR: 0.000125
Epoch 92/100, Loss: 3.1171, LR: 0.000125
Epoch 93/100, Loss: 3.1167, LR: 0.000125
Epoch 94/100, Loss: 3.1172, LR: 0.000125
Epoch 95/100, Loss: 3.1171, LR: 0.000125
Epoch 96/100, Loss: 3.1164, LR: 0.000125
Epoch 97/100, Loss: 3.1169, LR: 0.000125
Epoch 98/100, Loss: 3.1164, LR: 0.000125
Epoch 99/100, Loss: 3.1172, LR: 0.000125
Epoch 100/100, Loss: 3.1166, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.515
Change BestPlayer
>> Model updated (win rate: 51.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.3
>> Cycle 10 completed. Checkpoint saved.

Train 11 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 1000, Games per worker: 125
>> MCTS batch size: 8, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/125 games completed
Worker 7: 20/125 games completed
Worker 6: 20/125 games completed
Worker 4: 20/125 games completed
Worker 2: 20/125 games completed
Worker 3: 20/125 games completed
Worker 7: 30/125 games completed
Worker 1: 20/125 games completed
Worker 5: 20/125 games completed
Worker 8: 20/125 games completed
Worker 6: 30/125 games completed
Worker 7: 40/125 games completed
Worker 4: 30/125 games completed
Worker 3: 30/125 games completed
Worker 2: 30/125 games completed
Worker 1: 30/125 games completed
Worker 5: 30/125 games completed
Worker 8: 30/125 games completed
Worker 7: 50/125 games completed
Worker 6: 40/125 games completed
Worker 4: 40/125 games completed
Worker 3: 40/125 games completed
Worker 1: 40/125 games completed
Worker 7: 60/125 games completed
Worker 2: 40/125 games completed
Worker 5: 40/125 games completed
Worker 8: 40/125 games completed
Worker 6: 50/125 games completed
Worker 7: 70/125 games completed
Worker 4: 50/125 games completed
Worker 3: 50/125 games completed
Worker 1: 50/125 games completed
Worker 2: 50/125 games completed
Worker 5: 50/125 games completed
Worker 7: 80/125 games completed
Worker 8: 50/125 games completed
Worker 6: 60/125 games completed
Worker 3: 60/125 games completed
Worker 4: 60/125 games completed
Worker 1: 60/125 games completed
Worker 7: 90/125 games completed
Worker 2: 60/125 games completed
Worker 5: 60/125 games completed
Worker 6: 70/125 games completed
Worker 8: 60/125 games completed
Worker 3: 70/125 games completed
Worker 7: 100/125 games completed
Worker 4: 70/125 games completed
Worker 1: 70/125 games completed
Worker 2: 70/125 games completed
Worker 6: 80/125 games completed
Worker 5: 70/125 games completed
Worker 7: 110/125 games completed
Worker 8: 70/125 games completed
Worker 3: 80/125 games completed
Worker 4: 80/125 games completed
Worker 2: 80/125 games completed
Worker 7: 120/125 games completed
Worker 1: 80/125 games completed
Worker 6: 90/125 games completed
Worker 5: 80/125 games completed
Worker 8: 80/125 games completed
Worker 3: 90/125 games completed
Worker 4: 90/125 games completed
Worker 2: 90/125 games completed
Worker 6: 100/125 games completed
Worker 1: 90/125 games completed
Worker 5: 90/125 games completed
Worker 8: 90/125 games completed
Worker 3: 100/125 games completed
Worker 4: 100/125 games completed
Worker 2: 100/125 games completed
Worker 6: 110/125 games completed
Worker 1: 100/125 games completed
Worker 5: 100/125 games completed
Worker 8: 100/125 games completed
Worker 3: 110/125 games completed
Worker 4: 110/125 games completed
Worker 2: 110/125 games completed
Worker 6: 120/125 games completed
Worker 5: 110/125 games completed
Worker 1: 110/125 games completed
Worker 8: 110/125 games completed
Worker 3: 120/125 games completed
Worker 4: 120/125 games completed
Worker 2: 120/125 games completed
Worker 5: 120/125 games completed
Worker 1: 120/125 games completed
Worker 8: 120/125 games completed
>> Collected 55088 training samples from 1000 games
>> Saved to ./data/20251030212955.history
>> Train 11
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.1561, LR: 0.000500
Epoch 2/100, Loss: 3.1425, LR: 0.000500
Epoch 3/100, Loss: 3.1349, LR: 0.000500
Epoch 4/100, Loss: 3.1292, LR: 0.000500
Epoch 5/100, Loss: 3.1268, LR: 0.000500
Epoch 6/100, Loss: 3.1263, LR: 0.000500
Epoch 7/100, Loss: 3.1243, LR: 0.000500
Epoch 8/100, Loss: 3.1244, LR: 0.000500
Epoch 9/100, Loss: 3.1234, LR: 0.000500
Epoch 10/100, Loss: 3.1236, LR: 0.000500
Epoch 11/100, Loss: 3.1230, LR: 0.000500
Epoch 12/100, Loss: 3.1264, LR: 0.000500
Epoch 13/100, Loss: 3.1271, LR: 0.000500
Epoch 14/100, Loss: 3.1244, LR: 0.000500
Epoch 15/100, Loss: 3.1232, LR: 0.000500
Epoch 16/100, Loss: 3.1230, LR: 0.000500
Epoch 17/100, Loss: 3.1220, LR: 0.000500
Epoch 18/100, Loss: 3.1223, LR: 0.000500
Epoch 19/100, Loss: 3.1234, LR: 0.000500
Epoch 20/100, Loss: 3.1230, LR: 0.000500
Epoch 21/100, Loss: 3.1255, LR: 0.000500
Epoch 22/100, Loss: 3.1250, LR: 0.000500
Epoch 23/100, Loss: 3.1240, LR: 0.000500
Epoch 24/100, Loss: 3.1239, LR: 0.000500
Epoch 25/100, Loss: 3.1229, LR: 0.000500
Epoch 26/100, Loss: 3.1235, LR: 0.000500
Epoch 27/100, Loss: 3.1224, LR: 0.000500
Epoch 28/100, Loss: 3.1218, LR: 0.000500
Epoch 29/100, Loss: 3.1227, LR: 0.000500
Epoch 30/100, Loss: 3.1233, LR: 0.000500
Epoch 31/100, Loss: 3.1243, LR: 0.000500
Epoch 32/100, Loss: 3.1228, LR: 0.000500
Epoch 33/100, Loss: 3.1230, LR: 0.000500
Epoch 34/100, Loss: 3.1235, LR: 0.000500
Epoch 35/100, Loss: 3.1252, LR: 0.000500
Epoch 36/100, Loss: 3.1251, LR: 0.000500
Epoch 37/100, Loss: 3.1229, LR: 0.000500
Epoch 38/100, Loss: 3.1230, LR: 0.000500
Epoch 39/100, Loss: 3.1230, LR: 0.000500
Epoch 40/100, Loss: 3.1224, LR: 0.000500
Epoch 41/100, Loss: 3.1229, LR: 0.000500
Epoch 42/100, Loss: 3.1228, LR: 0.000500
Epoch 43/100, Loss: 3.1231, LR: 0.000500
Epoch 44/100, Loss: 3.1232, LR: 0.000500
Epoch 45/100, Loss: 3.1231, LR: 0.000500
Epoch 46/100, Loss: 3.1233, LR: 0.000500
Epoch 47/100, Loss: 3.1230, LR: 0.000500
Epoch 48/100, Loss: 3.1228, LR: 0.000500
Epoch 49/100, Loss: 3.1231, LR: 0.000500
Epoch 50/100, Loss: 3.1226, LR: 0.000250
Epoch 51/100, Loss: 3.1224, LR: 0.000250
Epoch 52/100, Loss: 3.1216, LR: 0.000250
Epoch 53/100, Loss: 3.1218, LR: 0.000250
Epoch 54/100, Loss: 3.1225, LR: 0.000250
Epoch 55/100, Loss: 3.1217, LR: 0.000250
Epoch 56/100, Loss: 3.1222, LR: 0.000250
Epoch 57/100, Loss: 3.1226, LR: 0.000250
Epoch 58/100, Loss: 3.1222, LR: 0.000250
Epoch 59/100, Loss: 3.1224, LR: 0.000250
Epoch 60/100, Loss: 3.1225, LR: 0.000250
Epoch 61/100, Loss: 3.1225, LR: 0.000250
Epoch 62/100, Loss: 3.1230, LR: 0.000250
Epoch 63/100, Loss: 3.1228, LR: 0.000250
Epoch 64/100, Loss: 3.1225, LR: 0.000250
Epoch 65/100, Loss: 3.1223, LR: 0.000250
Epoch 66/100, Loss: 3.1222, LR: 0.000250
Epoch 67/100, Loss: 3.1227, LR: 0.000250
Epoch 68/100, Loss: 3.1225, LR: 0.000250
Epoch 69/100, Loss: 3.1223, LR: 0.000250
Epoch 70/100, Loss: 3.1218, LR: 0.000250
Epoch 71/100, Loss: 3.1225, LR: 0.000250
Epoch 72/100, Loss: 3.1228, LR: 0.000250
Epoch 73/100, Loss: 3.1226, LR: 0.000250
Epoch 74/100, Loss: 3.1230, LR: 0.000250
Epoch 75/100, Loss: 3.1216, LR: 0.000250
Epoch 76/100, Loss: 3.1227, LR: 0.000250
Epoch 77/100, Loss: 3.1219, LR: 0.000250
Epoch 78/100, Loss: 3.1221, LR: 0.000250
Epoch 79/100, Loss: 3.1223, LR: 0.000250
Epoch 80/100, Loss: 3.1222, LR: 0.000125
Epoch 81/100, Loss: 3.1217, LR: 0.000125
Epoch 82/100, Loss: 3.1224, LR: 0.000125
Epoch 83/100, Loss: 3.1215, LR: 0.000125
Epoch 84/100, Loss: 3.1216, LR: 0.000125
Epoch 85/100, Loss: 3.1216, LR: 0.000125
Epoch 86/100, Loss: 3.1222, LR: 0.000125
Epoch 87/100, Loss: 3.1219, LR: 0.000125
Epoch 88/100, Loss: 3.1223, LR: 0.000125
Epoch 89/100, Loss: 3.1219, LR: 0.000125
Epoch 90/100, Loss: 3.1223, LR: 0.000125
Epoch 91/100, Loss: 3.1221, LR: 0.000125
Epoch 92/100, Loss: 3.1221, LR: 0.000125
Epoch 93/100, Loss: 3.1217, LR: 0.000125
Epoch 94/100, Loss: 3.1218, LR: 0.000125
Epoch 95/100, Loss: 3.1216, LR: 0.000125
Epoch 96/100, Loss: 3.1233, LR: 0.000125
Epoch 97/100, Loss: 3.1221, LR: 0.000125
Epoch 98/100, Loss: 3.1214, LR: 0.000125
Epoch 99/100, Loss: 3.1213, LR: 0.000125
Epoch 100/100, Loss: 3.1223, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.625
Change BestPlayer
>> Model updated (win rate: 62.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.5
>> Cycle 11 completed. Checkpoint saved.

Train 12 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 1000, Games per worker: 125
>> MCTS batch size: 8, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/125 games completed
Worker 1: 20/125 games completed
Worker 2: 20/125 games completed
Worker 8: 20/125 games completed
Worker 5: 20/125 games completed
Worker 3: 20/125 games completed
Worker 6: 20/125 games completed
Worker 4: 20/125 games completed
Worker 7: 20/125 games completed
Worker 1: 30/125 games completed
Worker 3: 30/125 games completed
Worker 2: 30/125 games completed
Worker 8: 30/125 games completed
Worker 6: 30/125 games completed
Worker 5: 30/125 games completed
Worker 4: 30/125 games completed
Worker 7: 30/125 games completed
Worker 1: 40/125 games completed
Worker 3: 40/125 games completed
Worker 6: 40/125 games completed
Worker 8: 40/125 games completed
Worker 2: 40/125 games completed
Worker 4: 40/125 games completed
Worker 5: 40/125 games completed
Worker 7: 40/125 games completed
Worker 3: 50/125 games completed
Worker 1: 50/125 games completed
Worker 4: 50/125 games completed
Worker 6: 50/125 games completed
Worker 2: 50/125 games completed
Worker 8: 50/125 games completed
Worker 5: 50/125 games completed
Worker 7: 50/125 games completed
Worker 3: 60/125 games completed
Worker 1: 60/125 games completed
Worker 6: 60/125 games completed
Worker 4: 60/125 games completed
Worker 2: 60/125 games completed
Worker 8: 60/125 games completed
Worker 5: 60/125 games completed
Worker 7: 60/125 games completed
Worker 3: 70/125 games completed
Worker 1: 70/125 games completed
Worker 6: 70/125 games completed
Worker 8: 70/125 games completed
Worker 2: 70/125 games completed
Worker 4: 70/125 games completed
Worker 5: 70/125 games completed
Worker 7: 70/125 games completed
Worker 3: 80/125 games completed
Worker 1: 80/125 games completed
Worker 6: 80/125 games completed
Worker 8: 80/125 games completed
Worker 2: 80/125 games completed
Worker 4: 80/125 games completed
Worker 5: 80/125 games completed
Worker 7: 80/125 games completed
Worker 1: 90/125 games completed
Worker 3: 90/125 games completed
Worker 6: 90/125 games completed
Worker 8: 90/125 games completed
Worker 2: 90/125 games completed
Worker 4: 90/125 games completed
Worker 5: 90/125 games completed
Worker 7: 90/125 games completed
Worker 1: 100/125 games completed
Worker 3: 100/125 games completed
Worker 6: 100/125 games completed
Worker 8: 100/125 games completed
Worker 2: 100/125 games completed
Worker 4: 100/125 games completed
Worker 7: 100/125 games completed
Worker 5: 100/125 games completed
Worker 1: 110/125 games completed
Worker 3: 110/125 games completed
Worker 6: 110/125 games completed
Worker 8: 110/125 games completed
Worker 2: 110/125 games completed
Worker 4: 110/125 games completed
Worker 7: 110/125 games completed
Worker 5: 110/125 games completed
Worker 3: 120/125 games completed
Worker 1: 120/125 games completed
Worker 6: 120/125 games completed
Worker 8: 120/125 games completed
Worker 2: 120/125 games completed
Worker 4: 120/125 games completed
Worker 7: 120/125 games completed
Worker 5: 120/125 games completed
>> Collected 55300 training samples from 1000 games
>> Saved to ./data/20251030224324.history
>> Train 12
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.1634, LR: 0.000500
Epoch 2/100, Loss: 3.1440, LR: 0.000500
Epoch 3/100, Loss: 3.1345, LR: 0.000500
Epoch 4/100, Loss: 3.1347, LR: 0.000500
Epoch 5/100, Loss: 3.1326, LR: 0.000500
Epoch 6/100, Loss: 3.1352, LR: 0.000500
Epoch 7/100, Loss: 3.1269, LR: 0.000500
Epoch 8/100, Loss: 3.1288, LR: 0.000500
Epoch 9/100, Loss: 3.1322, LR: 0.000500
Epoch 10/100, Loss: 3.1260, LR: 0.000500
Epoch 11/100, Loss: 3.1327, LR: 0.000500
Epoch 12/100, Loss: 3.1301, LR: 0.000500
Epoch 13/100, Loss: 3.1266, LR: 0.000500
Epoch 14/100, Loss: 3.1267, LR: 0.000500
Epoch 15/100, Loss: 3.1317, LR: 0.000500
Epoch 16/100, Loss: 3.1248, LR: 0.000500
Epoch 17/100, Loss: 3.1297, LR: 0.000500
Epoch 18/100, Loss: 3.1264, LR: 0.000500
Epoch 19/100, Loss: 3.1275, LR: 0.000500
Epoch 20/100, Loss: 3.1310, LR: 0.000500
Epoch 21/100, Loss: 3.1361, LR: 0.000500
Epoch 22/100, Loss: 3.1319, LR: 0.000500
Epoch 23/100, Loss: 3.1258, LR: 0.000500
Epoch 24/100, Loss: 3.1305, LR: 0.000500
Epoch 25/100, Loss: 3.1295, LR: 0.000500
Epoch 26/100, Loss: 3.1283, LR: 0.000500
Epoch 27/100, Loss: 3.1269, LR: 0.000500
Epoch 28/100, Loss: 3.1289, LR: 0.000500
Epoch 29/100, Loss: 3.1230, LR: 0.000500
Epoch 30/100, Loss: 3.1303, LR: 0.000500
Epoch 31/100, Loss: 3.1325, LR: 0.000500
Epoch 32/100, Loss: 3.1252, LR: 0.000500
Epoch 33/100, Loss: 3.1279, LR: 0.000500
Epoch 34/100, Loss: 3.1280, LR: 0.000500
Epoch 35/100, Loss: 3.1355, LR: 0.000500
Epoch 36/100, Loss: 3.1300, LR: 0.000500
Epoch 37/100, Loss: 3.1289, LR: 0.000500
Epoch 38/100, Loss: 3.1308, LR: 0.000500
Epoch 39/100, Loss: 3.1311, LR: 0.000500
Epoch 40/100, Loss: 3.1279, LR: 0.000500
Epoch 41/100, Loss: 3.1282, LR: 0.000500
Epoch 42/100, Loss: 3.1233, LR: 0.000500
Epoch 43/100, Loss: 3.1250, LR: 0.000500
Epoch 44/100, Loss: 3.1324, LR: 0.000500
Epoch 45/100, Loss: 3.1272, LR: 0.000500
Epoch 46/100, Loss: 3.1288, LR: 0.000500
Epoch 47/100, Loss: 3.1261, LR: 0.000500
Epoch 48/100, Loss: 3.1278, LR: 0.000500
Epoch 49/100, Loss: 3.1304, LR: 0.000500
Epoch 50/100, Loss: 3.1278, LR: 0.000250
Epoch 51/100, Loss: 3.1249, LR: 0.000250
Epoch 52/100, Loss: 3.1280, LR: 0.000250
Epoch 53/100, Loss: 3.1307, LR: 0.000250
Epoch 54/100, Loss: 3.1296, LR: 0.000250
Epoch 55/100, Loss: 3.1296, LR: 0.000250
Epoch 56/100, Loss: 3.1294, LR: 0.000250
Epoch 57/100, Loss: 3.1251, LR: 0.000250
Epoch 58/100, Loss: 3.1258, LR: 0.000250
Epoch 59/100, Loss: 3.1302, LR: 0.000250
Epoch 60/100, Loss: 3.1308, LR: 0.000250
Epoch 61/100, Loss: 3.1268, LR: 0.000250
Epoch 62/100, Loss: 3.1231, LR: 0.000250
Epoch 63/100, Loss: 3.1257, LR: 0.000250
Epoch 64/100, Loss: 3.1268, LR: 0.000250
Epoch 65/100, Loss: 3.1295, LR: 0.000250
Epoch 66/100, Loss: 3.1289, LR: 0.000250
Epoch 67/100, Loss: 3.1276, LR: 0.000250
Epoch 68/100, Loss: 3.1241, LR: 0.000250
Epoch 69/100, Loss: 3.1260, LR: 0.000250
Epoch 70/100, Loss: 3.1264, LR: 0.000250
Epoch 71/100, Loss: 3.1304, LR: 0.000250
Epoch 72/100, Loss: 3.1289, LR: 0.000250
Epoch 73/100, Loss: 3.1298, LR: 0.000250
Epoch 74/100, Loss: 3.1282, LR: 0.000250
Epoch 75/100, Loss: 3.1276, LR: 0.000250
Epoch 76/100, Loss: 3.1254, LR: 0.000250
Epoch 77/100, Loss: 3.1229, LR: 0.000250
Epoch 78/100, Loss: 3.1292, LR: 0.000250
Epoch 79/100, Loss: 3.1287, LR: 0.000250
Epoch 80/100, Loss: 3.1254, LR: 0.000125
Epoch 81/100, Loss: 3.1240, LR: 0.000125
Epoch 82/100, Loss: 3.1300, LR: 0.000125
Epoch 83/100, Loss: 3.1292, LR: 0.000125
Epoch 84/100, Loss: 3.1248, LR: 0.000125
Epoch 85/100, Loss: 3.1313, LR: 0.000125
Epoch 86/100, Loss: 3.1286, LR: 0.000125
Epoch 87/100, Loss: 3.1298, LR: 0.000125
Epoch 88/100, Loss: 3.1298, LR: 0.000125
Epoch 89/100, Loss: 3.1291, LR: 0.000125
Epoch 90/100, Loss: 3.1241, LR: 0.000125
Epoch 91/100, Loss: 3.1281, LR: 0.000125
Epoch 92/100, Loss: 3.1246, LR: 0.000125
Epoch 93/100, Loss: 3.1239, LR: 0.000125
Epoch 94/100, Loss: 3.1240, LR: 0.000125
Epoch 95/100, Loss: 3.1263, LR: 0.000125
Epoch 96/100, Loss: 3.1248, LR: 0.000125
Epoch 97/100, Loss: 3.1282, LR: 0.000125
Epoch 98/100, Loss: 3.1240, LR: 0.000125
Epoch 99/100, Loss: 3.1263, LR: 0.000125
Epoch 100/100, Loss: 3.1245, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.48
Change BestPlayer
>> Model updated (win rate: 48.0%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.5
>> Cycle 12 completed. Checkpoint saved.

Train 13 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 1000, Games per worker: 125
>> MCTS batch size: 8, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/125 games completed
Worker 5: 20/125 games completed
Worker 2: 20/125 games completed
Worker 7: 20/125 games completed
Worker 1: 20/125 games completed
Worker 4: 20/125 games completed
Worker 8: 20/125 games completed
Worker 3: 20/125 games completed
Worker 6: 20/125 games completed
Worker 5: 30/125 games completed
Worker 2: 30/125 games completed
Worker 7: 30/125 games completed
Worker 1: 30/125 games completed
Worker 5: 40/125 games completed
Worker 3: 30/125 games completed
Worker 4: 30/125 games completed
Worker 8: 30/125 games completed
Worker 6: 30/125 games completed
Worker 5: 50/125 games completed
Worker 1: 40/125 games completed
Worker 7: 40/125 games completed
Worker 2: 40/125 games completed
Worker 8: 40/125 games completed
Worker 4: 40/125 games completed
Worker 3: 40/125 games completed
Worker 6: 40/125 games completed
Worker 5: 60/125 games completed
Worker 1: 50/125 games completed
Worker 2: 50/125 games completed
Worker 7: 50/125 games completed
Worker 4: 50/125 games completed
Worker 8: 50/125 games completed
Worker 3: 50/125 games completed
Worker 6: 50/125 games completed
Worker 5: 70/125 games completed
Worker 1: 60/125 games completed
Worker 7: 60/125 games completed
Worker 2: 60/125 games completed
Worker 5: 80/125 games completed
Worker 4: 60/125 games completed
Worker 8: 60/125 games completed
Worker 3: 60/125 games completed
Worker 6: 60/125 games completed
Worker 1: 70/125 games completed
Worker 5: 90/125 games completed
Worker 7: 70/125 games completed
Worker 2: 70/125 games completed
Worker 8: 70/125 games completed
Worker 4: 70/125 games completed
Worker 3: 70/125 games completed
Worker 6: 70/125 games completed
Worker 5: 100/125 games completed
Worker 1: 80/125 games completed
Worker 7: 80/125 games completed
Worker 2: 80/125 games completed
Worker 8: 80/125 games completed
Worker 3: 80/125 games completed
Worker 6: 80/125 games completed
Worker 4: 80/125 games completed
Worker 5: 110/125 games completed
Worker 1: 90/125 games completed
Worker 7: 90/125 games completed
Worker 2: 90/125 games completed
Worker 8: 90/125 games completed
Worker 5: 120/125 games completed
Worker 6: 90/125 games completed
Worker 3: 90/125 games completed
Worker 4: 90/125 games completed
Worker 1: 100/125 games completed
Worker 7: 100/125 games completed
Worker 2: 100/125 games completed
Worker 8: 100/125 games completed
Worker 3: 100/125 games completed
Worker 6: 100/125 games completed
Worker 4: 100/125 games completed
Worker 1: 110/125 games completed
Worker 7: 110/125 games completed
Worker 2: 110/125 games completed
Worker 8: 110/125 games completed
Worker 4: 110/125 games completed
Worker 3: 110/125 games completed
Worker 6: 110/125 games completed
Worker 1: 120/125 games completed
Worker 7: 120/125 games completed
Worker 2: 120/125 games completed
Worker 8: 120/125 games completed
Worker 4: 120/125 games completed
Worker 6: 120/125 games completed
Worker 3: 120/125 games completed
>> Collected 55623 training samples from 1000 games
>> Saved to ./data/20251030235724.history
>> Train 13
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.1393, LR: 0.000500
Epoch 2/100, Loss: 3.1253, LR: 0.000500
Epoch 3/100, Loss: 3.1173, LR: 0.000500
Epoch 4/100, Loss: 3.1136, LR: 0.000500
Epoch 5/100, Loss: 3.1104, LR: 0.000500
Epoch 6/100, Loss: 3.1087, LR: 0.000500
Epoch 7/100, Loss: 3.1079, LR: 0.000500
Epoch 8/100, Loss: 3.1083, LR: 0.000500
Epoch 9/100, Loss: 3.1078, LR: 0.000500
Epoch 10/100, Loss: 3.1073, LR: 0.000500
Epoch 11/100, Loss: 3.1064, LR: 0.000500
Epoch 12/100, Loss: 3.1061, LR: 0.000500
Epoch 13/100, Loss: 3.1059, LR: 0.000500
Epoch 14/100, Loss: 3.1060, LR: 0.000500
Epoch 15/100, Loss: 3.1062, LR: 0.000500
Epoch 16/100, Loss: 3.1064, LR: 0.000500
Epoch 17/100, Loss: 3.1112, LR: 0.000500
Epoch 18/100, Loss: 3.1083, LR: 0.000500
Epoch 19/100, Loss: 3.1069, LR: 0.000500
Epoch 20/100, Loss: 3.1058, LR: 0.000500
Epoch 21/100, Loss: 3.1056, LR: 0.000500
Epoch 22/100, Loss: 3.1054, LR: 0.000500
Epoch 23/100, Loss: 3.1057, LR: 0.000500
Epoch 24/100, Loss: 3.1058, LR: 0.000500
Epoch 25/100, Loss: 3.1062, LR: 0.000500
Epoch 26/100, Loss: 3.1071, LR: 0.000500
Epoch 27/100, Loss: 3.1094, LR: 0.000500
Epoch 28/100, Loss: 3.1074, LR: 0.000500
Epoch 29/100, Loss: 3.1064, LR: 0.000500
Epoch 30/100, Loss: 3.1060, LR: 0.000500
Epoch 31/100, Loss: 3.1053, LR: 0.000500
Epoch 32/100, Loss: 3.1056, LR: 0.000500
Epoch 33/100, Loss: 3.1055, LR: 0.000500
Epoch 34/100, Loss: 3.1055, LR: 0.000500
Epoch 35/100, Loss: 3.1056, LR: 0.000500
Epoch 36/100, Loss: 3.1055, LR: 0.000500
Epoch 37/100, Loss: 3.1056, LR: 0.000500
Epoch 38/100, Loss: 3.1066, LR: 0.000500
Epoch 39/100, Loss: 3.1093, LR: 0.000500
Epoch 40/100, Loss: 3.1081, LR: 0.000500
Epoch 41/100, Loss: 3.1063, LR: 0.000500
Epoch 42/100, Loss: 3.1052, LR: 0.000500
Epoch 43/100, Loss: 3.1057, LR: 0.000500
Epoch 44/100, Loss: 3.1057, LR: 0.000500
Epoch 45/100, Loss: 3.1056, LR: 0.000500
Epoch 46/100, Loss: 3.1054, LR: 0.000500
Epoch 47/100, Loss: 3.1059, LR: 0.000500
Epoch 48/100, Loss: 3.1057, LR: 0.000500
Epoch 49/100, Loss: 3.1058, LR: 0.000500
Epoch 50/100, Loss: 3.1064, LR: 0.000250
Epoch 51/100, Loss: 3.1057, LR: 0.000250
Epoch 52/100, Loss: 3.1054, LR: 0.000250
Epoch 53/100, Loss: 3.1054, LR: 0.000250
Epoch 54/100, Loss: 3.1050, LR: 0.000250
Epoch 55/100, Loss: 3.1052, LR: 0.000250
Epoch 56/100, Loss: 3.1051, LR: 0.000250
Epoch 57/100, Loss: 3.1056, LR: 0.000250
Epoch 58/100, Loss: 3.1054, LR: 0.000250
Epoch 59/100, Loss: 3.1054, LR: 0.000250
Epoch 60/100, Loss: 3.1054, LR: 0.000250
Epoch 61/100, Loss: 3.1053, LR: 0.000250
Epoch 62/100, Loss: 3.1051, LR: 0.000250
Epoch 63/100, Loss: 3.1055, LR: 0.000250
Epoch 64/100, Loss: 3.1052, LR: 0.000250
Epoch 65/100, Loss: 3.1057, LR: 0.000250
Epoch 66/100, Loss: 3.1062, LR: 0.000250
Epoch 67/100, Loss: 3.1058, LR: 0.000250
Epoch 68/100, Loss: 3.1055, LR: 0.000250
Epoch 69/100, Loss: 3.1056, LR: 0.000250
Epoch 70/100, Loss: 3.1049, LR: 0.000250
Epoch 71/100, Loss: 3.1051, LR: 0.000250
Epoch 72/100, Loss: 3.1056, LR: 0.000250
Epoch 73/100, Loss: 3.1059, LR: 0.000250
Epoch 74/100, Loss: 3.1050, LR: 0.000250
Epoch 75/100, Loss: 3.1057, LR: 0.000250
Epoch 76/100, Loss: 3.1051, LR: 0.000250
Epoch 77/100, Loss: 3.1052, LR: 0.000250
Epoch 78/100, Loss: 3.1051, LR: 0.000250
Epoch 79/100, Loss: 3.1049, LR: 0.000250
Epoch 80/100, Loss: 3.1053, LR: 0.000125
Epoch 81/100, Loss: 3.1050, LR: 0.000125
Epoch 82/100, Loss: 3.1050, LR: 0.000125
Epoch 83/100, Loss: 3.1051, LR: 0.000125
Epoch 84/100, Loss: 3.1052, LR: 0.000125
Epoch 85/100, Loss: 3.1055, LR: 0.000125
Epoch 86/100, Loss: 3.1056, LR: 0.000125
Epoch 87/100, Loss: 3.1053, LR: 0.000125
Epoch 88/100, Loss: 3.1051, LR: 0.000125
Epoch 89/100, Loss: 3.1054, LR: 0.000125
Epoch 90/100, Loss: 3.1052, LR: 0.000125
Epoch 91/100, Loss: 3.1054, LR: 0.000125
Epoch 92/100, Loss: 3.1056, LR: 0.000125
Epoch 93/100, Loss: 3.1051, LR: 0.000125
Epoch 94/100, Loss: 3.1049, LR: 0.000125
Epoch 95/100, Loss: 3.1051, LR: 0.000125
Epoch 96/100, Loss: 3.1054, LR: 0.000125
Epoch 97/100, Loss: 3.1049, LR: 0.000125
Epoch 98/100, Loss: 3.1055, LR: 0.000125
Epoch 99/100, Loss: 3.1050, LR: 0.000125
Epoch 100/100, Loss: 3.1053, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.585
Change BestPlayer
>> Model updated (win rate: 58.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.5
>> Cycle 13 completed. Checkpoint saved.

Train 14 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 1000, Games per worker: 125
>> MCTS batch size: 8, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/125 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/125 games completed
Worker 7: 20/125 games completed
Worker 5: 20/125 games completed
Worker 6: 20/125 games completed
Worker 4: 20/125 games completed
Worker 1: 20/125 games completed
Worker 8: 20/125 games completed
Worker 3: 20/125 games completed
Worker 2: 20/125 games completed
Worker 7: 30/125 games completed
Worker 5: 30/125 games completed
Worker 6: 30/125 games completed
Worker 8: 30/125 games completed
Worker 1: 30/125 games completed
Worker 4: 30/125 games completed
Worker 3: 30/125 games completed
Worker 2: 30/125 games completed
Worker 5: 40/125 games completed
Worker 6: 40/125 games completed
Worker 7: 40/125 games completed
Worker 8: 40/125 games completed
Worker 4: 40/125 games completed
Worker 1: 40/125 games completed
Worker 3: 40/125 games completed
Worker 2: 40/125 games completed
Worker 5: 50/125 games completed
Worker 6: 50/125 games completed
Worker 4: 50/125 games completed
Worker 7: 50/125 games completed
Worker 8: 50/125 games completed
Worker 1: 50/125 games completed
Worker 2: 50/125 games completed
Worker 3: 50/125 games completed
Worker 5: 60/125 games completed
Worker 4: 60/125 games completed
Worker 6: 60/125 games completed
Worker 7: 60/125 games completed
Worker 1: 60/125 games completed
Worker 8: 60/125 games completed
Worker 2: 60/125 games completed
Worker 3: 60/125 games completed
Worker 5: 70/125 games completed
Worker 4: 70/125 games completed
Worker 6: 70/125 games completed
Worker 1: 70/125 games completed
Worker 7: 70/125 games completed
Worker 8: 70/125 games completed
Worker 2: 70/125 games completed
Worker 3: 70/125 games completed
Worker 5: 80/125 games completed
Worker 6: 80/125 games completed
Worker 4: 80/125 games completed
Worker 8: 80/125 games completed
Worker 1: 80/125 games completed
Worker 7: 80/125 games completed
Worker 2: 80/125 games completed
Worker 3: 80/125 games completed
Worker 5: 90/125 games completed
Worker 8: 90/125 games completed
Worker 6: 90/125 games completed
Worker 4: 90/125 games completed
Worker 7: 90/125 games completed
Worker 1: 90/125 games completed
Worker 2: 90/125 games completed
Worker 3: 90/125 games completed
Worker 5: 100/125 games completed
Worker 6: 100/125 games completed
Worker 8: 100/125 games completed
Worker 4: 100/125 games completed
Worker 1: 100/125 games completed
Worker 7: 100/125 games completed
Worker 2: 100/125 games completed
Worker 3: 100/125 games completed
Worker 5: 110/125 games completed
Worker 8: 110/125 games completed
Worker 4: 110/125 games completed
Worker 1: 110/125 games completed
Worker 6: 110/125 games completed
Worker 7: 110/125 games completed
Worker 2: 110/125 games completed
Worker 3: 110/125 games completed
Worker 5: 120/125 games completed
Worker 8: 120/125 games completed
Worker 6: 120/125 games completed
Worker 1: 120/125 games completed
Worker 4: 120/125 games completed
Worker 2: 120/125 games completed
Worker 7: 120/125 games completed
Worker 3: 120/125 games completed
>> Collected 55548 training samples from 1000 games
>> Saved to ./data/20251031011152.history
>> Train 14
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.1454, LR: 0.000500
Epoch 2/100, Loss: 3.1329, LR: 0.000500
Epoch 3/100, Loss: 3.1255, LR: 0.000500
Epoch 4/100, Loss: 3.1211, LR: 0.000500
Epoch 5/100, Loss: 3.1187, LR: 0.000500
Epoch 6/100, Loss: 3.1172, LR: 0.000500
Epoch 7/100, Loss: 3.1162, LR: 0.000500
Epoch 8/100, Loss: 3.1154, LR: 0.000500
Epoch 9/100, Loss: 3.1149, LR: 0.000500
Epoch 10/100, Loss: 3.1146, LR: 0.000500
Epoch 11/100, Loss: 3.1151, LR: 0.000500
Epoch 12/100, Loss: 3.1196, LR: 0.000500
Epoch 13/100, Loss: 3.1162, LR: 0.000500
Epoch 14/100, Loss: 3.1146, LR: 0.000500
Epoch 15/100, Loss: 3.1141, LR: 0.000500
Epoch 16/100, Loss: 3.1139, LR: 0.000500
Epoch 17/100, Loss: 3.1138, LR: 0.000500
Epoch 18/100, Loss: 3.1138, LR: 0.000500
Epoch 19/100, Loss: 3.1138, LR: 0.000500
Epoch 20/100, Loss: 3.1137, LR: 0.000500
Epoch 21/100, Loss: 3.1140, LR: 0.000500
Epoch 22/100, Loss: 3.1151, LR: 0.000500
Epoch 23/100, Loss: 3.1186, LR: 0.000500
Epoch 24/100, Loss: 3.1147, LR: 0.000500
Epoch 25/100, Loss: 3.1139, LR: 0.000500
Epoch 26/100, Loss: 3.1137, LR: 0.000500
Epoch 27/100, Loss: 3.1136, LR: 0.000500
Epoch 28/100, Loss: 3.1136, LR: 0.000500
Epoch 29/100, Loss: 3.1138, LR: 0.000500
Epoch 30/100, Loss: 3.1138, LR: 0.000500
Epoch 31/100, Loss: 3.1140, LR: 0.000500
Epoch 32/100, Loss: 3.1158, LR: 0.000500
Epoch 33/100, Loss: 3.1171, LR: 0.000500
Epoch 34/100, Loss: 3.1152, LR: 0.000500
Epoch 35/100, Loss: 3.1146, LR: 0.000500
Epoch 36/100, Loss: 3.1141, LR: 0.000500
Epoch 37/100, Loss: 3.1138, LR: 0.000500
Epoch 38/100, Loss: 3.1136, LR: 0.000500
Epoch 39/100, Loss: 3.1135, LR: 0.000500
Epoch 40/100, Loss: 3.1135, LR: 0.000500
Epoch 41/100, Loss: 3.1137, LR: 0.000500
Epoch 42/100, Loss: 3.1137, LR: 0.000500
Epoch 43/100, Loss: 3.1139, LR: 0.000500
Epoch 44/100, Loss: 3.1142, LR: 0.000500
Epoch 45/100, Loss: 3.1164, LR: 0.000500
Epoch 46/100, Loss: 3.1147, LR: 0.000500
Epoch 47/100, Loss: 3.1152, LR: 0.000500
Epoch 48/100, Loss: 3.1142, LR: 0.000500
Epoch 49/100, Loss: 3.1137, LR: 0.000500
Epoch 50/100, Loss: 3.1137, LR: 0.000250
Epoch 51/100, Loss: 3.1134, LR: 0.000250
Epoch 52/100, Loss: 3.1134, LR: 0.000250
Epoch 53/100, Loss: 3.1134, LR: 0.000250
Epoch 54/100, Loss: 3.1133, LR: 0.000250
Epoch 55/100, Loss: 3.1133, LR: 0.000250
Epoch 56/100, Loss: 3.1134, LR: 0.000250
Epoch 57/100, Loss: 3.1134, LR: 0.000250
Epoch 58/100, Loss: 3.1134, LR: 0.000250
Epoch 59/100, Loss: 3.1134, LR: 0.000250
Epoch 60/100, Loss: 3.1134, LR: 0.000250
Epoch 61/100, Loss: 3.1134, LR: 0.000250
Epoch 62/100, Loss: 3.1134, LR: 0.000250
Epoch 63/100, Loss: 3.1134, LR: 0.000250
Epoch 64/100, Loss: 3.1135, LR: 0.000250
Epoch 65/100, Loss: 3.1140, LR: 0.000250
Epoch 66/100, Loss: 3.1134, LR: 0.000250
Epoch 67/100, Loss: 3.1133, LR: 0.000250
Epoch 68/100, Loss: 3.1133, LR: 0.000250
Epoch 69/100, Loss: 3.1133, LR: 0.000250
Epoch 70/100, Loss: 3.1133, LR: 0.000250
Epoch 71/100, Loss: 3.1133, LR: 0.000250
Epoch 72/100, Loss: 3.1135, LR: 0.000250
Epoch 73/100, Loss: 3.1134, LR: 0.000250
Epoch 74/100, Loss: 3.1133, LR: 0.000250
Epoch 75/100, Loss: 3.1134, LR: 0.000250
Epoch 76/100, Loss: 3.1134, LR: 0.000250
Epoch 77/100, Loss: 3.1134, LR: 0.000250
Epoch 78/100, Loss: 3.1134, LR: 0.000250
Epoch 79/100, Loss: 3.1134, LR: 0.000250
Epoch 80/100, Loss: 3.1134, LR: 0.000125
Epoch 81/100, Loss: 3.1133, LR: 0.000125
Epoch 82/100, Loss: 3.1132, LR: 0.000125
Epoch 83/100, Loss: 3.1132, LR: 0.000125
Epoch 84/100, Loss: 3.1132, LR: 0.000125
Epoch 85/100, Loss: 3.1132, LR: 0.000125
Epoch 86/100, Loss: 3.1132, LR: 0.000125
Epoch 87/100, Loss: 3.1132, LR: 0.000125
Epoch 88/100, Loss: 3.1133, LR: 0.000125
Epoch 89/100, Loss: 3.1133, LR: 0.000125
Epoch 90/100, Loss: 3.1132, LR: 0.000125
Epoch 91/100, Loss: 3.1132, LR: 0.000125
Epoch 92/100, Loss: 3.1132, LR: 0.000125
Epoch 93/100, Loss: 3.1132, LR: 0.000125
Epoch 94/100, Loss: 3.1132, LR: 0.000125
Epoch 95/100, Loss: 3.1132, LR: 0.000125
Epoch 96/100, Loss: 3.1132, LR: 0.000125
Epoch 97/100, Loss: 3.1133, LR: 0.000125
Epoch 98/100, Loss: 3.1133, LR: 0.000125
Epoch 99/100, Loss: 3.1132, LR: 0.000125
Epoch 100/100, Loss: 3.1132, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.555
Change BestPlayer
>> Model updated (win rate: 55.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.6
>> Cycle 14 completed. Checkpoint saved.

Train 15 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 8 workers (C++ backend)
>> Total games: 1000, Games per worker: 125
>> MCTS batch size: 8, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> Resuming from cycle 15

Train 15 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1000, Games per worker: 100
>> MCTS batch size: 64, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/100 games completed
Worker 9: 20/100 games completed
Worker 10: 20/100 games completed
Worker 7: 20/100 games completed
Worker 3: 20/100 games completed
Worker 8: 20/100 games completed
Worker 5: 20/100 games completed
Worker 4: 20/100 games completed
Worker 2: 20/100 games completed
Worker 1: 20/100 games completed
Worker 6: 20/100 games completed
Worker 9: 30/100 games completed
Worker 4: 30/100 games completed
Worker 10: 30/100 games completed
Worker 7: 30/100 games completed
Worker 5: 30/100 games completed
Worker 3: 30/100 games completed
Worker 8: 30/100 games completed
Worker 6: 30/100 games completed
Worker 2: 30/100 games completed
Worker 1: 30/100 games completed
Worker 4: 40/100 games completed
Worker 9: 40/100 games completed
Worker 7: 40/100 games completed
Worker 5: 40/100 games completed
Worker 2: 40/100 games completed
Worker 10: 40/100 games completed
Worker 3: 40/100 games completed
Worker 8: 40/100 games completed
Worker 6: 40/100 games completed
Worker 1: 40/100 games completed
Worker 4: 50/100 games completed
Worker 9: 50/100 games completed
Worker 5: 50/100 games completed
Worker 7: 50/100 games completed
Worker 3: 50/100 games completed
Worker 2: 50/100 games completed
Worker 8: 50/100 games completed
Worker 10: 50/100 games completed
Worker 6: 50/100 games completed
Worker 1: 50/100 games completed
Worker 4: 60/100 games completed
Worker 5: 60/100 games completed
Worker 9: 60/100 games completed
Worker 8: 60/100 games completed
Worker 7: 60/100 games completed
Worker 3: 60/100 games completed
Worker 2: 60/100 games completed
Worker 1: 60/100 games completed
Worker 6: 60/100 games completed
Worker 10: 60/100 games completed
Worker 4: 70/100 games completed
Worker 8: 70/100 games completed
Worker 9: 70/100 games completed
Worker 7: 70/100 games completed
Worker 5: 70/100 games completed
Worker 3: 70/100 games completed
Worker 2: 70/100 games completed
Worker 10: 70/100 games completed
Worker 1: 70/100 games completed
Worker 6: 70/100 games completed
Worker 4: 80/100 games completed
Worker 7: 80/100 games completed
Worker 8: 80/100 games completed
Worker 5: 80/100 games completed
Worker 9: 80/100 games completed
Worker 2: 80/100 games completed
Worker 3: 80/100 games completed
Worker 1: 80/100 games completed
Worker 10: 80/100 games completed
Worker 6: 80/100 games completed
Worker 4: 90/100 games completed
Worker 8: 90/100 games completed
Worker 7: 90/100 games completed
Worker 5: 90/100 games completed
Worker 9: 90/100 games completed
Worker 2: 90/100 games completed
Worker 3: 90/100 games completed
Worker 6: 90/100 games completed
Worker 10: 90/100 games completed
Worker 1: 90/100 games completed
Worker 4: 100/100 games completed
Worker 7: 100/100 games completed
Worker 8: 100/100 games completed
Worker 5: 100/100 games completed
Worker 3: 100/100 games completed
Worker 2: 100/100 games completed
Worker 9: 100/100 games completed
Worker 1: 100/100 games completed
Worker 10: 100/100 games completed
Worker 6: 100/100 games completed
>> Collected 54912 training samples from 1000 games
>> Saved to ./data/20251031020541.history
>> Train 15
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.2910, LR: 0.000500
Epoch 2/100, Loss: 3.1439, LR: 0.000500
Epoch 3/100, Loss: 3.1273, LR: 0.000500
Epoch 4/100, Loss: 3.1172, LR: 0.000500
Epoch 5/100, Loss: 3.1099, LR: 0.000500
Epoch 6/100, Loss: 3.1060, LR: 0.000500
Epoch 7/100, Loss: 3.1027, LR: 0.000500
Epoch 8/100, Loss: 3.1013, LR: 0.000500
Epoch 9/100, Loss: 3.1006, LR: 0.000500
Epoch 10/100, Loss: 3.0976, LR: 0.000500
Epoch 11/100, Loss: 3.0956, LR: 0.000500
Epoch 12/100, Loss: 3.0938, LR: 0.000500
Epoch 13/100, Loss: 3.0934, LR: 0.000500
Epoch 14/100, Loss: 3.0924, LR: 0.000500
Epoch 15/100, Loss: 3.0926, LR: 0.000500
Epoch 16/100, Loss: 3.0933, LR: 0.000500
Epoch 17/100, Loss: 3.0912, LR: 0.000500
Epoch 18/100, Loss: 3.0911, LR: 0.000500
Epoch 19/100, Loss: 3.0904, LR: 0.000500
Epoch 20/100, Loss: 3.0907, LR: 0.000500
Epoch 21/100, Loss: 3.0904, LR: 0.000500
Epoch 22/100, Loss: 3.0898, LR: 0.000500
Epoch 23/100, Loss: 3.0893, LR: 0.000500
Epoch 24/100, Loss: 3.0894, LR: 0.000500
Epoch 25/100, Loss: 3.0900, LR: 0.000500
Epoch 26/100, Loss: 3.0896, LR: 0.000500
Epoch 27/100, Loss: 3.0885, LR: 0.000500
Epoch 28/100, Loss: 3.0880, LR: 0.000500
Epoch 29/100, Loss: 3.0886, LR: 0.000500
Epoch 30/100, Loss: 3.0884, LR: 0.000500
Epoch 31/100, Loss: 3.0898, LR: 0.000500
Epoch 32/100, Loss: 3.0894, LR: 0.000500
Epoch 33/100, Loss: 3.0884, LR: 0.000500
Epoch 34/100, Loss: 3.0875, LR: 0.000500
Epoch 35/100, Loss: 3.0875, LR: 0.000500
Epoch 36/100, Loss: 3.0875, LR: 0.000500
Epoch 37/100, Loss: 3.0873, LR: 0.000500
Epoch 38/100, Loss: 3.0871, LR: 0.000500
Epoch 39/100, Loss: 3.0877, LR: 0.000500
Epoch 40/100, Loss: 3.0873, LR: 0.000500
Epoch 41/100, Loss: 3.0877, LR: 0.000500
Epoch 42/100, Loss: 3.0873, LR: 0.000500
Epoch 43/100, Loss: 3.0869, LR: 0.000500
Epoch 44/100, Loss: 3.0867, LR: 0.000500
Epoch 45/100, Loss: 3.0867, LR: 0.000500
Epoch 46/100, Loss: 3.0873, LR: 0.000500
Epoch 47/100, Loss: 3.0877, LR: 0.000500
Epoch 48/100, Loss: 3.0871, LR: 0.000500
Epoch 49/100, Loss: 3.0870, LR: 0.000500
Epoch 50/100, Loss: 3.0864, LR: 0.000250
Epoch 51/100, Loss: 3.0862, LR: 0.000250
Epoch 52/100, Loss: 3.0857, LR: 0.000250
Epoch 53/100, Loss: 3.0852, LR: 0.000250
Epoch 54/100, Loss: 3.0851, LR: 0.000250
Epoch 55/100, Loss: 3.0851, LR: 0.000250
Epoch 56/100, Loss: 3.0851, LR: 0.000250
Epoch 57/100, Loss: 3.0852, LR: 0.000250
Epoch 58/100, Loss: 3.0853, LR: 0.000250
Epoch 59/100, Loss: 3.0854, LR: 0.000250
Epoch 60/100, Loss: 3.0854, LR: 0.000250
Epoch 61/100, Loss: 3.0853, LR: 0.000250
Epoch 62/100, Loss: 3.0853, LR: 0.000250
Epoch 63/100, Loss: 3.0854, LR: 0.000250
Epoch 64/100, Loss: 3.0853, LR: 0.000250
Epoch 65/100, Loss: 3.0853, LR: 0.000250
Epoch 66/100, Loss: 3.0852, LR: 0.000250
Epoch 67/100, Loss: 3.0852, LR: 0.000250
Epoch 68/100, Loss: 3.0852, LR: 0.000250
Epoch 69/100, Loss: 3.0852, LR: 0.000250
Epoch 70/100, Loss: 3.0852, LR: 0.000250
Epoch 71/100, Loss: 3.0852, LR: 0.000250
Epoch 72/100, Loss: 3.0852, LR: 0.000250
Epoch 73/100, Loss: 3.0852, LR: 0.000250
Epoch 74/100, Loss: 3.0854, LR: 0.000250
Epoch 75/100, Loss: 3.0852, LR: 0.000250
Epoch 76/100, Loss: 3.0851, LR: 0.000250
Epoch 77/100, Loss: 3.0850, LR: 0.000250
Epoch 78/100, Loss: 3.0851, LR: 0.000250
Epoch 79/100, Loss: 3.0851, LR: 0.000250
Epoch 80/100, Loss: 3.0855, LR: 0.000125
Epoch 81/100, Loss: 3.0850, LR: 0.000125
Epoch 82/100, Loss: 3.0848, LR: 0.000125
Epoch 83/100, Loss: 3.0847, LR: 0.000125
Epoch 84/100, Loss: 3.0847, LR: 0.000125
Epoch 85/100, Loss: 3.0847, LR: 0.000125
Epoch 86/100, Loss: 3.0847, LR: 0.000125
Epoch 87/100, Loss: 3.0847, LR: 0.000125
Epoch 88/100, Loss: 3.0849, LR: 0.000125
Epoch 89/100, Loss: 3.0847, LR: 0.000125
Epoch 90/100, Loss: 3.0847, LR: 0.000125
Epoch 91/100, Loss: 3.0847, LR: 0.000125
Epoch 92/100, Loss: 3.0847, LR: 0.000125
Epoch 93/100, Loss: 3.0847, LR: 0.000125
Epoch 94/100, Loss: 3.0847, LR: 0.000125
Epoch 95/100, Loss: 3.0847, LR: 0.000125
Epoch 96/100, Loss: 3.0847, LR: 0.000125
Epoch 97/100, Loss: 3.0847, LR: 0.000125
Epoch 98/100, Loss: 3.0847, LR: 0.000125
Epoch 99/100, Loss: 3.0847, LR: 0.000125
Epoch 100/100, Loss: 3.0847, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.45
Change BestPlayer
>> Model updated (win rate: 45.0%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.35
>> Cycle 15 completed. Checkpoint saved.

Train 16 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1000, Games per worker: 100
>> MCTS batch size: 64, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/100 games completed
Worker 6: 20/100 games completed
Worker 8: 20/100 games completed
Worker 1: 20/100 games completed
Worker 7: 20/100 games completed
Worker 9: 20/100 games completed
Worker 2: 20/100 games completed
Worker 3: 20/100 games completed
Worker 4: 20/100 games completed
Worker 10: 20/100 games completed
Worker 5: 20/100 games completed
Worker 6: 30/100 games completed
Worker 1: 30/100 games completed
Worker 8: 30/100 games completed
Worker 2: 30/100 games completed
Worker 7: 30/100 games completed
Worker 9: 30/100 games completed
Worker 10: 30/100 games completed
Worker 3: 30/100 games completed
Worker 5: 30/100 games completed
Worker 4: 30/100 games completed
Worker 6: 40/100 games completed
Worker 2: 40/100 games completed
Worker 8: 40/100 games completed
Worker 1: 40/100 games completed
Worker 7: 40/100 games completed
Worker 3: 40/100 games completed
Worker 9: 40/100 games completed
Worker 5: 40/100 games completed
Worker 4: 40/100 games completed
Worker 10: 40/100 games completed
Worker 6: 50/100 games completed
Worker 2: 50/100 games completed
Worker 8: 50/100 games completed
Worker 7: 50/100 games completed
Worker 1: 50/100 games completed
Worker 3: 50/100 games completed
Worker 9: 50/100 games completed
Worker 5: 50/100 games completed
Worker 10: 50/100 games completed
Worker 4: 50/100 games completed
Worker 6: 60/100 games completed
Worker 7: 60/100 games completed
Worker 2: 60/100 games completed
Worker 8: 60/100 games completed
Worker 1: 60/100 games completed
Worker 3: 60/100 games completed
Worker 10: 60/100 games completed
Worker 9: 60/100 games completed
Worker 5: 60/100 games completed
Worker 4: 60/100 games completed
Worker 6: 70/100 games completed
Worker 2: 70/100 games completed
Worker 7: 70/100 games completed
Worker 8: 70/100 games completed
Worker 1: 70/100 games completed
Worker 10: 70/100 games completed
Worker 3: 70/100 games completed
Worker 5: 70/100 games completed
Worker 9: 70/100 games completed
Worker 4: 70/100 games completed
Worker 6: 80/100 games completed
Worker 2: 80/100 games completed
Worker 8: 80/100 games completed
Worker 1: 80/100 games completed
Worker 7: 80/100 games completed
Worker 5: 80/100 games completed
Worker 3: 80/100 games completed
Worker 10: 80/100 games completed
Worker 9: 80/100 games completed
Worker 4: 80/100 games completed
Worker 6: 90/100 games completed
Worker 2: 90/100 games completed
Worker 7: 90/100 games completed
Worker 1: 90/100 games completed
Worker 8: 90/100 games completed
Worker 5: 90/100 games completed
Worker 3: 90/100 games completed
Worker 10: 90/100 games completed
Worker 9: 90/100 games completed
Worker 4: 90/100 games completed
Worker 6: 100/100 games completed
Worker 2: 100/100 games completed
Worker 7: 100/100 games completed
Worker 5: 100/100 games completed
Worker 8: 100/100 games completed
Worker 1: 100/100 games completed
Worker 10: 100/100 games completed
Worker 3: 100/100 games completed
Worker 9: 100/100 games completed
Worker 4: 100/100 games completed
>> Collected 54720 training samples from 1000 games
>> Saved to ./data/20251031024224.history
>> Train 16
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.1166, LR: 0.000500
Epoch 2/100, Loss: 3.1032, LR: 0.000500
Epoch 3/100, Loss: 3.0972, LR: 0.000500
Epoch 4/100, Loss: 3.0939, LR: 0.000500
Epoch 5/100, Loss: 3.0908, LR: 0.000500
Epoch 6/100, Loss: 3.0911, LR: 0.000500
Epoch 7/100, Loss: 3.0892, LR: 0.000500
Epoch 8/100, Loss: 3.0904, LR: 0.000500
Epoch 9/100, Loss: 3.0882, LR: 0.000500
Epoch 10/100, Loss: 3.0879, LR: 0.000500
Epoch 11/100, Loss: 3.0872, LR: 0.000500
Epoch 12/100, Loss: 3.0870, LR: 0.000500
Epoch 13/100, Loss: 3.0890, LR: 0.000500
Epoch 14/100, Loss: 3.0892, LR: 0.000500
Epoch 15/100, Loss: 3.0879, LR: 0.000500
Epoch 16/100, Loss: 3.0875, LR: 0.000500
Epoch 17/100, Loss: 3.0861, LR: 0.000500
Epoch 18/100, Loss: 3.0867, LR: 0.000500
Epoch 19/100, Loss: 3.0869, LR: 0.000500
Epoch 20/100, Loss: 3.0874, LR: 0.000500
Epoch 21/100, Loss: 3.0882, LR: 0.000500
Epoch 22/100, Loss: 3.0888, LR: 0.000500
Epoch 23/100, Loss: 3.0870, LR: 0.000500
Epoch 24/100, Loss: 3.0864, LR: 0.000500
Epoch 25/100, Loss: 3.0858, LR: 0.000500
Epoch 26/100, Loss: 3.0862, LR: 0.000500
Epoch 27/100, Loss: 3.0864, LR: 0.000500
Epoch 28/100, Loss: 3.0873, LR: 0.000500
Epoch 29/100, Loss: 3.0864, LR: 0.000500
Epoch 30/100, Loss: 3.0866, LR: 0.000500
Epoch 31/100, Loss: 3.0860, LR: 0.000500
Epoch 32/100, Loss: 3.0859, LR: 0.000500
Epoch 33/100, Loss: 3.0858, LR: 0.000500
Epoch 34/100, Loss: 3.0859, LR: 0.000500
Epoch 35/100, Loss: 3.0873, LR: 0.000500
Epoch 36/100, Loss: 3.0896, LR: 0.000500
Epoch 37/100, Loss: 3.0872, LR: 0.000500
Epoch 38/100, Loss: 3.0863, LR: 0.000500
Epoch 39/100, Loss: 3.0858, LR: 0.000500
Epoch 40/100, Loss: 3.0860, LR: 0.000500
Epoch 41/100, Loss: 3.0861, LR: 0.000500
Epoch 42/100, Loss: 3.0858, LR: 0.000500
Epoch 43/100, Loss: 3.0860, LR: 0.000500
Epoch 44/100, Loss: 3.0864, LR: 0.000500
Epoch 45/100, Loss: 3.0862, LR: 0.000500
Epoch 46/100, Loss: 3.0877, LR: 0.000500
Epoch 47/100, Loss: 3.0864, LR: 0.000500
Epoch 48/100, Loss: 3.0859, LR: 0.000500
Epoch 49/100, Loss: 3.0855, LR: 0.000500
Epoch 50/100, Loss: 3.0865, LR: 0.000250
Epoch 51/100, Loss: 3.0849, LR: 0.000250
Epoch 52/100, Loss: 3.0854, LR: 0.000250
Epoch 53/100, Loss: 3.0853, LR: 0.000250
Epoch 54/100, Loss: 3.0852, LR: 0.000250
Epoch 55/100, Loss: 3.0851, LR: 0.000250
Epoch 56/100, Loss: 3.0856, LR: 0.000250
Epoch 57/100, Loss: 3.0853, LR: 0.000250
Epoch 58/100, Loss: 3.0858, LR: 0.000250
Epoch 59/100, Loss: 3.0849, LR: 0.000250
Epoch 60/100, Loss: 3.0855, LR: 0.000250
Epoch 61/100, Loss: 3.0848, LR: 0.000250
Epoch 62/100, Loss: 3.0852, LR: 0.000250
Epoch 63/100, Loss: 3.0852, LR: 0.000250
Epoch 64/100, Loss: 3.0854, LR: 0.000250
Epoch 65/100, Loss: 3.0851, LR: 0.000250
Epoch 66/100, Loss: 3.0850, LR: 0.000250
Epoch 67/100, Loss: 3.0857, LR: 0.000250
Epoch 68/100, Loss: 3.0852, LR: 0.000250
Epoch 69/100, Loss: 3.0852, LR: 0.000250
Epoch 70/100, Loss: 3.0854, LR: 0.000250
Epoch 71/100, Loss: 3.0852, LR: 0.000250
Epoch 72/100, Loss: 3.0852, LR: 0.000250
Epoch 73/100, Loss: 3.0852, LR: 0.000250
Epoch 74/100, Loss: 3.0854, LR: 0.000250
Epoch 75/100, Loss: 3.0851, LR: 0.000250
Epoch 76/100, Loss: 3.0849, LR: 0.000250
Epoch 77/100, Loss: 3.0860, LR: 0.000250
Epoch 78/100, Loss: 3.0855, LR: 0.000250
Epoch 79/100, Loss: 3.0854, LR: 0.000250
Epoch 80/100, Loss: 3.0845, LR: 0.000125
Epoch 81/100, Loss: 3.0850, LR: 0.000125
Epoch 82/100, Loss: 3.0851, LR: 0.000125
Epoch 83/100, Loss: 3.0846, LR: 0.000125
Epoch 84/100, Loss: 3.0851, LR: 0.000125
Epoch 85/100, Loss: 3.0846, LR: 0.000125
Epoch 86/100, Loss: 3.0848, LR: 0.000125
Epoch 87/100, Loss: 3.0852, LR: 0.000125
Epoch 88/100, Loss: 3.0851, LR: 0.000125
Epoch 89/100, Loss: 3.0848, LR: 0.000125
Epoch 90/100, Loss: 3.0851, LR: 0.000125
Epoch 91/100, Loss: 3.0848, LR: 0.000125
Epoch 92/100, Loss: 3.0854, LR: 0.000125
Epoch 93/100, Loss: 3.0847, LR: 0.000125
Epoch 94/100, Loss: 3.0848, LR: 0.000125
Epoch 95/100, Loss: 3.0852, LR: 0.000125
Epoch 96/100, Loss: 3.0848, LR: 0.000125
Epoch 97/100, Loss: 3.0847, LR: 0.000125
Epoch 98/100, Loss: 3.0852, LR: 0.000125
Epoch 99/100, Loss: 3.0851, LR: 0.000125
Epoch 100/100, Loss: 3.0851, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.485
Change BestPlayer
>> Model updated (win rate: 48.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.15
>> Cycle 16 completed. Checkpoint saved.

Train 17 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1000, Games per worker: 100
>> MCTS batch size: 64, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/100 games completed
Worker 1: 20/100 games completed
Worker 8: 20/100 games completed
Worker 6: 20/100 games completed
Worker 2: 20/100 games completed
Worker 10: 20/100 games completed
Worker 3: 20/100 games completed
Worker 9: 20/100 games completed
Worker 5: 20/100 games completed
Worker 4: 20/100 games completed
Worker 7: 20/100 games completed
Worker 1: 30/100 games completed
Worker 7: 30/100 games completed
Worker 2: 30/100 games completed
Worker 3: 30/100 games completed
Worker 6: 30/100 games completed
Worker 10: 30/100 games completed
Worker 8: 30/100 games completed
Worker 4: 30/100 games completed
Worker 5: 30/100 games completed
Worker 9: 30/100 games completed
Worker 1: 40/100 games completed
Worker 3: 40/100 games completed
Worker 7: 40/100 games completed
Worker 6: 40/100 games completed
Worker 8: 40/100 games completed
Worker 5: 40/100 games completed
Worker 2: 40/100 games completed
Worker 10: 40/100 games completed
Worker 4: 40/100 games completed
Worker 9: 40/100 games completed
Worker 1: 50/100 games completed
Worker 7: 50/100 games completed
Worker 3: 50/100 games completed
Worker 10: 50/100 games completed
Worker 2: 50/100 games completed
Worker 8: 50/100 games completed
Worker 5: 50/100 games completed
Worker 6: 50/100 games completed
Worker 4: 50/100 games completed
Worker 9: 50/100 games completed
Worker 1: 60/100 games completed
Worker 2: 60/100 games completed
Worker 7: 60/100 games completed
Worker 3: 60/100 games completed
Worker 8: 60/100 games completed
Worker 10: 60/100 games completed
Worker 5: 60/100 games completed
Worker 4: 60/100 games completed
Worker 6: 60/100 games completed
Worker 9: 60/100 games completed
Worker 1: 70/100 games completed
Worker 7: 70/100 games completed
Worker 2: 70/100 games completed
Worker 6: 70/100 games completed
Worker 8: 70/100 games completed
Worker 3: 70/100 games completed
Worker 5: 70/100 games completed
Worker 10: 70/100 games completed
Worker 4: 70/100 games completed
Worker 9: 70/100 games completed
Worker 1: 80/100 games completed
Worker 2: 80/100 games completed
Worker 6: 80/100 games completed
Worker 7: 80/100 games completed
Worker 8: 80/100 games completed
Worker 10: 80/100 games completed
Worker 4: 80/100 games completed
Worker 3: 80/100 games completed
Worker 5: 80/100 games completed
Worker 9: 80/100 games completed
Worker 1: 90/100 games completed
Worker 2: 90/100 games completed
Worker 6: 90/100 games completed
Worker 8: 90/100 games completed
Worker 7: 90/100 games completed
Worker 4: 90/100 games completed
Worker 10: 90/100 games completed
Worker 5: 90/100 games completed
Worker 3: 90/100 games completed
Worker 9: 90/100 games completed
Worker 1: 100/100 games completed
Worker 2: 100/100 games completed
Worker 6: 100/100 games completed
Worker 7: 100/100 games completed
Worker 5: 100/100 games completed
Worker 8: 100/100 games completed
Worker 4: 100/100 games completed
Worker 3: 100/100 games completed
Worker 10: 100/100 games completed
Worker 9: 100/100 games completed
>> Collected 54793 training samples from 1000 games
>> Saved to ./data/20251031031902.history
>> Train 17
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.0988, LR: 0.000500
Epoch 2/100, Loss: 3.0869, LR: 0.000500
Epoch 3/100, Loss: 3.0803, LR: 0.000500
Epoch 4/100, Loss: 3.0783, LR: 0.000500
Epoch 5/100, Loss: 3.0770, LR: 0.000500
Epoch 6/100, Loss: 3.0751, LR: 0.000500
Epoch 7/100, Loss: 3.0731, LR: 0.000500
Epoch 8/100, Loss: 3.0744, LR: 0.000500
Epoch 9/100, Loss: 3.0729, LR: 0.000500
Epoch 10/100, Loss: 3.0724, LR: 0.000500
Epoch 11/100, Loss: 3.0746, LR: 0.000500
Epoch 12/100, Loss: 3.0739, LR: 0.000500
Epoch 13/100, Loss: 3.0740, LR: 0.000500
Epoch 14/100, Loss: 3.0697, LR: 0.000500
Epoch 15/100, Loss: 3.0730, LR: 0.000500
Epoch 16/100, Loss: 3.0723, LR: 0.000500
Epoch 17/100, Loss: 3.0729, LR: 0.000500
Epoch 18/100, Loss: 3.0761, LR: 0.000500
Epoch 19/100, Loss: 3.0765, LR: 0.000500
Epoch 20/100, Loss: 3.0743, LR: 0.000500
Epoch 21/100, Loss: 3.0718, LR: 0.000500
Epoch 22/100, Loss: 3.0750, LR: 0.000500
Epoch 23/100, Loss: 3.0756, LR: 0.000500
Epoch 24/100, Loss: 3.0743, LR: 0.000500
Epoch 25/100, Loss: 3.0741, LR: 0.000500
Epoch 26/100, Loss: 3.0759, LR: 0.000500
Epoch 27/100, Loss: 3.0741, LR: 0.000500
Epoch 28/100, Loss: 3.0747, LR: 0.000500
Epoch 29/100, Loss: 3.0747, LR: 0.000500
Epoch 30/100, Loss: 3.0720, LR: 0.000500
Epoch 31/100, Loss: 3.0715, LR: 0.000500
Epoch 32/100, Loss: 3.0747, LR: 0.000500
Epoch 33/100, Loss: 3.0723, LR: 0.000500
Epoch 34/100, Loss: 3.0734, LR: 0.000500
Epoch 35/100, Loss: 3.0740, LR: 0.000500
Epoch 36/100, Loss: 3.0733, LR: 0.000500
Epoch 37/100, Loss: 3.0738, LR: 0.000500
Epoch 38/100, Loss: 3.0716, LR: 0.000500
Epoch 39/100, Loss: 3.0708, LR: 0.000500
Epoch 40/100, Loss: 3.0739, LR: 0.000500
Epoch 41/100, Loss: 3.0720, LR: 0.000500
Epoch 42/100, Loss: 3.0727, LR: 0.000500
Epoch 43/100, Loss: 3.0745, LR: 0.000500
Epoch 44/100, Loss: 3.0740, LR: 0.000500
Epoch 45/100, Loss: 3.0722, LR: 0.000500
Epoch 46/100, Loss: 3.0738, LR: 0.000500
Epoch 47/100, Loss: 3.0730, LR: 0.000500
Epoch 48/100, Loss: 3.0744, LR: 0.000500
Epoch 49/100, Loss: 3.0732, LR: 0.000500
Epoch 50/100, Loss: 3.0727, LR: 0.000250
Epoch 51/100, Loss: 3.0725, LR: 0.000250
Epoch 52/100, Loss: 3.0701, LR: 0.000250
Epoch 53/100, Loss: 3.0708, LR: 0.000250
Epoch 54/100, Loss: 3.0724, LR: 0.000250
Epoch 55/100, Loss: 3.0715, LR: 0.000250
Epoch 56/100, Loss: 3.0706, LR: 0.000250
Epoch 57/100, Loss: 3.0732, LR: 0.000250
Epoch 58/100, Loss: 3.0741, LR: 0.000250
Epoch 59/100, Loss: 3.0722, LR: 0.000250
Epoch 60/100, Loss: 3.0731, LR: 0.000250
Epoch 61/100, Loss: 3.0697, LR: 0.000250
Epoch 62/100, Loss: 3.0704, LR: 0.000250
Epoch 63/100, Loss: 3.0723, LR: 0.000250
Epoch 64/100, Loss: 3.0713, LR: 0.000250
Epoch 65/100, Loss: 3.0721, LR: 0.000250
Epoch 66/100, Loss: 3.0713, LR: 0.000250
Epoch 67/100, Loss: 3.0702, LR: 0.000250
Epoch 68/100, Loss: 3.0693, LR: 0.000250
Epoch 69/100, Loss: 3.0736, LR: 0.000250
Epoch 70/100, Loss: 3.0739, LR: 0.000250
Epoch 71/100, Loss: 3.0735, LR: 0.000250
Epoch 72/100, Loss: 3.0703, LR: 0.000250
Epoch 73/100, Loss: 3.0707, LR: 0.000250
Epoch 74/100, Loss: 3.0695, LR: 0.000250
Epoch 75/100, Loss: 3.0721, LR: 0.000250
Epoch 76/100, Loss: 3.0713, LR: 0.000250
Epoch 77/100, Loss: 3.0730, LR: 0.000250
Epoch 78/100, Loss: 3.0735, LR: 0.000250
Epoch 79/100, Loss: 3.0714, LR: 0.000250
Epoch 80/100, Loss: 3.0711, LR: 0.000125
Epoch 81/100, Loss: 3.0719, LR: 0.000125
Epoch 82/100, Loss: 3.0724, LR: 0.000125
Epoch 83/100, Loss: 3.0712, LR: 0.000125
Epoch 84/100, Loss: 3.0719, LR: 0.000125
Epoch 85/100, Loss: 3.0721, LR: 0.000125
Epoch 86/100, Loss: 3.0714, LR: 0.000125
Epoch 87/100, Loss: 3.0712, LR: 0.000125
Epoch 88/100, Loss: 3.0705, LR: 0.000125
Epoch 89/100, Loss: 3.0710, LR: 0.000125
Epoch 90/100, Loss: 3.0731, LR: 0.000125
Epoch 91/100, Loss: 3.0716, LR: 0.000125
Epoch 92/100, Loss: 3.0717, LR: 0.000125
Epoch 93/100, Loss: 3.0718, LR: 0.000125
Epoch 94/100, Loss: 3.0737, LR: 0.000125
Epoch 95/100, Loss: 3.0723, LR: 0.000125
Epoch 96/100, Loss: 3.0726, LR: 0.000125
Epoch 97/100, Loss: 3.0726, LR: 0.000125
Epoch 98/100, Loss: 3.0724, LR: 0.000125
Epoch 99/100, Loss: 3.0729, LR: 0.000125
Epoch 100/100, Loss: 3.0720, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.435
Change BestPlayer
>> Model updated (win rate: 43.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.35
>> Cycle 17 completed. Checkpoint saved.

Train 18 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1000, Games per worker: 100
>> MCTS batch size: 64, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/100 games completed
Worker 10: 20/100 games completed
Worker 7: 20/100 games completed
Worker 5: 20/100 games completed
Worker 1: 20/100 games completed
Worker 4: 20/100 games completed
Worker 2: 20/100 games completed
Worker 6: 20/100 games completed
Worker 8: 20/100 games completed
Worker 3: 20/100 games completed
Worker 9: 20/100 games completed
Worker 10: 30/100 games completed
Worker 1: 30/100 games completed
Worker 7: 30/100 games completed
Worker 5: 30/100 games completed
Worker 8: 30/100 games completed
Worker 4: 30/100 games completed
Worker 2: 30/100 games completed
Worker 6: 30/100 games completed
Worker 3: 30/100 games completed
Worker 9: 30/100 games completed
Worker 7: 40/100 games completed
Worker 10: 40/100 games completed
Worker 6: 40/100 games completed
Worker 1: 40/100 games completed
Worker 8: 40/100 games completed
Worker 4: 40/100 games completed
Worker 5: 40/100 games completed
Worker 9: 40/100 games completed
Worker 2: 40/100 games completed
Worker 3: 40/100 games completed
Worker 7: 50/100 games completed
Worker 10: 50/100 games completed
Worker 5: 50/100 games completed
Worker 6: 50/100 games completed
Worker 1: 50/100 games completed
Worker 8: 50/100 games completed
Worker 4: 50/100 games completed
Worker 2: 50/100 games completed
Worker 9: 50/100 games completed
Worker 3: 50/100 games completed
Worker 6: 60/100 games completed
Worker 10: 60/100 games completed
Worker 5: 60/100 games completed
Worker 1: 60/100 games completed
Worker 7: 60/100 games completed
Worker 2: 60/100 games completed
Worker 8: 60/100 games completed
Worker 4: 60/100 games completed
Worker 3: 60/100 games completed
Worker 9: 60/100 games completed
Worker 6: 70/100 games completed
Worker 1: 70/100 games completed
Worker 5: 70/100 games completed
Worker 10: 70/100 games completed
Worker 7: 70/100 games completed
Worker 4: 70/100 games completed
Worker 2: 70/100 games completed
Worker 8: 70/100 games completed
Worker 9: 70/100 games completed
Worker 3: 70/100 games completed
Worker 6: 80/100 games completed
Worker 7: 80/100 games completed
Worker 1: 80/100 games completed
Worker 5: 80/100 games completed
Worker 10: 80/100 games completed
Worker 4: 80/100 games completed
Worker 9: 80/100 games completed
Worker 2: 80/100 games completed
Worker 8: 80/100 games completed
Worker 3: 80/100 games completed
Worker 7: 90/100 games completed
Worker 6: 90/100 games completed
Worker 10: 90/100 games completed
Worker 5: 90/100 games completed
Worker 1: 90/100 games completed
Worker 2: 90/100 games completed
Worker 4: 90/100 games completed
Worker 8: 90/100 games completed
Worker 9: 90/100 games completed
Worker 3: 90/100 games completed
Worker 7: 100/100 games completed
Worker 10: 100/100 games completed
Worker 5: 100/100 games completed
Worker 2: 100/100 games completed
Worker 6: 100/100 games completed
Worker 1: 100/100 games completed
Worker 4: 100/100 games completed
Worker 8: 100/100 games completed
Worker 9: 100/100 games completed
Worker 3: 100/100 games completed
>> Collected 54074 training samples from 1000 games
>> Saved to ./data/20251031035548.history
>> Train 18
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.1122, LR: 0.000500
Epoch 2/100, Loss: 3.1023, LR: 0.000500
Epoch 3/100, Loss: 3.0974, LR: 0.000500
Epoch 4/100, Loss: 3.0951, LR: 0.000500
Epoch 5/100, Loss: 3.0941, LR: 0.000500
Epoch 6/100, Loss: 3.0935, LR: 0.000500
Epoch 7/100, Loss: 3.0935, LR: 0.000500
Epoch 8/100, Loss: 3.0920, LR: 0.000500
Epoch 9/100, Loss: 3.0926, LR: 0.000500
Epoch 10/100, Loss: 3.0934, LR: 0.000500
Epoch 11/100, Loss: 3.0927, LR: 0.000500
Epoch 12/100, Loss: 3.0930, LR: 0.000500
Epoch 13/100, Loss: 3.0921, LR: 0.000500
Epoch 14/100, Loss: 3.0913, LR: 0.000500
Epoch 15/100, Loss: 3.0915, LR: 0.000500
Epoch 16/100, Loss: 3.0909, LR: 0.000500
Epoch 17/100, Loss: 3.0924, LR: 0.000500
Epoch 18/100, Loss: 3.0931, LR: 0.000500
Epoch 19/100, Loss: 3.0936, LR: 0.000500
Epoch 20/100, Loss: 3.0927, LR: 0.000500
Epoch 21/100, Loss: 3.0930, LR: 0.000500
Epoch 22/100, Loss: 3.0926, LR: 0.000500
Epoch 23/100, Loss: 3.0920, LR: 0.000500
Epoch 24/100, Loss: 3.0913, LR: 0.000500
Epoch 25/100, Loss: 3.0910, LR: 0.000500
Epoch 26/100, Loss: 3.0918, LR: 0.000500
Epoch 27/100, Loss: 3.0914, LR: 0.000500
Epoch 28/100, Loss: 3.0916, LR: 0.000500
Epoch 29/100, Loss: 3.0922, LR: 0.000500
Epoch 30/100, Loss: 3.0920, LR: 0.000500
Epoch 31/100, Loss: 3.0916, LR: 0.000500
Epoch 32/100, Loss: 3.0920, LR: 0.000500
Epoch 33/100, Loss: 3.0915, LR: 0.000500
Epoch 34/100, Loss: 3.0909, LR: 0.000500
Epoch 35/100, Loss: 3.0904, LR: 0.000500
Epoch 36/100, Loss: 3.0913, LR: 0.000500
Epoch 37/100, Loss: 3.0918, LR: 0.000500
Epoch 38/100, Loss: 3.0944, LR: 0.000500
Epoch 39/100, Loss: 3.0924, LR: 0.000500
Epoch 40/100, Loss: 3.0923, LR: 0.000500
Epoch 41/100, Loss: 3.0913, LR: 0.000500
Epoch 42/100, Loss: 3.0910, LR: 0.000500
Epoch 43/100, Loss: 3.0912, LR: 0.000500
Epoch 44/100, Loss: 3.0908, LR: 0.000500
Epoch 45/100, Loss: 3.0914, LR: 0.000500
Epoch 46/100, Loss: 3.0912, LR: 0.000500
Epoch 47/100, Loss: 3.0911, LR: 0.000500
Epoch 48/100, Loss: 3.0925, LR: 0.000500
Epoch 49/100, Loss: 3.0937, LR: 0.000500
Epoch 50/100, Loss: 3.0918, LR: 0.000250
Epoch 51/100, Loss: 3.0910, LR: 0.000250
Epoch 52/100, Loss: 3.0908, LR: 0.000250
Epoch 53/100, Loss: 3.0908, LR: 0.000250
Epoch 54/100, Loss: 3.0904, LR: 0.000250
Epoch 55/100, Loss: 3.0911, LR: 0.000250
Epoch 56/100, Loss: 3.0908, LR: 0.000250
Epoch 57/100, Loss: 3.0908, LR: 0.000250
Epoch 58/100, Loss: 3.0907, LR: 0.000250
Epoch 59/100, Loss: 3.0909, LR: 0.000250
Epoch 60/100, Loss: 3.0911, LR: 0.000250
Epoch 61/100, Loss: 3.0913, LR: 0.000250
Epoch 62/100, Loss: 3.0912, LR: 0.000250
Epoch 63/100, Loss: 3.0910, LR: 0.000250
Epoch 64/100, Loss: 3.0908, LR: 0.000250
Epoch 65/100, Loss: 3.0905, LR: 0.000250
Epoch 66/100, Loss: 3.0913, LR: 0.000250
Epoch 67/100, Loss: 3.0909, LR: 0.000250
Epoch 68/100, Loss: 3.0907, LR: 0.000250
Epoch 69/100, Loss: 3.0909, LR: 0.000250
Epoch 70/100, Loss: 3.0915, LR: 0.000250
Epoch 71/100, Loss: 3.0914, LR: 0.000250
Epoch 72/100, Loss: 3.0909, LR: 0.000250
Epoch 73/100, Loss: 3.0902, LR: 0.000250
Epoch 74/100, Loss: 3.0911, LR: 0.000250
Epoch 75/100, Loss: 3.0905, LR: 0.000250
Epoch 76/100, Loss: 3.0907, LR: 0.000250
Epoch 77/100, Loss: 3.0910, LR: 0.000250
Epoch 78/100, Loss: 3.0910, LR: 0.000250
Epoch 79/100, Loss: 3.0913, LR: 0.000250
Epoch 80/100, Loss: 3.0915, LR: 0.000125
Epoch 81/100, Loss: 3.0910, LR: 0.000125
Epoch 82/100, Loss: 3.0903, LR: 0.000125
Epoch 83/100, Loss: 3.0913, LR: 0.000125
Epoch 84/100, Loss: 3.0905, LR: 0.000125
Epoch 85/100, Loss: 3.0908, LR: 0.000125
Epoch 86/100, Loss: 3.0913, LR: 0.000125
Epoch 87/100, Loss: 3.0906, LR: 0.000125
Epoch 88/100, Loss: 3.0908, LR: 0.000125
Epoch 89/100, Loss: 3.0902, LR: 0.000125
Epoch 90/100, Loss: 3.0910, LR: 0.000125
Epoch 91/100, Loss: 3.0908, LR: 0.000125
Epoch 92/100, Loss: 3.0906, LR: 0.000125
Epoch 93/100, Loss: 3.0903, LR: 0.000125
Epoch 94/100, Loss: 3.0908, LR: 0.000125
Epoch 95/100, Loss: 3.0906, LR: 0.000125
Epoch 96/100, Loss: 3.0907, LR: 0.000125
Epoch 97/100, Loss: 3.0906, LR: 0.000125
Epoch 98/100, Loss: 3.0910, LR: 0.000125
Epoch 99/100, Loss: 3.0902, LR: 0.000125
Epoch 100/100, Loss: 3.0912, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.515
Change BestPlayer
>> Model updated (win rate: 51.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.25
>> Cycle 18 completed. Checkpoint saved.

Train 19 ====================================
>> Game Count: 1000
>> MCTS Simulations: 200
>> Learning Rate: 0.0005
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1000, Games per worker: 100
>> MCTS batch size: 64, PV evaluate count: 200
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/100 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/100 games completed
Worker 4: 20/100 games completed
Worker 7: 20/100 games completed
Worker 2: 20/100 games completed
Worker 6: 20/100 games completed
Worker 3: 20/100 games completed
Worker 1: 20/100 games completed
Worker 5: 20/100 games completed
Worker 10: 20/100 games completed
Worker 8: 20/100 games completed
Worker 9: 20/100 games completed
Worker 4: 30/100 games completed
Worker 2: 30/100 games completed
Worker 7: 30/100 games completed
Worker 6: 30/100 games completed
Worker 3: 30/100 games completed
Worker 5: 30/100 games completed
Worker 1: 30/100 games completed
Worker 10: 30/100 games completed
Worker 9: 30/100 games completed
Worker 8: 30/100 games completed
Worker 4: 40/100 games completed
Worker 6: 40/100 games completed
Worker 2: 40/100 games completed
Worker 7: 40/100 games completed
Worker 3: 40/100 games completed
Worker 5: 40/100 games completed
Worker 1: 40/100 games completed
Worker 9: 40/100 games completed
Worker 10: 40/100 games completed
Worker 8: 40/100 games completed
Worker 4: 50/100 games completed
Worker 6: 50/100 games completed
Worker 2: 50/100 games completed
Worker 7: 50/100 games completed
Worker 1: 50/100 games completed
Worker 3: 50/100 games completed
Worker 9: 50/100 games completed
Worker 5: 50/100 games completed
Worker 10: 50/100 games completed
Worker 8: 50/100 games completed
Worker 4: 60/100 games completed
Worker 6: 60/100 games completed
Worker 2: 60/100 games completed
Worker 7: 60/100 games completed
Worker 1: 60/100 games completed
Worker 9: 60/100 games completed
Worker 3: 60/100 games completed
Worker 10: 60/100 games completed
Worker 5: 60/100 games completed
Worker 8: 60/100 games completed
Worker 4: 70/100 games completed
Worker 6: 70/100 games completed
Worker 2: 70/100 games completed
Worker 1: 70/100 games completed
Worker 7: 70/100 games completed
Worker 9: 70/100 games completed
Worker 3: 70/100 games completed
Worker 5: 70/100 games completed
Worker 10: 70/100 games completed
Worker 8: 70/100 games completed
Worker 4: 80/100 games completed
Worker 2: 80/100 games completed
Worker 6: 80/100 games completed
Worker 1: 80/100 games completed
Worker 9: 80/100 games completed
Worker 7: 80/100 games completed
Worker 3: 80/100 games completed
Worker 5: 80/100 games completed
Worker 10: 80/100 games completed
Worker 8: 80/100 games completed
Worker 4: 90/100 games completed
Worker 2: 90/100 games completed
Worker 6: 90/100 games completed
Worker 1: 90/100 games completed
Worker 3: 90/100 games completed
Worker 9: 90/100 games completed
Worker 7: 90/100 games completed
Worker 5: 90/100 games completed
Worker 10: 90/100 games completed
Worker 8: 90/100 games completed
Worker 4: 100/100 games completed
Worker 1: 100/100 games completed
Worker 2: 100/100 games completed
Worker 6: 100/100 games completed
Worker 3: 100/100 games completed
Worker 5: 100/100 games completed
Worker 9: 100/100 games completed
Worker 7: 100/100 games completed
Worker 10: 100/100 games completed
Worker 8: 100/100 games completed
>> Collected 54955 training samples from 1000 games
>> Saved to ./data/20251031043245.history
>> Train 19
>> Learning Rate: 0.0005
Epoch 1/100, Loss: 3.0748, LR: 0.000500
Epoch 2/100, Loss: 3.0644, LR: 0.000500
Epoch 3/100, Loss: 3.0605, LR: 0.000500
Epoch 4/100, Loss: 3.0595, LR: 0.000500
Epoch 5/100, Loss: 3.0589, LR: 0.000500
Epoch 6/100, Loss: 3.0573, LR: 0.000500
Epoch 7/100, Loss: 3.0571, LR: 0.000500
Epoch 8/100, Loss: 3.0564, LR: 0.000500
Epoch 9/100, Loss: 3.0569, LR: 0.000500
Epoch 10/100, Loss: 3.0573, LR: 0.000500
Epoch 11/100, Loss: 3.0588, LR: 0.000500
Epoch 12/100, Loss: 3.0573, LR: 0.000500
Epoch 13/100, Loss: 3.0575, LR: 0.000500
Epoch 14/100, Loss: 3.0570, LR: 0.000500
Epoch 15/100, Loss: 3.0569, LR: 0.000500
Epoch 16/100, Loss: 3.0564, LR: 0.000500
Epoch 17/100, Loss: 3.0570, LR: 0.000500
Epoch 18/100, Loss: 3.0565, LR: 0.000500
Epoch 19/100, Loss: 3.0579, LR: 0.000500
Epoch 20/100, Loss: 3.0577, LR: 0.000500
Epoch 21/100, Loss: 3.0572, LR: 0.000500
Epoch 22/100, Loss: 3.0572, LR: 0.000500
Epoch 23/100, Loss: 3.0563, LR: 0.000500
Epoch 24/100, Loss: 3.0573, LR: 0.000500
Epoch 25/100, Loss: 3.0564, LR: 0.000500
Epoch 26/100, Loss: 3.0565, LR: 0.000500
Epoch 27/100, Loss: 3.0558, LR: 0.000500
Epoch 28/100, Loss: 3.0558, LR: 0.000500
Epoch 29/100, Loss: 3.0566, LR: 0.000500
Epoch 30/100, Loss: 3.0569, LR: 0.000500
Epoch 31/100, Loss: 3.0567, LR: 0.000500
Epoch 32/100, Loss: 3.0570, LR: 0.000500
Epoch 33/100, Loss: 3.0565, LR: 0.000500
Epoch 34/100, Loss: 3.0569, LR: 0.000500
Epoch 35/100, Loss: 3.0569, LR: 0.000500
Epoch 36/100, Loss: 3.0562, LR: 0.000500
Epoch 37/100, Loss: 3.0570, LR: 0.000500
Epoch 38/100, Loss: 3.0581, LR: 0.000500
Epoch 39/100, Loss: 3.0569, LR: 0.000500
Epoch 40/100, Loss: 3.0571, LR: 0.000500
Epoch 41/100, Loss: 3.0569, LR: 0.000500
Epoch 42/100, Loss: 3.0562, LR: 0.000500
Epoch 43/100, Loss: 3.0573, LR: 0.000500
Epoch 44/100, Loss: 3.0554, LR: 0.000500
Epoch 45/100, Loss: 3.0556, LR: 0.000500
Epoch 46/100, Loss: 3.0564, LR: 0.000500
Epoch 47/100, Loss: 3.0572, LR: 0.000500
Epoch 48/100, Loss: 3.0570, LR: 0.000500
Epoch 49/100, Loss: 3.0567, LR: 0.000500
Epoch 50/100, Loss: 3.0569, LR: 0.000250
Epoch 51/100, Loss: 3.0571, LR: 0.000250
Epoch 52/100, Loss: 3.0559, LR: 0.000250
Epoch 53/100, Loss: 3.0561, LR: 0.000250
Epoch 54/100, Loss: 3.0556, LR: 0.000250
Epoch 55/100, Loss: 3.0552, LR: 0.000250
Epoch 56/100, Loss: 3.0561, LR: 0.000250
Epoch 57/100, Loss: 3.0559, LR: 0.000250
Epoch 58/100, Loss: 3.0565, LR: 0.000250
Epoch 59/100, Loss: 3.0565, LR: 0.000250
Epoch 60/100, Loss: 3.0563, LR: 0.000250
Epoch 61/100, Loss: 3.0554, LR: 0.000250
Epoch 62/100, Loss: 3.0557, LR: 0.000250
Epoch 63/100, Loss: 3.0564, LR: 0.000250
Epoch 64/100, Loss: 3.0555, LR: 0.000250
Epoch 65/100, Loss: 3.0554, LR: 0.000250
Epoch 66/100, Loss: 3.0560, LR: 0.000250
Epoch 67/100, Loss: 3.0554, LR: 0.000250
Epoch 68/100, Loss: 3.0557, LR: 0.000250
Epoch 69/100, Loss: 3.0560, LR: 0.000250
Epoch 70/100, Loss: 3.0560, LR: 0.000250
Epoch 71/100, Loss: 3.0564, LR: 0.000250
Epoch 72/100, Loss: 3.0554, LR: 0.000250
Epoch 73/100, Loss: 3.0558, LR: 0.000250
Epoch 74/100, Loss: 3.0568, LR: 0.000250
Epoch 75/100, Loss: 3.0564, LR: 0.000250
Epoch 76/100, Loss: 3.0562, LR: 0.000250
Epoch 77/100, Loss: 3.0555, LR: 0.000250
Epoch 78/100, Loss: 3.0559, LR: 0.000250
Epoch 79/100, Loss: 3.0556, LR: 0.000250
Epoch 80/100, Loss: 3.0555, LR: 0.000125
Epoch 81/100, Loss: 3.0561, LR: 0.000125
Epoch 82/100, Loss: 3.0562, LR: 0.000125
Epoch 83/100, Loss: 3.0564, LR: 0.000125
Epoch 84/100, Loss: 3.0555, LR: 0.000125
Epoch 85/100, Loss: 3.0561, LR: 0.000125
Epoch 86/100, Loss: 3.0558, LR: 0.000125
Epoch 87/100, Loss: 3.0552, LR: 0.000125
Epoch 88/100, Loss: 3.0547, LR: 0.000125
Epoch 89/100, Loss: 3.0564, LR: 0.000125
Epoch 90/100, Loss: 3.0559, LR: 0.000125
Epoch 91/100, Loss: 3.0555, LR: 0.000125
Epoch 92/100, Loss: 3.0554, LR: 0.000125
Epoch 93/100, Loss: 3.0558, LR: 0.000125
Epoch 94/100, Loss: 3.0565, LR: 0.000125
Epoch 95/100, Loss: 3.0559, LR: 0.000125
Epoch 96/100, Loss: 3.0555, LR: 0.000125
Epoch 97/100, Loss: 3.0557, LR: 0.000125
Epoch 98/100, Loss: 3.0558, LR: 0.000125
Epoch 99/100, Loss: 3.0559, LR: 0.000125
Epoch 100/100, Loss: 3.0556, LR: 0.000125
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.565
Change BestPlayer
>> Model updated (win rate: 56.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.55
>> Cycle 19 completed. Checkpoint saved.

Train 20 ====================================
>> Game Count: 1500
>> MCTS Simulations: 400
>> Learning Rate: 0.0002
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1500, Games per worker: 150
>> MCTS batch size: 64, PV evaluate count: 400
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/150 games completed
Worker 3: 20/150 games completed
Worker 9: 20/150 games completed
Worker 1: 20/150 games completed
Worker 5: 20/150 games completed
Worker 7: 20/150 games completed
Worker 10: 20/150 games completed
Worker 4: 20/150 games completed
Worker 2: 20/150 games completed
Worker 6: 20/150 games completed
Worker 8: 20/150 games completed
Worker 9: 30/150 games completed
Worker 3: 30/150 games completed
Worker 5: 30/150 games completed
Worker 1: 30/150 games completed
Worker 7: 30/150 games completed
Worker 10: 30/150 games completed
Worker 2: 30/150 games completed
Worker 8: 30/150 games completed
Worker 4: 30/150 games completed
Worker 6: 30/150 games completed
Worker 5: 40/150 games completed
Worker 9: 40/150 games completed
Worker 3: 40/150 games completed
Worker 1: 40/150 games completed
Worker 10: 40/150 games completed
Worker 7: 40/150 games completed
Worker 8: 40/150 games completed
Worker 2: 40/150 games completed
Worker 4: 40/150 games completed
Worker 6: 40/150 games completed
Worker 5: 50/150 games completed
Worker 9: 50/150 games completed
Worker 3: 50/150 games completed
Worker 1: 50/150 games completed
Worker 10: 50/150 games completed
Worker 4: 50/150 games completed
Worker 2: 50/150 games completed
Worker 6: 50/150 games completed
Worker 7: 50/150 games completed
Worker 8: 50/150 games completed
Worker 5: 60/150 games completed
Worker 9: 60/150 games completed
Worker 10: 60/150 games completed
Worker 1: 60/150 games completed
Worker 3: 60/150 games completed
Worker 7: 60/150 games completed
Worker 6: 60/150 games completed
Worker 4: 60/150 games completed
Worker 2: 60/150 games completed
Worker 8: 60/150 games completed
Worker 9: 70/150 games completed
Worker 5: 70/150 games completed
Worker 1: 70/150 games completed
Worker 6: 70/150 games completed
Worker 10: 70/150 games completed
Worker 3: 70/150 games completed
Worker 7: 70/150 games completed
Worker 4: 70/150 games completed
Worker 8: 70/150 games completed
Worker 2: 70/150 games completed
Worker 5: 80/150 games completed
Worker 9: 80/150 games completed
Worker 3: 80/150 games completed
Worker 6: 80/150 games completed
Worker 1: 80/150 games completed
Worker 7: 80/150 games completed
Worker 10: 80/150 games completed
Worker 4: 80/150 games completed
Worker 8: 80/150 games completed
Worker 2: 80/150 games completed
Worker 5: 90/150 games completed
Worker 9: 90/150 games completed
Worker 3: 90/150 games completed
Worker 1: 90/150 games completed
Worker 6: 90/150 games completed
Worker 7: 90/150 games completed
Worker 10: 90/150 games completed
Worker 4: 90/150 games completed
Worker 8: 90/150 games completed
Worker 2: 90/150 games completed
Worker 5: 100/150 games completed
Worker 9: 100/150 games completed
Worker 3: 100/150 games completed
Worker 1: 100/150 games completed
Worker 6: 100/150 games completed
Worker 7: 100/150 games completed
Worker 10: 100/150 games completed
Worker 4: 100/150 games completed
Worker 8: 100/150 games completed
Worker 2: 100/150 games completed
Worker 5: 110/150 games completed
Worker 9: 110/150 games completed
Worker 1: 110/150 games completed
Worker 6: 110/150 games completed
Worker 7: 110/150 games completed
Worker 3: 110/150 games completed
Worker 10: 110/150 games completed
Worker 8: 110/150 games completed
Worker 4: 110/150 games completed
Worker 2: 110/150 games completed
Worker 9: 120/150 games completed
Worker 5: 120/150 games completed
Worker 1: 120/150 games completed
Worker 6: 120/150 games completed
Worker 3: 120/150 games completed
Worker 10: 120/150 games completed
Worker 7: 120/150 games completed
Worker 8: 120/150 games completed
Worker 4: 120/150 games completed
Worker 2: 120/150 games completed
Worker 5: 130/150 games completed
Worker 9: 130/150 games completed
Worker 1: 130/150 games completed
Worker 6: 130/150 games completed
Worker 3: 130/150 games completed
Worker 10: 130/150 games completed
Worker 7: 130/150 games completed
Worker 8: 130/150 games completed
Worker 4: 130/150 games completed
Worker 2: 130/150 games completed
Worker 9: 140/150 games completed
Worker 5: 140/150 games completed
Worker 1: 140/150 games completed
Worker 6: 140/150 games completed
Worker 7: 140/150 games completed
Worker 3: 140/150 games completed
Worker 10: 140/150 games completed
Worker 8: 140/150 games completed
Worker 4: 140/150 games completed
Worker 2: 140/150 games completed
Worker 9: 150/150 games completed
Worker 5: 150/150 games completed
Worker 1: 150/150 games completed
Worker 6: 150/150 games completed
Worker 3: 150/150 games completed
Worker 8: 150/150 games completed
Worker 7: 150/150 games completed
Worker 10: 150/150 games completed
Worker 4: 150/150 games completed
Worker 2: 150/150 games completed
>> Collected 88038 training samples from 1500 games
>> Saved to ./data/20251031053214.history
>> Train 20
>> Learning Rate: 0.0002
Epoch 1/100, Loss: 3.6964, LR: 0.000200
Epoch 2/100, Loss: 3.5867, LR: 0.000200
Epoch 3/100, Loss: 3.5807, LR: 0.000200
Epoch 4/100, Loss: 3.5771, LR: 0.000200
Epoch 5/100, Loss: 3.5743, LR: 0.000200
Epoch 6/100, Loss: 3.5721, LR: 0.000200
Epoch 7/100, Loss: 3.5703, LR: 0.000200
Epoch 8/100, Loss: 3.5688, LR: 0.000200
Epoch 9/100, Loss: 3.5676, LR: 0.000200
Epoch 10/100, Loss: 3.5667, LR: 0.000200
Epoch 11/100, Loss: 3.5660, LR: 0.000200
Epoch 12/100, Loss: 3.5655, LR: 0.000200
Epoch 13/100, Loss: 3.5653, LR: 0.000200
Epoch 14/100, Loss: 3.5647, LR: 0.000200
Epoch 15/100, Loss: 3.5643, LR: 0.000200
Epoch 16/100, Loss: 3.5642, LR: 0.000200
Epoch 17/100, Loss: 3.5639, LR: 0.000200
Epoch 18/100, Loss: 3.5637, LR: 0.000200
Epoch 19/100, Loss: 3.5637, LR: 0.000200
Epoch 20/100, Loss: 3.5636, LR: 0.000200
Epoch 21/100, Loss: 3.5632, LR: 0.000200
Epoch 22/100, Loss: 3.5633, LR: 0.000200
Epoch 23/100, Loss: 3.5630, LR: 0.000200
Epoch 24/100, Loss: 3.5630, LR: 0.000200
Epoch 25/100, Loss: 3.5630, LR: 0.000200
Epoch 26/100, Loss: 3.5628, LR: 0.000200
Epoch 27/100, Loss: 3.5629, LR: 0.000200
Epoch 28/100, Loss: 3.5627, LR: 0.000200
Epoch 29/100, Loss: 3.5626, LR: 0.000200
Epoch 30/100, Loss: 3.5626, LR: 0.000200
Epoch 31/100, Loss: 3.5626, LR: 0.000200
Epoch 32/100, Loss: 3.5624, LR: 0.000200
Epoch 33/100, Loss: 3.5623, LR: 0.000200
Epoch 34/100, Loss: 3.5624, LR: 0.000200
Epoch 35/100, Loss: 3.5624, LR: 0.000200
Epoch 36/100, Loss: 3.5624, LR: 0.000200
Epoch 37/100, Loss: 3.5622, LR: 0.000200
Epoch 38/100, Loss: 3.5622, LR: 0.000200
Epoch 39/100, Loss: 3.5623, LR: 0.000200
Epoch 40/100, Loss: 3.5622, LR: 0.000200
Epoch 41/100, Loss: 3.5622, LR: 0.000200
Epoch 42/100, Loss: 3.5622, LR: 0.000200
Epoch 43/100, Loss: 3.5624, LR: 0.000200
Epoch 44/100, Loss: 3.5623, LR: 0.000200
Epoch 45/100, Loss: 3.5620, LR: 0.000200
Epoch 46/100, Loss: 3.5619, LR: 0.000200
Epoch 47/100, Loss: 3.5620, LR: 0.000200
Epoch 48/100, Loss: 3.5621, LR: 0.000200
Epoch 49/100, Loss: 3.5622, LR: 0.000200
Epoch 50/100, Loss: 3.5621, LR: 0.000100
Epoch 51/100, Loss: 3.5619, LR: 0.000100
Epoch 52/100, Loss: 3.5618, LR: 0.000100
Epoch 53/100, Loss: 3.5617, LR: 0.000100
Epoch 54/100, Loss: 3.5618, LR: 0.000100
Epoch 55/100, Loss: 3.5617, LR: 0.000100
Epoch 56/100, Loss: 3.5618, LR: 0.000100
Epoch 57/100, Loss: 3.5617, LR: 0.000100
Epoch 58/100, Loss: 3.5617, LR: 0.000100
Epoch 59/100, Loss: 3.5618, LR: 0.000100
Epoch 60/100, Loss: 3.5617, LR: 0.000100
Epoch 61/100, Loss: 3.5617, LR: 0.000100
Epoch 62/100, Loss: 3.5617, LR: 0.000100
Epoch 63/100, Loss: 3.5618, LR: 0.000100
Epoch 64/100, Loss: 3.5617, LR: 0.000100
Epoch 65/100, Loss: 3.5617, LR: 0.000100
Epoch 66/100, Loss: 3.5617, LR: 0.000100
Epoch 67/100, Loss: 3.5617, LR: 0.000100
Epoch 68/100, Loss: 3.5617, LR: 0.000100
Epoch 69/100, Loss: 3.5617, LR: 0.000100
Epoch 70/100, Loss: 3.5616, LR: 0.000100
Epoch 71/100, Loss: 3.5617, LR: 0.000100
Epoch 72/100, Loss: 3.5616, LR: 0.000100
Epoch 73/100, Loss: 3.5616, LR: 0.000100
Epoch 74/100, Loss: 3.5615, LR: 0.000100
Epoch 75/100, Loss: 3.5617, LR: 0.000100
Epoch 76/100, Loss: 3.5616, LR: 0.000100
Epoch 77/100, Loss: 3.5617, LR: 0.000100
Epoch 78/100, Loss: 3.5616, LR: 0.000100
Epoch 79/100, Loss: 3.5616, LR: 0.000100
Epoch 80/100, Loss: 3.5617, LR: 0.000050
Epoch 81/100, Loss: 3.5616, LR: 0.000050
Epoch 82/100, Loss: 3.5616, LR: 0.000050
Epoch 83/100, Loss: 3.5615, LR: 0.000050
Epoch 84/100, Loss: 3.5615, LR: 0.000050
Epoch 85/100, Loss: 3.5614, LR: 0.000050
Epoch 86/100, Loss: 3.5615, LR: 0.000050
Epoch 87/100, Loss: 3.5614, LR: 0.000050
Epoch 88/100, Loss: 3.5616, LR: 0.000050
Epoch 89/100, Loss: 3.5615, LR: 0.000050
Epoch 90/100, Loss: 3.5615, LR: 0.000050
Epoch 91/100, Loss: 3.5615, LR: 0.000050
Epoch 92/100, Loss: 3.5615, LR: 0.000050
Epoch 93/100, Loss: 3.5615, LR: 0.000050
Epoch 94/100, Loss: 3.5616, LR: 0.000050
Epoch 95/100, Loss: 3.5616, LR: 0.000050
Epoch 96/100, Loss: 3.5614, LR: 0.000050
Epoch 97/100, Loss: 3.5615, LR: 0.000050
Epoch 98/100, Loss: 3.5615, LR: 0.000050
Epoch 99/100, Loss: 3.5615, LR: 0.000050
Epoch 100/100, Loss: 3.5615, LR: 0.000050
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.52
Change BestPlayer
>> Model updated (win rate: 52.0%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.4
>> Cycle 20 completed. Checkpoint saved.

Train 21 ====================================
>> Game Count: 1500
>> MCTS Simulations: 400
>> Learning Rate: 0.0002
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1500, Games per worker: 150
>> MCTS batch size: 64, PV evaluate count: 400
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/150 games completed
Worker 10: 20/150 games completed
Worker 6: 20/150 games completed
Worker 2: 20/150 games completed
Worker 5: 20/150 games completed
Worker 4: 20/150 games completed
Worker 1: 20/150 games completed
Worker 3: 20/150 games completed
Worker 9: 20/150 games completed
Worker 8: 20/150 games completed
Worker 7: 20/150 games completed
Worker 10: 30/150 games completed
Worker 2: 30/150 games completed
Worker 6: 30/150 games completed
Worker 5: 30/150 games completed
Worker 4: 30/150 games completed
Worker 8: 30/150 games completed
Worker 7: 30/150 games completed
Worker 1: 30/150 games completed
Worker 9: 30/150 games completed
Worker 3: 30/150 games completed
Worker 10: 40/150 games completed
Worker 2: 40/150 games completed
Worker 5: 40/150 games completed
Worker 4: 40/150 games completed
Worker 6: 40/150 games completed
Worker 8: 40/150 games completed
Worker 7: 40/150 games completed
Worker 1: 40/150 games completed
Worker 3: 40/150 games completed
Worker 9: 40/150 games completed
Worker 2: 50/150 games completed
Worker 10: 50/150 games completed
Worker 5: 50/150 games completed
Worker 6: 50/150 games completed
Worker 4: 50/150 games completed
Worker 8: 50/150 games completed
Worker 1: 50/150 games completed
Worker 7: 50/150 games completed
Worker 9: 50/150 games completed
Worker 3: 50/150 games completed
Worker 2: 60/150 games completed
Worker 10: 60/150 games completed
Worker 6: 60/150 games completed
Worker 5: 60/150 games completed
Worker 1: 60/150 games completed
Worker 7: 60/150 games completed
Worker 4: 60/150 games completed
Worker 8: 60/150 games completed
Worker 9: 60/150 games completed
Worker 3: 60/150 games completed
Worker 6: 70/150 games completed
Worker 2: 70/150 games completed
Worker 5: 70/150 games completed
Worker 7: 70/150 games completed
Worker 10: 70/150 games completed
Worker 8: 70/150 games completed
Worker 1: 70/150 games completed
Worker 4: 70/150 games completed
Worker 9: 70/150 games completed
Worker 3: 70/150 games completed
Worker 6: 80/150 games completed
Worker 5: 80/150 games completed
Worker 2: 80/150 games completed
Worker 7: 80/150 games completed
Worker 10: 80/150 games completed
Worker 1: 80/150 games completed
Worker 8: 80/150 games completed
Worker 4: 80/150 games completed
Worker 9: 80/150 games completed
Worker 3: 80/150 games completed
Worker 2: 90/150 games completed
Worker 6: 90/150 games completed
Worker 10: 90/150 games completed
Worker 5: 90/150 games completed
Worker 7: 90/150 games completed
Worker 1: 90/150 games completed
Worker 3: 90/150 games completed
Worker 4: 90/150 games completed
Worker 8: 90/150 games completed
Worker 9: 90/150 games completed
Worker 2: 100/150 games completed
Worker 6: 100/150 games completed
Worker 5: 100/150 games completed
Worker 10: 100/150 games completed
Worker 1: 100/150 games completed
Worker 7: 100/150 games completed
Worker 3: 100/150 games completed
Worker 4: 100/150 games completed
Worker 8: 100/150 games completed
Worker 9: 100/150 games completed
Worker 2: 110/150 games completed
Worker 5: 110/150 games completed
Worker 6: 110/150 games completed
Worker 1: 110/150 games completed
Worker 10: 110/150 games completed
Worker 7: 110/150 games completed
Worker 3: 110/150 games completed
Worker 4: 110/150 games completed
Worker 8: 110/150 games completed
Worker 9: 110/150 games completed
Worker 2: 120/150 games completed
Worker 6: 120/150 games completed
Worker 5: 120/150 games completed
Worker 1: 120/150 games completed
Worker 7: 120/150 games completed
Worker 4: 120/150 games completed
Worker 3: 120/150 games completed
Worker 10: 120/150 games completed
Worker 8: 120/150 games completed
Worker 9: 120/150 games completed
Worker 2: 130/150 games completed
Worker 6: 130/150 games completed
Worker 1: 130/150 games completed
Worker 5: 130/150 games completed
Worker 10: 130/150 games completed
Worker 7: 130/150 games completed
Worker 4: 130/150 games completed
Worker 3: 130/150 games completed
Worker 8: 130/150 games completed
Worker 9: 130/150 games completed
Worker 2: 140/150 games completed
Worker 6: 140/150 games completed
Worker 1: 140/150 games completed
Worker 7: 140/150 games completed
Worker 5: 140/150 games completed
Worker 10: 140/150 games completed
Worker 4: 140/150 games completed
Worker 3: 140/150 games completed
Worker 8: 140/150 games completed
Worker 9: 140/150 games completed
Worker 2: 150/150 games completed
Worker 1: 150/150 games completed
Worker 6: 150/150 games completed
Worker 7: 150/150 games completed
Worker 5: 150/150 games completed
Worker 10: 150/150 games completed
Worker 3: 150/150 games completed
Worker 8: 150/150 games completed
Worker 4: 150/150 games completed
Worker 9: 150/150 games completed
>> Collected 88385 training samples from 1500 games
>> Saved to ./data/20251031064214.history
>> Train 21
>> Learning Rate: 0.0002
Epoch 1/100, Loss: 3.5754, LR: 0.000200
Epoch 2/100, Loss: 3.5703, LR: 0.000200
Epoch 3/100, Loss: 3.5678, LR: 0.000200
Epoch 4/100, Loss: 3.5660, LR: 0.000200
Epoch 5/100, Loss: 3.5646, LR: 0.000200
Epoch 6/100, Loss: 3.5638, LR: 0.000200
Epoch 7/100, Loss: 3.5632, LR: 0.000200
Epoch 8/100, Loss: 3.5627, LR: 0.000200
Epoch 9/100, Loss: 3.5627, LR: 0.000200
Epoch 10/100, Loss: 3.5622, LR: 0.000200
Epoch 11/100, Loss: 3.5622, LR: 0.000200
Epoch 12/100, Loss: 3.5620, LR: 0.000200
Epoch 13/100, Loss: 3.5617, LR: 0.000200
Epoch 14/100, Loss: 3.5616, LR: 0.000200
Epoch 15/100, Loss: 3.5617, LR: 0.000200
Epoch 16/100, Loss: 3.5614, LR: 0.000200
Epoch 17/100, Loss: 3.5615, LR: 0.000200
Epoch 18/100, Loss: 3.5611, LR: 0.000200
Epoch 19/100, Loss: 3.5612, LR: 0.000200
Epoch 20/100, Loss: 3.5613, LR: 0.000200
Epoch 21/100, Loss: 3.5609, LR: 0.000200
Epoch 22/100, Loss: 3.5611, LR: 0.000200
Epoch 23/100, Loss: 3.5608, LR: 0.000200
Epoch 24/100, Loss: 3.5609, LR: 0.000200
Epoch 25/100, Loss: 3.5610, LR: 0.000200
Epoch 26/100, Loss: 3.5608, LR: 0.000200
Epoch 27/100, Loss: 3.5607, LR: 0.000200
Epoch 28/100, Loss: 3.5611, LR: 0.000200
Epoch 29/100, Loss: 3.5604, LR: 0.000200
Epoch 30/100, Loss: 3.5607, LR: 0.000200
Epoch 31/100, Loss: 3.5609, LR: 0.000200
Epoch 32/100, Loss: 3.5606, LR: 0.000200
Epoch 33/100, Loss: 3.5608, LR: 0.000200
Epoch 34/100, Loss: 3.5604, LR: 0.000200
Epoch 35/100, Loss: 3.5606, LR: 0.000200
Epoch 36/100, Loss: 3.5605, LR: 0.000200
Epoch 37/100, Loss: 3.5607, LR: 0.000200
Epoch 38/100, Loss: 3.5605, LR: 0.000200
Epoch 39/100, Loss: 3.5608, LR: 0.000200
Epoch 40/100, Loss: 3.5604, LR: 0.000200
Epoch 41/100, Loss: 3.5604, LR: 0.000200
Epoch 42/100, Loss: 3.5605, LR: 0.000200
Epoch 43/100, Loss: 3.5605, LR: 0.000200
Epoch 44/100, Loss: 3.5602, LR: 0.000200
Epoch 45/100, Loss: 3.5605, LR: 0.000200
Epoch 46/100, Loss: 3.5605, LR: 0.000200
Epoch 47/100, Loss: 3.5603, LR: 0.000200
Epoch 48/100, Loss: 3.5605, LR: 0.000200
Epoch 49/100, Loss: 3.5602, LR: 0.000200
Epoch 50/100, Loss: 3.5605, LR: 0.000100
Epoch 51/100, Loss: 3.5600, LR: 0.000100
Epoch 52/100, Loss: 3.5604, LR: 0.000100
Epoch 53/100, Loss: 3.5602, LR: 0.000100
Epoch 54/100, Loss: 3.5602, LR: 0.000100
Epoch 55/100, Loss: 3.5602, LR: 0.000100
Epoch 56/100, Loss: 3.5601, LR: 0.000100
Epoch 57/100, Loss: 3.5603, LR: 0.000100
Epoch 58/100, Loss: 3.5601, LR: 0.000100
Epoch 59/100, Loss: 3.5602, LR: 0.000100
Epoch 60/100, Loss: 3.5599, LR: 0.000100
Epoch 61/100, Loss: 3.5602, LR: 0.000100
Epoch 62/100, Loss: 3.5604, LR: 0.000100
Epoch 63/100, Loss: 3.5600, LR: 0.000100
Epoch 64/100, Loss: 3.5601, LR: 0.000100
Epoch 65/100, Loss: 3.5601, LR: 0.000100
Epoch 66/100, Loss: 3.5601, LR: 0.000100
Epoch 67/100, Loss: 3.5600, LR: 0.000100
Epoch 68/100, Loss: 3.5600, LR: 0.000100
Epoch 69/100, Loss: 3.5600, LR: 0.000100
Epoch 70/100, Loss: 3.5601, LR: 0.000100
Epoch 71/100, Loss: 3.5601, LR: 0.000100
Epoch 72/100, Loss: 3.5602, LR: 0.000100
Epoch 73/100, Loss: 3.5601, LR: 0.000100
Epoch 74/100, Loss: 3.5602, LR: 0.000100
Epoch 75/100, Loss: 3.5601, LR: 0.000100
Epoch 76/100, Loss: 3.5601, LR: 0.000100
Epoch 77/100, Loss: 3.5601, LR: 0.000100
Epoch 78/100, Loss: 3.5602, LR: 0.000100
Epoch 79/100, Loss: 3.5604, LR: 0.000100
Epoch 80/100, Loss: 3.5600, LR: 0.000050
Epoch 81/100, Loss: 3.5601, LR: 0.000050
Epoch 82/100, Loss: 3.5599, LR: 0.000050
Epoch 83/100, Loss: 3.5599, LR: 0.000050
Epoch 84/100, Loss: 3.5600, LR: 0.000050
Epoch 85/100, Loss: 3.5601, LR: 0.000050
Epoch 86/100, Loss: 3.5603, LR: 0.000050
Epoch 87/100, Loss: 3.5601, LR: 0.000050
Epoch 88/100, Loss: 3.5602, LR: 0.000050
Epoch 89/100, Loss: 3.5600, LR: 0.000050
Epoch 90/100, Loss: 3.5600, LR: 0.000050
Epoch 91/100, Loss: 3.5601, LR: 0.000050
Epoch 92/100, Loss: 3.5601, LR: 0.000050
Epoch 93/100, Loss: 3.5601, LR: 0.000050
Epoch 94/100, Loss: 3.5601, LR: 0.000050
Epoch 95/100, Loss: 3.5599, LR: 0.000050
Epoch 96/100, Loss: 3.5600, LR: 0.000050
Epoch 97/100, Loss: 3.5603, LR: 0.000050
Epoch 98/100, Loss: 3.5601, LR: 0.000050
Epoch 99/100, Loss: 3.5603, LR: 0.000050
Epoch 100/100, Loss: 3.5602, LR: 0.000050
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.475
Change BestPlayer
>> Model updated (win rate: 47.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.35
>> Cycle 21 completed. Checkpoint saved.

Train 22 ====================================
>> Game Count: 1500
>> MCTS Simulations: 400
>> Learning Rate: 0.0002
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1500, Games per worker: 150
>> MCTS batch size: 64, PV evaluate count: 400
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/150 games completed
Worker 10: 20/150 games completed
Worker 3: 20/150 games completed
Worker 4: 20/150 games completed
Worker 8: 20/150 games completed
Worker 7: 20/150 games completed
Worker 9: 20/150 games completed
Worker 1: 20/150 games completed
Worker 6: 20/150 games completed
Worker 2: 20/150 games completed
Worker 5: 20/150 games completed
Worker 4: 30/150 games completed
Worker 3: 30/150 games completed
Worker 10: 30/150 games completed
Worker 8: 30/150 games completed
Worker 1: 30/150 games completed
Worker 7: 30/150 games completed
Worker 6: 30/150 games completed
Worker 9: 30/150 games completed
Worker 2: 30/150 games completed
Worker 5: 30/150 games completed
Worker 4: 40/150 games completed
Worker 10: 40/150 games completed
Worker 3: 40/150 games completed
Worker 1: 40/150 games completed
Worker 8: 40/150 games completed
Worker 9: 40/150 games completed
Worker 2: 40/150 games completed
Worker 7: 40/150 games completed
Worker 6: 40/150 games completed
Worker 5: 40/150 games completed
Worker 4: 50/150 games completed
Worker 10: 50/150 games completed
Worker 3: 50/150 games completed
Worker 1: 50/150 games completed
Worker 9: 50/150 games completed
Worker 2: 50/150 games completed
Worker 8: 50/150 games completed
Worker 7: 50/150 games completed
Worker 5: 50/150 games completed
Worker 6: 50/150 games completed
Worker 4: 60/150 games completed
Worker 10: 60/150 games completed
Worker 3: 60/150 games completed
Worker 1: 60/150 games completed
Worker 2: 60/150 games completed
Worker 8: 60/150 games completed
Worker 9: 60/150 games completed
Worker 7: 60/150 games completed
Worker 5: 60/150 games completed
Worker 6: 60/150 games completed
Worker 10: 70/150 games completed
Worker 3: 70/150 games completed
Worker 4: 70/150 games completed
Worker 1: 70/150 games completed
Worker 8: 70/150 games completed
Worker 2: 70/150 games completed
Worker 9: 70/150 games completed
Worker 7: 70/150 games completed
Worker 5: 70/150 games completed
Worker 6: 70/150 games completed
Worker 10: 80/150 games completed
Worker 4: 80/150 games completed
Worker 3: 80/150 games completed
Worker 1: 80/150 games completed
Worker 8: 80/150 games completed
Worker 2: 80/150 games completed
Worker 7: 80/150 games completed
Worker 9: 80/150 games completed
Worker 5: 80/150 games completed
Worker 6: 80/150 games completed
Worker 4: 90/150 games completed
Worker 10: 90/150 games completed
Worker 1: 90/150 games completed
Worker 3: 90/150 games completed
Worker 2: 90/150 games completed
Worker 8: 90/150 games completed
Worker 7: 90/150 games completed
Worker 6: 90/150 games completed
Worker 9: 90/150 games completed
Worker 5: 90/150 games completed
Worker 4: 100/150 games completed
Worker 1: 100/150 games completed
Worker 10: 100/150 games completed
Worker 3: 100/150 games completed
Worker 8: 100/150 games completed
Worker 2: 100/150 games completed
Worker 7: 100/150 games completed
Worker 6: 100/150 games completed
Worker 9: 100/150 games completed
Worker 5: 100/150 games completed
Worker 1: 110/150 games completed
Worker 4: 110/150 games completed
Worker 10: 110/150 games completed
Worker 3: 110/150 games completed
Worker 8: 110/150 games completed
Worker 7: 110/150 games completed
Worker 2: 110/150 games completed
Worker 6: 110/150 games completed
Worker 9: 110/150 games completed
Worker 5: 110/150 games completed
Worker 1: 120/150 games completed
Worker 4: 120/150 games completed
Worker 10: 120/150 games completed
Worker 3: 120/150 games completed
Worker 8: 120/150 games completed
Worker 2: 120/150 games completed
Worker 6: 120/150 games completed
Worker 7: 120/150 games completed
Worker 9: 120/150 games completed
Worker 5: 120/150 games completed
Worker 1: 130/150 games completed
Worker 4: 130/150 games completed
Worker 3: 130/150 games completed
Worker 10: 130/150 games completed
Worker 8: 130/150 games completed
Worker 7: 130/150 games completed
Worker 2: 130/150 games completed
Worker 6: 130/150 games completed
Worker 9: 130/150 games completed
Worker 5: 130/150 games completed
Worker 1: 140/150 games completed
Worker 4: 140/150 games completed
Worker 3: 140/150 games completed
Worker 10: 140/150 games completed
Worker 8: 140/150 games completed
Worker 7: 140/150 games completed
Worker 2: 140/150 games completed
Worker 6: 140/150 games completed
Worker 9: 140/150 games completed
Worker 5: 140/150 games completed
Worker 1: 150/150 games completed
Worker 4: 150/150 games completed
Worker 3: 150/150 games completed
Worker 7: 150/150 games completed
Worker 10: 150/150 games completed
Worker 8: 150/150 games completed
Worker 2: 150/150 games completed
Worker 6: 150/150 games completed
Worker 5: 150/150 games completed
Worker 9: 150/150 games completed
>> Collected 88009 training samples from 1500 games
>> Saved to ./data/20251031075219.history
>> Train 22
>> Learning Rate: 0.0002
Epoch 1/100, Loss: 3.5933, LR: 0.000200
Epoch 2/100, Loss: 3.5886, LR: 0.000200
Epoch 3/100, Loss: 3.5865, LR: 0.000200
Epoch 4/100, Loss: 3.5846, LR: 0.000200
Epoch 5/100, Loss: 3.5833, LR: 0.000200
Epoch 6/100, Loss: 3.5830, LR: 0.000200
Epoch 7/100, Loss: 3.5820, LR: 0.000200
Epoch 8/100, Loss: 3.5815, LR: 0.000200
Epoch 9/100, Loss: 3.5816, LR: 0.000200
Epoch 10/100, Loss: 3.5813, LR: 0.000200
Epoch 11/100, Loss: 3.5809, LR: 0.000200
Epoch 12/100, Loss: 3.5806, LR: 0.000200
Epoch 13/100, Loss: 3.5808, LR: 0.000200
Epoch 14/100, Loss: 3.5807, LR: 0.000200
Epoch 15/100, Loss: 3.5809, LR: 0.000200
Epoch 16/100, Loss: 3.5807, LR: 0.000200
Epoch 17/100, Loss: 3.5802, LR: 0.000200
Epoch 18/100, Loss: 3.5803, LR: 0.000200
Epoch 19/100, Loss: 3.5805, LR: 0.000200
Epoch 20/100, Loss: 3.5804, LR: 0.000200
Epoch 21/100, Loss: 3.5805, LR: 0.000200
Epoch 22/100, Loss: 3.5801, LR: 0.000200
Epoch 23/100, Loss: 3.5797, LR: 0.000200
Epoch 24/100, Loss: 3.5797, LR: 0.000200
Epoch 25/100, Loss: 3.5802, LR: 0.000200
Epoch 26/100, Loss: 3.5801, LR: 0.000200
Epoch 27/100, Loss: 3.5798, LR: 0.000200
Epoch 28/100, Loss: 3.5799, LR: 0.000200
Epoch 29/100, Loss: 3.5799, LR: 0.000200
Epoch 30/100, Loss: 3.5799, LR: 0.000200
Epoch 31/100, Loss: 3.5799, LR: 0.000200
Epoch 32/100, Loss: 3.5800, LR: 0.000200
Epoch 33/100, Loss: 3.5798, LR: 0.000200
Epoch 34/100, Loss: 3.5799, LR: 0.000200
Epoch 35/100, Loss: 3.5797, LR: 0.000200
Epoch 36/100, Loss: 3.5799, LR: 0.000200
Epoch 37/100, Loss: 3.5799, LR: 0.000200
Epoch 38/100, Loss: 3.5799, LR: 0.000200
Epoch 39/100, Loss: 3.5798, LR: 0.000200
Epoch 40/100, Loss: 3.5798, LR: 0.000200
Epoch 41/100, Loss: 3.5798, LR: 0.000200
Epoch 42/100, Loss: 3.5796, LR: 0.000200
Epoch 43/100, Loss: 3.5797, LR: 0.000200
Epoch 44/100, Loss: 3.5796, LR: 0.000200
Epoch 45/100, Loss: 3.5796, LR: 0.000200
Epoch 46/100, Loss: 3.5792, LR: 0.000200
Epoch 47/100, Loss: 3.5793, LR: 0.000200
Epoch 48/100, Loss: 3.5795, LR: 0.000200
Epoch 49/100, Loss: 3.5798, LR: 0.000200
Epoch 50/100, Loss: 3.5797, LR: 0.000100
Epoch 51/100, Loss: 3.5794, LR: 0.000100
Epoch 52/100, Loss: 3.5794, LR: 0.000100
Epoch 53/100, Loss: 3.5791, LR: 0.000100
Epoch 54/100, Loss: 3.5793, LR: 0.000100
Epoch 55/100, Loss: 3.5795, LR: 0.000100
Epoch 56/100, Loss: 3.5794, LR: 0.000100
Epoch 57/100, Loss: 3.5794, LR: 0.000100
Epoch 58/100, Loss: 3.5793, LR: 0.000100
Epoch 59/100, Loss: 3.5793, LR: 0.000100
Epoch 60/100, Loss: 3.5793, LR: 0.000100
Epoch 61/100, Loss: 3.5794, LR: 0.000100
Epoch 62/100, Loss: 3.5793, LR: 0.000100
Epoch 63/100, Loss: 3.5793, LR: 0.000100
Epoch 64/100, Loss: 3.5794, LR: 0.000100
Epoch 65/100, Loss: 3.5793, LR: 0.000100
Epoch 66/100, Loss: 3.5793, LR: 0.000100
Epoch 67/100, Loss: 3.5795, LR: 0.000100
Epoch 68/100, Loss: 3.5797, LR: 0.000100
Epoch 69/100, Loss: 3.5795, LR: 0.000100
Epoch 70/100, Loss: 3.5792, LR: 0.000100
Epoch 71/100, Loss: 3.5795, LR: 0.000100
Epoch 72/100, Loss: 3.5795, LR: 0.000100
Epoch 73/100, Loss: 3.5795, LR: 0.000100
Epoch 74/100, Loss: 3.5792, LR: 0.000100
Epoch 75/100, Loss: 3.5794, LR: 0.000100
Epoch 76/100, Loss: 3.5791, LR: 0.000100
Epoch 77/100, Loss: 3.5794, LR: 0.000100
Epoch 78/100, Loss: 3.5792, LR: 0.000100
Epoch 79/100, Loss: 3.5795, LR: 0.000100
Epoch 80/100, Loss: 3.5793, LR: 0.000050
Epoch 81/100, Loss: 3.5793, LR: 0.000050
Epoch 82/100, Loss: 3.5792, LR: 0.000050
Epoch 83/100, Loss: 3.5795, LR: 0.000050
Epoch 84/100, Loss: 3.5791, LR: 0.000050
Epoch 85/100, Loss: 3.5792, LR: 0.000050
Epoch 86/100, Loss: 3.5794, LR: 0.000050
Epoch 87/100, Loss: 3.5790, LR: 0.000050
Epoch 88/100, Loss: 3.5794, LR: 0.000050
Epoch 89/100, Loss: 3.5793, LR: 0.000050
Epoch 90/100, Loss: 3.5793, LR: 0.000050
Epoch 91/100, Loss: 3.5795, LR: 0.000050
Epoch 92/100, Loss: 3.5793, LR: 0.000050
Epoch 93/100, Loss: 3.5795, LR: 0.000050
Epoch 94/100, Loss: 3.5795, LR: 0.000050
Epoch 95/100, Loss: 3.5794, LR: 0.000050
Epoch 96/100, Loss: 3.5796, LR: 0.000050
Epoch 97/100, Loss: 3.5794, LR: 0.000050
Epoch 98/100, Loss: 3.5790, LR: 0.000050
Epoch 99/100, Loss: 3.5793, LR: 0.000050
Epoch 100/100, Loss: 3.5794, LR: 0.000050
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.545
Change BestPlayer
>> Model updated (win rate: 54.5%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.4
>> Cycle 22 completed. Checkpoint saved.

Train 23 ====================================
>> Game Count: 1500
>> MCTS Simulations: 400
>> Learning Rate: 0.0002
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1500, Games per worker: 150
>> MCTS batch size: 64, PV evaluate count: 400
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/150 games completed
Worker 1: 20/150 games completed
Worker 9: 20/150 games completed
Worker 3: 20/150 games completed
Worker 10: 20/150 games completed
Worker 4: 20/150 games completed
Worker 5: 20/150 games completed
Worker 7: 20/150 games completed
Worker 2: 20/150 games completed
Worker 8: 20/150 games completed
Worker 6: 20/150 games completed
Worker 9: 30/150 games completed
Worker 3: 30/150 games completed
Worker 1: 30/150 games completed
Worker 8: 30/150 games completed
Worker 10: 30/150 games completed
Worker 5: 30/150 games completed
Worker 4: 30/150 games completed
Worker 7: 30/150 games completed
Worker 6: 30/150 games completed
Worker 2: 30/150 games completed
Worker 9: 40/150 games completed
Worker 3: 40/150 games completed
Worker 1: 40/150 games completed
Worker 8: 40/150 games completed
Worker 10: 40/150 games completed
Worker 5: 40/150 games completed
Worker 4: 40/150 games completed
Worker 7: 40/150 games completed
Worker 6: 40/150 games completed
Worker 2: 40/150 games completed
Worker 3: 50/150 games completed
Worker 9: 50/150 games completed
Worker 1: 50/150 games completed
Worker 10: 50/150 games completed
Worker 8: 50/150 games completed
Worker 6: 50/150 games completed
Worker 4: 50/150 games completed
Worker 5: 50/150 games completed
Worker 7: 50/150 games completed
Worker 2: 50/150 games completed
Worker 3: 60/150 games completed
Worker 9: 60/150 games completed
Worker 4: 60/150 games completed
Worker 10: 60/150 games completed
Worker 6: 60/150 games completed
Worker 5: 60/150 games completed
Worker 1: 60/150 games completed
Worker 7: 60/150 games completed
Worker 8: 60/150 games completed
Worker 2: 60/150 games completed
Worker 3: 70/150 games completed
Worker 9: 70/150 games completed
Worker 10: 70/150 games completed
Worker 7: 70/150 games completed
Worker 1: 70/150 games completed
Worker 6: 70/150 games completed
Worker 4: 70/150 games completed
Worker 5: 70/150 games completed
Worker 8: 70/150 games completed
Worker 2: 70/150 games completed
Worker 3: 80/150 games completed
Worker 9: 80/150 games completed
Worker 10: 80/150 games completed
Worker 7: 80/150 games completed
Worker 1: 80/150 games completed
Worker 6: 80/150 games completed
Worker 4: 80/150 games completed
Worker 8: 80/150 games completed
Worker 5: 80/150 games completed
Worker 2: 80/150 games completed
Worker 3: 90/150 games completed
Worker 9: 90/150 games completed
Worker 7: 90/150 games completed
Worker 10: 90/150 games completed
Worker 5: 90/150 games completed
Worker 2: 90/150 games completed
Worker 8: 90/150 games completed
Worker 6: 90/150 games completed
Worker 1: 90/150 games completed
Worker 4: 90/150 games completed
Worker 3: 100/150 games completed
Worker 9: 100/150 games completed
Worker 7: 100/150 games completed
Worker 10: 100/150 games completed
Worker 1: 100/150 games completed
Worker 4: 100/150 games completed
Worker 8: 100/150 games completed
Worker 5: 100/150 games completed
Worker 6: 100/150 games completed
Worker 2: 100/150 games completed
Worker 3: 110/150 games completed
Worker 9: 110/150 games completed
Worker 7: 110/150 games completed
Worker 10: 110/150 games completed
Worker 5: 110/150 games completed
Worker 6: 110/150 games completed
Worker 4: 110/150 games completed
Worker 8: 110/150 games completed
Worker 2: 110/150 games completed
Worker 1: 110/150 games completed
Worker 3: 120/150 games completed
Worker 9: 120/150 games completed
Worker 7: 120/150 games completed
Worker 10: 120/150 games completed
Worker 2: 120/150 games completed
Worker 5: 120/150 games completed
Worker 6: 120/150 games completed
Worker 4: 120/150 games completed
Worker 8: 120/150 games completed
Worker 1: 120/150 games completed
Worker 3: 130/150 games completed
Worker 7: 130/150 games completed
Worker 9: 130/150 games completed
Worker 10: 130/150 games completed
Worker 5: 130/150 games completed
Worker 2: 130/150 games completed
Worker 6: 130/150 games completed
Worker 8: 130/150 games completed
Worker 1: 130/150 games completed
Worker 4: 130/150 games completed
Worker 3: 140/150 games completed
Worker 7: 140/150 games completed
Worker 9: 140/150 games completed
Worker 10: 140/150 games completed
Worker 2: 140/150 games completed
Worker 5: 140/150 games completed
Worker 6: 140/150 games completed
Worker 8: 140/150 games completed
Worker 4: 140/150 games completed
Worker 1: 140/150 games completed
Worker 3: 150/150 games completed
Worker 7: 150/150 games completed
Worker 9: 150/150 games completed
Worker 10: 150/150 games completed
Worker 5: 150/150 games completed
Worker 4: 150/150 games completed
Worker 2: 150/150 games completed
Worker 6: 150/150 games completed
Worker 8: 150/150 games completed
Worker 1: 150/150 games completed
>> Collected 87981 training samples from 1500 games
>> Saved to ./data/20251031090218.history
>> Train 23
>> Learning Rate: 0.0002
Epoch 1/100, Loss: 3.5802, LR: 0.000200
Epoch 2/100, Loss: 3.5759, LR: 0.000200
Epoch 3/100, Loss: 3.5736, LR: 0.000200
Epoch 4/100, Loss: 3.5729, LR: 0.000200
Epoch 5/100, Loss: 3.5715, LR: 0.000200
Epoch 6/100, Loss: 3.5701, LR: 0.000200
Epoch 7/100, Loss: 3.5701, LR: 0.000200
Epoch 8/100, Loss: 3.5698, LR: 0.000200
Epoch 9/100, Loss: 3.5700, LR: 0.000200
Epoch 10/100, Loss: 3.5693, LR: 0.000200
Epoch 11/100, Loss: 3.5696, LR: 0.000200
Epoch 12/100, Loss: 3.5691, LR: 0.000200
Epoch 13/100, Loss: 3.5688, LR: 0.000200
Epoch 14/100, Loss: 3.5689, LR: 0.000200
Epoch 15/100, Loss: 3.5690, LR: 0.000200
Epoch 16/100, Loss: 3.5687, LR: 0.000200
Epoch 17/100, Loss: 3.5691, LR: 0.000200
Epoch 18/100, Loss: 3.5681, LR: 0.000200
Epoch 19/100, Loss: 3.5688, LR: 0.000200
Epoch 20/100, Loss: 3.5690, LR: 0.000200
Epoch 21/100, Loss: 3.5683, LR: 0.000200
Epoch 22/100, Loss: 3.5680, LR: 0.000200
Epoch 23/100, Loss: 3.5687, LR: 0.000200
Epoch 24/100, Loss: 3.5685, LR: 0.000200
Epoch 25/100, Loss: 3.5688, LR: 0.000200
Epoch 26/100, Loss: 3.5686, LR: 0.000200
Epoch 27/100, Loss: 3.5681, LR: 0.000200
Epoch 28/100, Loss: 3.5682, LR: 0.000200
Epoch 29/100, Loss: 3.5683, LR: 0.000200
Epoch 30/100, Loss: 3.5681, LR: 0.000200
Epoch 31/100, Loss: 3.5678, LR: 0.000200
Epoch 32/100, Loss: 3.5687, LR: 0.000200
Epoch 33/100, Loss: 3.5678, LR: 0.000200
Epoch 34/100, Loss: 3.5680, LR: 0.000200
Epoch 35/100, Loss: 3.5685, LR: 0.000200
Epoch 36/100, Loss: 3.5683, LR: 0.000200
Epoch 37/100, Loss: 3.5679, LR: 0.000200
Epoch 38/100, Loss: 3.5683, LR: 0.000200
Epoch 39/100, Loss: 3.5682, LR: 0.000200
Epoch 40/100, Loss: 3.5680, LR: 0.000200
Epoch 41/100, Loss: 3.5680, LR: 0.000200
Epoch 42/100, Loss: 3.5675, LR: 0.000200
Epoch 43/100, Loss: 3.5681, LR: 0.000200
Epoch 44/100, Loss: 3.5681, LR: 0.000200
Epoch 45/100, Loss: 3.5681, LR: 0.000200
Epoch 46/100, Loss: 3.5682, LR: 0.000200
Epoch 47/100, Loss: 3.5683, LR: 0.000200
Epoch 48/100, Loss: 3.5678, LR: 0.000200
Epoch 49/100, Loss: 3.5682, LR: 0.000200
Epoch 50/100, Loss: 3.5681, LR: 0.000100
Epoch 51/100, Loss: 3.5680, LR: 0.000100
Epoch 52/100, Loss: 3.5672, LR: 0.000100
Epoch 53/100, Loss: 3.5679, LR: 0.000100
Epoch 54/100, Loss: 3.5678, LR: 0.000100
Epoch 55/100, Loss: 3.5674, LR: 0.000100
Epoch 56/100, Loss: 3.5676, LR: 0.000100
Epoch 57/100, Loss: 3.5680, LR: 0.000100
Epoch 58/100, Loss: 3.5677, LR: 0.000100
Epoch 59/100, Loss: 3.5676, LR: 0.000100
Epoch 60/100, Loss: 3.5678, LR: 0.000100
Epoch 61/100, Loss: 3.5678, LR: 0.000100
Epoch 62/100, Loss: 3.5679, LR: 0.000100
Epoch 63/100, Loss: 3.5680, LR: 0.000100
Epoch 64/100, Loss: 3.5680, LR: 0.000100
Epoch 65/100, Loss: 3.5679, LR: 0.000100
Epoch 66/100, Loss: 3.5678, LR: 0.000100
Epoch 67/100, Loss: 3.5677, LR: 0.000100
Epoch 68/100, Loss: 3.5675, LR: 0.000100
Epoch 69/100, Loss: 3.5675, LR: 0.000100
Epoch 70/100, Loss: 3.5675, LR: 0.000100
Epoch 71/100, Loss: 3.5677, LR: 0.000100
Epoch 72/100, Loss: 3.5674, LR: 0.000100
Epoch 73/100, Loss: 3.5679, LR: 0.000100
Epoch 74/100, Loss: 3.5675, LR: 0.000100
Epoch 75/100, Loss: 3.5680, LR: 0.000100
Epoch 76/100, Loss: 3.5680, LR: 0.000100
Epoch 77/100, Loss: 3.5674, LR: 0.000100
Epoch 78/100, Loss: 3.5682, LR: 0.000100
Epoch 79/100, Loss: 3.5673, LR: 0.000100
Epoch 80/100, Loss: 3.5678, LR: 0.000050
Epoch 81/100, Loss: 3.5677, LR: 0.000050
Epoch 82/100, Loss: 3.5675, LR: 0.000050
Epoch 83/100, Loss: 3.5680, LR: 0.000050
Epoch 84/100, Loss: 3.5673, LR: 0.000050
Epoch 85/100, Loss: 3.5679, LR: 0.000050
Epoch 86/100, Loss: 3.5678, LR: 0.000050
Epoch 87/100, Loss: 3.5679, LR: 0.000050
Epoch 88/100, Loss: 3.5673, LR: 0.000050
Epoch 89/100, Loss: 3.5675, LR: 0.000050
Epoch 90/100, Loss: 3.5679, LR: 0.000050
Epoch 91/100, Loss: 3.5679, LR: 0.000050
Epoch 92/100, Loss: 3.5672, LR: 0.000050
Epoch 93/100, Loss: 3.5674, LR: 0.000050
Epoch 94/100, Loss: 3.5681, LR: 0.000050
Epoch 95/100, Loss: 3.5674, LR: 0.000050
Epoch 96/100, Loss: 3.5676, LR: 0.000050
Epoch 97/100, Loss: 3.5674, LR: 0.000050
Epoch 98/100, Loss: 3.5677, LR: 0.000050
Epoch 99/100, Loss: 3.5678, LR: 0.000050
Epoch 100/100, Loss: 3.5671, LR: 0.000050
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Evaluate 82/100
Evaluate 83/100
Evaluate 84/100
Evaluate 85/100
Evaluate 86/100
Evaluate 87/100
Evaluate 88/100
Evaluate 89/100
Evaluate 90/100
Evaluate 91/100
Evaluate 92/100
Evaluate 93/100
Evaluate 94/100
Evaluate 95/100
Evaluate 96/100
Evaluate 97/100
Evaluate 98/100
Evaluate 99/100
Evaluate 100/100
AveragePoint 0.55
Change BestPlayer
>> Model updated (win rate: 55.0%)

Evaluate 1/10
Evaluate 2/10
Evaluate 3/10
Evaluate 4/10
Evaluate 5/10
Evaluate 6/10
Evaluate 7/10
Evaluate 8/10
Evaluate 9/10
Evaluate 10/10
VS_Random 0.25
>> Cycle 23 completed. Checkpoint saved.

Train 24 ====================================
>> Game Count: 1500
>> MCTS Simulations: 400
>> Learning Rate: 0.0002
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 1500, Games per worker: 150
>> MCTS batch size: 64, PV evaluate count: 400
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 4: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 6: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 2: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 9: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 1: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 3: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 10: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 7: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 8: 10/150 games completed
Using C++ backend for MCTS
>> CUDA optimizations enabled:
   - TF32: True
   - cuDNN benchmark: True
>> PyTorch CPU threads: 8
Worker 5: 10/150 games completed
Worker 2: 20/150 games completed
Worker 10: 20/150 games completed
Worker 6: 20/150 games completed
Worker 9: 20/150 games completed
Worker 1: 20/150 games completed
Worker 3: 20/150 games completed
Worker 7: 20/150 games completed
Worker 4: 20/150 games completed
Worker 5: 20/150 games completed
Worker 8: 20/150 games completed
Worker 6: 30/150 games completed
Worker 2: 30/150 games completed
Worker 7: 30/150 games completed
Worker 9: 30/150 games completed
Worker 3: 30/150 games completed
Worker 1: 30/150 games completed
Worker 4: 30/150 games completed
Worker 10: 30/150 games completed
Worker 8: 30/150 games completed
Worker 5: 30/150 games completed
Worker 7: 40/150 games completed
Worker 4: 40/150 games completed
Worker 6: 40/150 games completed
Worker 9: 40/150 games completed
Worker 2: 40/150 games completed
Worker 10: 40/150 games completed
Worker 1: 40/150 games completed
Worker 3: 40/150 games completed
Worker 8: 40/150 games completed
Worker 5: 40/150 games completed
Worker 4: 50/150 games completed
Worker 6: 50/150 games completed
Worker 1: 50/150 games completed
Worker 7: 50/150 games completed
Worker 10: 50/150 games completed
Worker 9: 50/150 games completed
Worker 2: 50/150 games completed
Worker 3: 50/150 games completed
Worker 8: 50/150 games completed
Worker 5: 50/150 games completed
Worker 4: 60/150 games completed
Worker 6: 60/150 games completed
Worker 1: 60/150 games completed
Worker 7: 60/150 games completed
Worker 10: 60/150 games completed
Worker 9: 60/150 games completed
Worker 2: 60/150 games completed
Worker 3: 60/150 games completed
Worker 8: 60/150 games completed
Worker 5: 60/150 games completed
Worker 4: 70/150 games completed
Worker 1: 70/150 games completed
Worker 6: 70/150 games completed
Worker 7: 70/150 games completed
Worker 9: 70/150 games completed
Worker 10: 70/150 games completed
Worker 2: 70/150 games completed
Worker 3: 70/150 games completed
Worker 8: 70/150 games completed
Worker 5: 70/150 games completed
Worker 4: 80/150 games completed
Worker 1: 80/150 games completed
Worker 2: 80/150 games completed
Worker 7: 80/150 games completed
Worker 10: 80/150 games completed
Worker 6: 80/150 games completed
Worker 9: 80/150 games completed
Worker 3: 80/150 games completed
Worker 8: 80/150 games completed
Worker 5: 80/150 games completed
Worker 4: 90/150 games completed
Worker 2: 90/150 games completed
Worker 7: 90/150 games completed
Worker 1: 90/150 games completed
Worker 6: 90/150 games completed
Worker 10: 90/150 games completed
Worker 9: 90/150 games completed
Worker 3: 90/150 games completed
Worker 8: 90/150 games completed
Worker 5: 90/150 games completed
Worker 4: 100/150 games completed
Worker 2: 100/150 games completed
Worker 7: 100/150 games completed
Worker 6: 100/150 games completed
Worker 1: 100/150 games completed
Worker 10: 100/150 games completed
Worker 9: 100/150 games completed
Worker 3: 100/150 games completed
Worker 8: 100/150 games completed
Worker 5: 100/150 games completed
Worker 4: 110/150 games completed
Worker 2: 110/150 games completed
Worker 7: 110/150 games completed
Worker 1: 110/150 games completed
Worker 6: 110/150 games completed
Worker 10: 110/150 games completed
Worker 3: 110/150 games completed
Worker 9: 110/150 games completed
Worker 8: 110/150 games completed
Worker 5: 110/150 games completed
Worker 4: 120/150 games completed
Worker 7: 120/150 games completed
Worker 2: 120/150 games completed
Worker 6: 120/150 games completed
Worker 1: 120/150 games completed
Worker 3: 120/150 games completed
Worker 9: 120/150 games completed
Worker 10: 120/150 games completed
Worker 8: 120/150 games completed
Worker 5: 120/150 games completed
Worker 4: 130/150 games completed
Worker 7: 130/150 games completed
Worker 2: 130/150 games completed
Worker 6: 130/150 games completed
Worker 1: 130/150 games completed
Worker 3: 130/150 games completed
Worker 10: 130/150 games completed
Worker 9: 130/150 games completed
Worker 8: 130/150 games completed
Worker 5: 130/150 games completed
Worker 4: 140/150 games completed
Worker 7: 140/150 games completed
Worker 2: 140/150 games completed
Worker 6: 140/150 games completed
Worker 3: 140/150 games completed
Worker 10: 140/150 games completed
Worker 9: 140/150 games completed
Worker 1: 140/150 games completed
Worker 8: 140/150 games completed
Worker 5: 140/150 games completed
Worker 4: 150/150 games completed
Worker 2: 150/150 games completed
Worker 7: 150/150 games completed
Worker 9: 150/150 games completed
Worker 6: 150/150 games completed
Worker 3: 150/150 games completed
Worker 10: 150/150 games completed
Worker 1: 150/150 games completed
Worker 8: 150/150 games completed
Worker 5: 150/150 games completed
>> Collected 88006 training samples from 1500 games
>> Saved to ./data/20251031101300.history
>> Train 24
>> Learning Rate: 0.0002
Epoch 1/100, Loss: 3.5983, LR: 0.000200
Epoch 2/100, Loss: 3.5937, LR: 0.000200
Epoch 3/100, Loss: 3.5912, LR: 0.000200
Epoch 4/100, Loss: 3.5898, LR: 0.000200
Epoch 5/100, Loss: 3.5889, LR: 0.000200
Epoch 6/100, Loss: 3.5886, LR: 0.000200
Epoch 7/100, Loss: 3.5879, LR: 0.000200
Epoch 8/100, Loss: 3.5876, LR: 0.000200
Epoch 9/100, Loss: 3.5874, LR: 0.000200
Epoch 10/100, Loss: 3.5869, LR: 0.000200
Epoch 11/100, Loss: 3.5872, LR: 0.000200
Epoch 12/100, Loss: 3.5867, LR: 0.000200
Epoch 13/100, Loss: 3.5869, LR: 0.000200
Epoch 14/100, Loss: 3.5870, LR: 0.000200
Epoch 15/100, Loss: 3.5868, LR: 0.000200
Epoch 16/100, Loss: 3.5869, LR: 0.000200
Epoch 17/100, Loss: 3.5866, LR: 0.000200
Epoch 18/100, Loss: 3.5865, LR: 0.000200
Epoch 19/100, Loss: 3.5864, LR: 0.000200
Epoch 20/100, Loss: 3.5864, LR: 0.000200
Epoch 21/100, Loss: 3.5865, LR: 0.000200
Epoch 22/100, Loss: 3.5866, LR: 0.000200
Epoch 23/100, Loss: 3.5861, LR: 0.000200
Epoch 24/100, Loss: 3.5867, LR: 0.000200
Epoch 25/100, Loss: 3.5863, LR: 0.000200
Epoch 26/100, Loss: 3.5864, LR: 0.000200
Epoch 27/100, Loss: 3.5860, LR: 0.000200
Epoch 28/100, Loss: 3.5862, LR: 0.000200
Epoch 29/100, Loss: 3.5863, LR: 0.000200
Epoch 30/100, Loss: 3.5867, LR: 0.000200
Epoch 31/100, Loss: 3.5861, LR: 0.000200
Epoch 32/100, Loss: 3.5861, LR: 0.000200
Epoch 33/100, Loss: 3.5861, LR: 0.000200
Epoch 34/100, Loss: 3.5861, LR: 0.000200
Epoch 35/100, Loss: 3.5862, LR: 0.000200
Epoch 36/100, Loss: 3.5861, LR: 0.000200
Epoch 37/100, Loss: 3.5863, LR: 0.000200
Epoch 38/100, Loss: 3.5859, LR: 0.000200
Epoch 39/100, Loss: 3.5859, LR: 0.000200
Epoch 40/100, Loss: 3.5861, LR: 0.000200
Epoch 41/100, Loss: 3.5861, LR: 0.000200
Epoch 42/100, Loss: 3.5862, LR: 0.000200
Epoch 43/100, Loss: 3.5859, LR: 0.000200
Epoch 44/100, Loss: 3.5859, LR: 0.000200
Epoch 45/100, Loss: 3.5860, LR: 0.000200
Epoch 46/100, Loss: 3.5860, LR: 0.000200
Epoch 47/100, Loss: 3.5860, LR: 0.000200
Epoch 48/100, Loss: 3.5858, LR: 0.000200
Epoch 49/100, Loss: 3.5860, LR: 0.000200
Epoch 50/100, Loss: 3.5856, LR: 0.000100
Epoch 51/100, Loss: 3.5858, LR: 0.000100
Epoch 52/100, Loss: 3.5860, LR: 0.000100
Epoch 53/100, Loss: 3.5855, LR: 0.000100
Epoch 54/100, Loss: 3.5857, LR: 0.000100
Epoch 55/100, Loss: 3.5859, LR: 0.000100
Epoch 56/100, Loss: 3.5858, LR: 0.000100
Epoch 57/100, Loss: 3.5858, LR: 0.000100
Epoch 58/100, Loss: 3.5856, LR: 0.000100
Epoch 59/100, Loss: 3.5854, LR: 0.000100
Epoch 60/100, Loss: 3.5858, LR: 0.000100
Epoch 61/100, Loss: 3.5858, LR: 0.000100
Epoch 62/100, Loss: 3.5859, LR: 0.000100
Epoch 63/100, Loss: 3.5861, LR: 0.000100
Epoch 64/100, Loss: 3.5856, LR: 0.000100
Epoch 65/100, Loss: 3.5854, LR: 0.000100
Epoch 66/100, Loss: 3.5858, LR: 0.000100
Epoch 67/100, Loss: 3.5857, LR: 0.000100
Epoch 68/100, Loss: 3.5858, LR: 0.000100
Epoch 69/100, Loss: 3.5860, LR: 0.000100
Epoch 70/100, Loss: 3.5859, LR: 0.000100
Epoch 71/100, Loss: 3.5856, LR: 0.000100
Epoch 72/100, Loss: 3.5856, LR: 0.000100
Epoch 73/100, Loss: 3.5858, LR: 0.000100
Epoch 74/100, Loss: 3.5853, LR: 0.000100
Epoch 75/100, Loss: 3.5856, LR: 0.000100
Epoch 76/100, Loss: 3.5857, LR: 0.000100
Epoch 77/100, Loss: 3.5857, LR: 0.000100
Epoch 78/100, Loss: 3.5860, LR: 0.000100
Epoch 79/100, Loss: 3.5857, LR: 0.000100
Epoch 80/100, Loss: 3.5859, LR: 0.000050
Epoch 81/100, Loss: 3.5857, LR: 0.000050
Epoch 82/100, Loss: 3.5856, LR: 0.000050
Epoch 83/100, Loss: 3.5855, LR: 0.000050
Epoch 84/100, Loss: 3.5856, LR: 0.000050
Epoch 85/100, Loss: 3.5855, LR: 0.000050
Epoch 86/100, Loss: 3.5856, LR: 0.000050
Epoch 87/100, Loss: 3.5857, LR: 0.000050
Epoch 88/100, Loss: 3.5858, LR: 0.000050
Epoch 89/100, Loss: 3.5856, LR: 0.000050
Epoch 90/100, Loss: 3.5859, LR: 0.000050
Epoch 91/100, Loss: 3.5858, LR: 0.000050
Epoch 92/100, Loss: 3.5856, LR: 0.000050
Epoch 93/100, Loss: 3.5857, LR: 0.000050
Epoch 94/100, Loss: 3.5859, LR: 0.000050
Epoch 95/100, Loss: 3.5856, LR: 0.000050
Epoch 96/100, Loss: 3.5856, LR: 0.000050
Epoch 97/100, Loss: 3.5858, LR: 0.000050
Epoch 98/100, Loss: 3.5856, LR: 0.000050
Epoch 99/100, Loss: 3.5860, LR: 0.000050
Epoch 100/100, Loss: 3.5858, LR: 0.000050
Model saved to ./model/latest.pth

Evaluate 1/100
Evaluate 2/100
Evaluate 3/100
Evaluate 4/100
Evaluate 5/100
Evaluate 6/100
Evaluate 7/100
Evaluate 8/100
Evaluate 9/100
Evaluate 10/100
Evaluate 11/100
Evaluate 12/100
Evaluate 13/100
Evaluate 14/100
Evaluate 15/100
Evaluate 16/100
Evaluate 17/100
Evaluate 18/100
Evaluate 19/100
Evaluate 20/100
Evaluate 21/100
Evaluate 22/100
Evaluate 23/100
Evaluate 24/100
Evaluate 25/100
Evaluate 26/100
Evaluate 27/100
Evaluate 28/100
Evaluate 29/100
Evaluate 30/100
Evaluate 31/100
Evaluate 32/100
Evaluate 33/100
Evaluate 34/100
Evaluate 35/100
Evaluate 36/100
Evaluate 37/100
Evaluate 38/100
Evaluate 39/100
Evaluate 40/100
Evaluate 41/100
Evaluate 42/100
Evaluate 43/100
Evaluate 44/100
Evaluate 45/100
Evaluate 46/100
Evaluate 47/100
Evaluate 48/100
Evaluate 49/100
Evaluate 50/100
Evaluate 51/100
Evaluate 52/100
Evaluate 53/100
Evaluate 54/100
Evaluate 55/100
Evaluate 56/100
Evaluate 57/100
Evaluate 58/100
Evaluate 59/100
Evaluate 60/100
Evaluate 61/100
Evaluate 62/100
Evaluate 63/100
Evaluate 64/100
Evaluate 65/100
Evaluate 66/100
Evaluate 67/100
Evaluate 68/100
Evaluate 69/100
Evaluate 70/100
Evaluate 71/100
Evaluate 72/100
Evaluate 73/100
Evaluate 74/100
Evaluate 75/100
Evaluate 76/100
Evaluate 77/100
Evaluate 78/100
Evaluate 79/100
Evaluate 80/100
Evaluate 81/100
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using C++ backend for MCTS
>> Starting from cycle 0

Train 0 (Attempt 0) ====================================
>> Successful Cycle: 0
>> Total Attempts: 0
>> Game Count: 500
>> MCTS Simulations: 100
>> Learning Rate: 0.001
>> Starting parallel self-play with 10 workers (C++ backend)
>> Total games: 500, Games per worker: 50
>> MCTS batch size: 64, PV evaluate count: 100
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Using GPU: NVIDIA GeForce RTX 3090
Using C++ backend for MCTS
>> Using parallel C++ backend for maximum speed!
Process SpawnPoolWorker-8:
