"""
ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
æ®µéšçš„ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®š
"""

import torch
import time
import psutil
import subprocess
import sys
from pathlib import Path
from dual_network import DN_INPUT_SHAPE

def get_gpu_memory_usage():
    """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    if not torch.cuda.is_available():
        return 0, 0, 0
    
    device = torch.cuda.current_device()
    total = torch.cuda.get_device_properties(device).total_memory / 1024**3
    allocated = torch.cuda.memory_allocated(device) / 1024**3
    reserved = torch.cuda.memory_reserved(device) / 1024**3
    
    return total, allocated, reserved

def get_ram_usage():
    """RAMä½¿ç”¨é‡ã‚’å–å¾—"""
    memory = psutil.virtual_memory()
    return memory.total / 1024**3, memory.used / 1024**3, memory.percent

def monitor_nvidia_smi():
    """nvidia-smiã§GPUä½¿ç”¨ç‡ã‚’ç¢ºèª"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=name,memory.total,memory.used,memory.free,utilization.gpu', 
             '--format=csv,noheader,nounits'],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except:
        pass
    return None

def test_batch_size(batch_size, num_games=10):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã‚»ãƒ«ãƒ•ãƒ—ãƒ¬ã‚¤ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹
    
    Args:
        batch_size: ãƒ†ã‚¹ãƒˆã™ã‚‹MCTSãƒãƒƒãƒã‚µã‚¤ã‚º
        num_games: ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚²ãƒ¼ãƒ æ•°
    
    Returns:
        (æˆåŠŸ, å¹³å‡æ™‚é–“, GPUä½¿ç”¨é‡)
    """
    print(f"\n{'='*60}")
    print(f"Testing Batch Size: {batch_size}")
    print(f"{'='*60}")
    
    # åˆæœŸãƒ¡ãƒ¢ãƒªçŠ¶æ…‹
    torch.cuda.empty_cache()
    time.sleep(1)
    
    gpu_total, gpu_allocated_before, gpu_reserved_before = get_gpu_memory_usage()
    ram_total, ram_used_before, ram_percent_before = get_ram_usage()
    
    print(f"\n[Before Test]")
    print(f"GPU Memory: {gpu_allocated_before:.2f}/{gpu_total:.1f} GB allocated")
    print(f"RAM: {ram_used_before:.1f}/{ram_total:.1f} GB used ({ram_percent_before:.1f}%)")
    
    # nvidia-smiã§ç¢ºèª
    nvidia_info = monitor_nvidia_smi()
    if nvidia_info:
        print(f"nvidia-smi: {nvidia_info}")
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        from dual_network import DN_INPUT_SHAPE
        from pathlib import Path
        import pickle
        
        # C++ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ç¢ºèª
        try:
            import uttt_cpp
            from pv_mcts_cpp import pv_mcts_scores_cpp
            cpp_available = True
            print("\n>> Using C++ backend")
        except ImportError:
            cpp_available = False
            print("\n>> C++ backend not available")
            return False, 0, 0
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        model_path = Path('./model/best.pth')
        if not model_path.exists():
            print("Error: model/best.pth not found")
            return False, 0, 0
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        from dual_network import DualNetwork
        model = DualNetwork()
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.to(device)
        model.eval()
        
        print(f"Model loaded on {device}")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¾Œï¼‰
        gpu_total, gpu_allocated_after_model, gpu_reserved_after_model = get_gpu_memory_usage()
        print(f"\n[After Model Load]")
        print(f"GPU Memory: {gpu_allocated_after_model:.2f}/{gpu_total:.1f} GB allocated")
        print(f"Model size: {(gpu_allocated_after_model - gpu_allocated_before):.2f} GB")
        
        # ã‚»ãƒ«ãƒ•ãƒ—ãƒ¬ã‚¤ã®ãƒ†ã‚¹ãƒˆ
        game_times = []
        
        print(f"\n[Running {num_games} games with batch_size={batch_size}]")
        
        for i in range(num_games):
            start_time = time.time()
            
            # 1ã‚²ãƒ¼ãƒ å®Ÿè¡Œ
            state = uttt_cpp.State()
            move_count = 0
            
            while not state.is_done():
                # MCTSã§ã‚¹ã‚³ã‚¢ã‚’å–å¾—
                scores = pv_mcts_scores_cpp(
                    model, state, 
                    temperature=1.0, 
                    evaluate_count=100,  # Train 0-9ã®è¨­å®š
                    batch_size=batch_size
                )
                
                # è¡Œå‹•ã‚’é¸æŠ
                action = torch.multinomial(torch.tensor(scores), 1).item()
                state = state.next(action)
                move_count += 1
            
            elapsed = time.time() - start_time
            game_times.append(elapsed)
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
            print(f"  Game {i+1}/{num_games}: {elapsed:.2f}s ({move_count} moves)", end='', flush=True)
            
            # GPUä½¿ç”¨é‡ã‚’è¡¨ç¤º
            _, gpu_allocated, gpu_reserved = get_gpu_memory_usage()
            print(f" | GPU: {gpu_allocated:.2f}GB", flush=True)
        
        # çµ±è¨ˆ
        avg_time = sum(game_times) / len(game_times)
        min_time = min(game_times)
        max_time = max(game_times)
        
        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹
        gpu_total, gpu_allocated_final, gpu_reserved_final = get_gpu_memory_usage()
        ram_total, ram_used_final, ram_percent_final = get_ram_usage()
        
        print(f"\n[After Test]")
        print(f"GPU Memory: {gpu_allocated_final:.2f}/{gpu_total:.1f} GB allocated")
        print(f"RAM: {ram_used_final:.1f}/{ram_total:.1f} GB used ({ram_percent_final:.1f}%)")
        
        # çµæœã‚µãƒãƒªãƒ¼
        print(f"\n{'='*60}")
        print(f"Results for Batch Size {batch_size}:")
        print(f"{'='*60}")
        print(f"âœ… SUCCESS")
        print(f"Average time per game: {avg_time:.2f}s")
        print(f"Min/Max time: {min_time:.2f}s / {max_time:.2f}s")
        print(f"Games per second: {1/avg_time:.2f}")
        print(f"Peak GPU Memory: {gpu_allocated_final:.2f} GB / {gpu_total:.1f} GB")
        print(f"GPU Memory increase: {(gpu_allocated_final - gpu_allocated_before):.2f} GB")
        
        # nvidia-smiæœ€çµ‚ç¢ºèª
        nvidia_info = monitor_nvidia_smi()
        if nvidia_info:
            print(f"\nnvidia-smi: {nvidia_info}")
        
        return True, avg_time, gpu_allocated_final
        
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print(f"\nâŒ OUT OF MEMORY ERROR")
            print(f"Batch size {batch_size} is too large!")
            
            # ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’è¡¨ç¤º
            gpu_total, gpu_allocated, gpu_reserved = get_gpu_memory_usage()
            print(f"GPU Memory at failure: {gpu_allocated:.2f}/{gpu_total:.1f} GB")
            
            return False, 0, gpu_allocated
        else:
            print(f"\nâŒ ERROR: {e}")
            return False, 0, 0
    
    except Exception as e:
        print(f"\nâŒ UNEXPECTED ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False, 0, 0
    
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        torch.cuda.empty_cache()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("="*60)
    print("Batch Size Optimization Benchmark")
    print("="*60)
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    print("\n[System Information]")
    if torch.cuda.is_available():
        device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(device)
        total_memory = torch.cuda.get_device_properties(device).total_memory / 1024**3
        print(f"GPU: {gpu_name}")
        print(f"VRAM: {total_memory:.1f} GB")
    else:
        print("GPU: Not available")
        return
    
    memory = psutil.virtual_memory()
    print(f"RAM: {memory.total / 1024**3:.1f} GB")
    print(f"CPU Cores: {psutil.cpu_count(logical=False)} physical")
    
    # ãƒ†ã‚¹ãƒˆã™ã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã®ãƒªã‚¹ãƒˆ
    batch_sizes = [16, 24, 32, 48, 64]
    num_test_games = 5  # å„ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚²ãƒ¼ãƒ æ•°
    
    print(f"\nTesting batch sizes: {batch_sizes}")
    print(f"Games per test: {num_test_games}")
    
    input("\nPress Enter to start benchmark...")
    
    # çµæœã‚’ä¿å­˜
    results = []
    
    for batch_size in batch_sizes:
        success, avg_time, gpu_memory = test_batch_size(batch_size, num_test_games)
        
        results.append({
            'batch_size': batch_size,
            'success': success,
            'avg_time': avg_time,
            'gpu_memory': gpu_memory
        })
        
        if not success:
            print(f"\nâš ï¸  Batch size {batch_size} failed. Stopping tests.")
            break
        
        # æ¬¡ã®ãƒ†ã‚¹ãƒˆã®å‰ã«å°‘ã—å¾…æ©Ÿ
        time.sleep(2)
    
    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
    print("\n" + "="*60)
    print("BENCHMARK RESULTS SUMMARY")
    print("="*60)
    
    print(f"\n{'Batch Size':<12} {'Status':<10} {'Avg Time':<12} {'Speed':<12} {'GPU Mem':<10}")
    print("-" * 60)
    
    successful_results = [r for r in results if r['success']]
    
    for r in results:
        status = "âœ… OK" if r['success'] else "âŒ FAILED"
        avg_time_str = f"{r['avg_time']:.2f}s" if r['success'] else "N/A"
        speed_str = f"{1/r['avg_time']:.2f} g/s" if r['success'] else "N/A"
        gpu_mem_str = f"{r['gpu_memory']:.2f} GB" if r['gpu_memory'] > 0 else "N/A"
        
        print(f"{r['batch_size']:<12} {status:<10} {avg_time_str:<12} {speed_str:<12} {gpu_mem_str:<10}")
    
    if successful_results:
        # æœ€é€Ÿã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¨å¥¨
        fastest = min(successful_results, key=lambda x: x['avg_time'])
        
        print("\n" + "="*60)
        print("RECOMMENDATION")
        print("="*60)
        print(f"âœ¨ Optimal Batch Size: {fastest['batch_size']}")
        print(f"   Average time: {fastest['avg_time']:.2f}s per game")
        print(f"   Speed: {1/fastest['avg_time']:.2f} games/second")
        print(f"   GPU Memory: {fastest['gpu_memory']:.2f} GB")
        
        # auto_tune.pyã®æ›´æ–°ã‚’ææ¡ˆ
        print("\nğŸ“ To apply this setting, update auto_tune.py:")
        print(f"   Change the batch size for your GPU to: {fastest['batch_size']}")

if __name__ == '__main__':
    main()
